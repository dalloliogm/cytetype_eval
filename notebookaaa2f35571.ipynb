{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ead28876",
   "metadata": {
    "_cell_guid": "391f069a-9082-494c-a26c-3b1de0700cf8",
    "_uuid": "2bfaec73-cd04-4e35-9058-fa6ff8085de0",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.007435,
     "end_time": "2025-08-18T05:36:23.762688",
     "exception": false,
     "start_time": "2025-08-18T05:36:23.755253",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# References\n",
    "\n",
    "1. https://www.kaggle.com/code/hideyukizushi/cmi25-imu-thm-tof-tf-blendingmodel-lb-82\n",
    "2. https://www.kaggle.com/code/wasupandceacar/lb-0-82-5fold-single-bert-model\n",
    "3. https://www.kaggle.com/datasets/kerta27/cmi-data-gated-gru (Trained from https://www.kaggle.com/code/pepushi/gated-gru-hybrid-ensemble-v02)\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "Basically, it's all the same - but the difference is in the dynamics.\n",
    "Let's try to compress the search space using this [scheme](#predict), frankly speaking, we don't know what will come of it. We draw a parallel here and in the [PS-s5e8](https://www.kaggle.com/competitions/playground-series-s5e8/code?competitionId=91719&sortBy=scoreDescending&excludeNonAccessedDatasources=true) and [DRW](https://www.kaggle.com/competitions/drw-crypto-market-prediction/code?competitionId=96164&sortBy=scoreDescending&excludeNonAccessedDatasources=true) competitions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f78a49",
   "metadata": {
    "_cell_guid": "a2972d12-bbe6-48da-b0d1-3c024a3ce776",
    "_uuid": "b634cefa-f1a0-46b2-9917-7520309b966f",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.005735,
     "end_time": "2025-08-18T05:36:23.774593",
     "exception": false,
     "start_time": "2025-08-18T05:36:23.768858",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8c51a64",
   "metadata": {
    "_cell_guid": "87809ac8-406b-4d0c-b8c5-504b071288c3",
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "1a6cdff9-4cf8-4c88-b56e-ec5e1d38874d",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-08-18T05:36:23.787958Z",
     "iopub.status.busy": "2025-08-18T05:36:23.787627Z",
     "iopub.status.idle": "2025-08-18T05:37:02.881336Z",
     "shell.execute_reply": "2025-08-18T05:37:02.880446Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 39.102156,
     "end_time": "2025-08-18T05:37:02.882695",
     "exception": false,
     "start_time": "2025-08-18T05:36:23.780539",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-18 05:36:31.053231: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1755495391.421720      19 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1755495391.532724      19 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ imports ready · tensorflow 2.18.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1755495409.242461      19 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
      "I0000 00:00:1755495409.243193      19 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13942 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ INFERENCE MODE – loading artefacts from /kaggle/input/cmi-d-111\n",
      "  Loading models for ensemble inference...\n",
      ">>>LoadModel>>> /kaggle/input/cmi-d-111/D-111_0.h5\n",
      ">>>LoadModel>>> /kaggle/input/cmi-d-111/D-111_1.h5\n",
      ">>>LoadModel>>> /kaggle/input/cmi-d-111/D-111_2.h5\n",
      ">>>LoadModel>>> /kaggle/input/cmi-d-111/D-111_3.h5\n",
      ">>>LoadModel>>> /kaggle/input/cmi-d-111/D-111_4.h5\n",
      ">>>LoadModel>>> /kaggle/input/cmi-d-111/D-111_5.h5\n",
      ">>>LoadModel>>> /kaggle/input/cmi-d-111/D-111_6.h5\n",
      ">>>LoadModel>>> /kaggle/input/cmi-d-111/D-111_7.h5\n",
      ">>>LoadModel>>> /kaggle/input/cmi-d-111/D-111_8.h5\n",
      ">>>LoadModel>>> /kaggle/input/cmi-d-111/D-111_9.h5\n",
      "--------------------------------------------------\n",
      ">>>LoadModel>>> /kaggle/input/cmi-d-111/v0629_0.h5\n",
      ">>>LoadModel>>> /kaggle/input/cmi-d-111/v0629_1.h5\n",
      ">>>LoadModel>>> /kaggle/input/cmi-d-111/v0629_2.h5\n",
      ">>>LoadModel>>> /kaggle/input/cmi-d-111/v0629_3.h5\n",
      ">>>LoadModel>>> /kaggle/input/cmi-d-111/v0629_4.h5\n",
      ">>>LoadModel>>> /kaggle/input/cmi-d-111/v0629_5.h5\n",
      ">>>LoadModel>>> /kaggle/input/cmi-d-111/v0629_6.h5\n",
      ">>>LoadModel>>> /kaggle/input/cmi-d-111/v0629_7.h5\n",
      ">>>LoadModel>>> /kaggle/input/cmi-d-111/v0629_8.h5\n",
      ">>>LoadModel>>> /kaggle/input/cmi-d-111/v0629_9.h5\n",
      "--------------------------------------------------\n",
      "[INFO]NumUseModels:20\n"
     ]
    }
   ],
   "source": [
    "import os, json, joblib, numpy as np, pandas as pd\n",
    "import random\n",
    "from pathlib import Path\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from tensorflow.keras.utils import Sequence, to_categorical, pad_sequences\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Conv1D, BatchNormalization, Activation, add, MaxPooling1D, Dropout,\n",
    "    Bidirectional, LSTM, GlobalAveragePooling1D, Dense, Multiply, Reshape,\n",
    "    Lambda, Concatenate, GRU, GaussianNoise\n",
    ")\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow as tf\n",
    "import polars as pl\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "def seed_everything(seed):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    tf.experimental.numpy.random.seed(seed)\n",
    "    os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
    "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "\n",
    "seed_everything(seed=42)\n",
    "# (Competition metric will only be imported when TRAINing)\n",
    "TRAIN = False                     # ← set to True when you want to train\n",
    "RAW_DIR = Path(\"/kaggle/input/cmi-detect-behavior-with-sensor-data\")\n",
    "PRETRAINED_DIR = Path(\"/kaggle/input/cmi-d-111\")\n",
    "EXPORT_DIR = Path(\"./\")                                    # artefacts will be saved here\n",
    "BATCH_SIZE = 64\n",
    "PAD_PERCENTILE = 95\n",
    "LR_INIT = 5e-4\n",
    "WD = 3e-3\n",
    "MIXUP_ALPHA = 0.4\n",
    "EPOCHS = 160\n",
    "PATIENCE = 40\n",
    "\n",
    "print(\"▶ imports ready · tensorflow\", tf.__version__)\n",
    "\n",
    "#Tensor Manipulations\n",
    "def time_sum(x):\n",
    "    return K.sum(x, axis=1)\n",
    "\n",
    "def squeeze_last_axis(x):\n",
    "    return tf.squeeze(x, axis=-1)\n",
    "\n",
    "def expand_last_axis(x):\n",
    "    return tf.expand_dims(x, axis=-1)\n",
    "\n",
    "def se_block(x, reduction=8):\n",
    "    ch = x.shape[-1]\n",
    "    se = GlobalAveragePooling1D()(x)\n",
    "    se = Dense(ch // reduction, activation='relu')(se)\n",
    "    se = Dense(ch, activation='sigmoid')(se)\n",
    "    se = Reshape((1, ch))(se)\n",
    "    return Multiply()([x, se])\n",
    "\n",
    "# Residual CNN Block with SE\n",
    "def residual_se_cnn_block(x, filters, kernel_size, pool_size=2, drop=0.3, wd=1e-4):\n",
    "    shortcut = x\n",
    "    for _ in range(2):\n",
    "        x = Conv1D(filters, kernel_size, padding='same', use_bias=False,\n",
    "                   kernel_regularizer=l2(wd))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "    x = se_block(x)\n",
    "    if shortcut.shape[-1] != filters:\n",
    "        shortcut = Conv1D(filters, 1, padding='same', use_bias=False,\n",
    "                          kernel_regularizer=l2(wd))(shortcut)\n",
    "        shortcut = BatchNormalization()(shortcut)\n",
    "    x = add([x, shortcut])\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling1D(pool_size)(x)\n",
    "    x = Dropout(drop)(x)\n",
    "    return x\n",
    "\n",
    "def attention_layer(inputs):\n",
    "    score = Dense(1, activation='tanh')(inputs)\n",
    "    score = Lambda(squeeze_last_axis)(score)\n",
    "    weights = Activation('softmax')(score)\n",
    "    weights = Lambda(expand_last_axis)(weights)\n",
    "    context = Multiply()([inputs, weights])\n",
    "    context = Lambda(time_sum)(context)\n",
    "    return context\n",
    "\n",
    "# Normalizes and cleans the time series sequence. \n",
    "\n",
    "def preprocess_sequence(df_seq: pd.DataFrame, feature_cols: list[str], scaler: StandardScaler):\n",
    "    mat = df_seq[feature_cols].ffill().bfill().fillna(0).values\n",
    "    return scaler.transform(mat).astype('float32')\n",
    "\n",
    "# MixUp the data argumentation in order to regularize the neural network. \n",
    "\n",
    "class MixupGenerator(Sequence):\n",
    "    def __init__(self, X, y, batch_size, alpha=0.2):\n",
    "        self.X, self.y = X, y\n",
    "        self.batch = batch_size\n",
    "        self.alpha = alpha\n",
    "        self.indices = np.arange(len(X))\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.X) / self.batch))\n",
    "    def __getitem__(self, i):\n",
    "        idx = self.indices[i*self.batch:(i+1)*self.batch]\n",
    "        Xb, yb = self.X[idx], self.y[idx]\n",
    "        lam = np.random.beta(self.alpha, self.alpha)\n",
    "        perm = np.random.permutation(len(Xb))\n",
    "        X_mix = lam * Xb + (1-lam) * Xb[perm]\n",
    "        y_mix = lam * yb + (1-lam) * yb[perm]\n",
    "        return X_mix, y_mix\n",
    "    def on_epoch_end(self):\n",
    "        np.random.shuffle(self.indices)\n",
    "\n",
    "\n",
    "def remove_gravity_from_acc(acc_data, rot_data):\n",
    "\n",
    "    if isinstance(acc_data, pd.DataFrame):\n",
    "        acc_values = acc_data[['acc_x', 'acc_y', 'acc_z']].values\n",
    "    else:\n",
    "        acc_values = acc_data\n",
    "\n",
    "    if isinstance(rot_data, pd.DataFrame):\n",
    "        quat_values = rot_data[['rot_x', 'rot_y', 'rot_z', 'rot_w']].values\n",
    "    else:\n",
    "        quat_values = rot_data\n",
    "\n",
    "    num_samples = acc_values.shape[0]\n",
    "    linear_accel = np.zeros_like(acc_values)\n",
    "    \n",
    "    gravity_world = np.array([0, 0, 9.81])\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        if np.all(np.isnan(quat_values[i])) or np.all(np.isclose(quat_values[i], 0)):\n",
    "            linear_accel[i, :] = acc_values[i, :] \n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            rotation = R.from_quat(quat_values[i])\n",
    "            gravity_sensor_frame = rotation.apply(gravity_world, inverse=True)\n",
    "            linear_accel[i, :] = acc_values[i, :] - gravity_sensor_frame\n",
    "        except ValueError:\n",
    "             linear_accel[i, :] = acc_values[i, :]\n",
    "             \n",
    "    return linear_accel\n",
    "\n",
    "def calculate_angular_velocity_from_quat(rot_data, time_delta=1/200): # Assuming 200Hz sampling rate\n",
    "    if isinstance(rot_data, pd.DataFrame):\n",
    "        quat_values = rot_data[['rot_x', 'rot_y', 'rot_z', 'rot_w']].values\n",
    "    else:\n",
    "        quat_values = rot_data\n",
    "\n",
    "    num_samples = quat_values.shape[0]\n",
    "    angular_vel = np.zeros((num_samples, 3))\n",
    "\n",
    "    for i in range(num_samples - 1):\n",
    "        q_t = quat_values[i]\n",
    "        q_t_plus_dt = quat_values[i+1]\n",
    "\n",
    "        if np.all(np.isnan(q_t)) or np.all(np.isclose(q_t, 0)) or \\\n",
    "           np.all(np.isnan(q_t_plus_dt)) or np.all(np.isclose(q_t_plus_dt, 0)):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            rot_t = R.from_quat(q_t)\n",
    "            rot_t_plus_dt = R.from_quat(q_t_plus_dt)\n",
    "\n",
    "            # Calculate the relative rotation\n",
    "            delta_rot = rot_t.inv() * rot_t_plus_dt\n",
    "            \n",
    "            # Convert delta rotation to angular velocity vector\n",
    "            # The rotation vector (Euler axis * angle) scaled by 1/dt\n",
    "            # is a good approximation for small delta_rot\n",
    "            angular_vel[i, :] = delta_rot.as_rotvec() / time_delta\n",
    "        except ValueError:\n",
    "            # If quaternion is invalid, angular velocity remains zero\n",
    "            pass\n",
    "            \n",
    "    return angular_vel\n",
    "    \n",
    "def calculate_angular_distance(rot_data):\n",
    "    if isinstance(rot_data, pd.DataFrame):\n",
    "        quat_values = rot_data[['rot_x', 'rot_y', 'rot_z', 'rot_w']].values\n",
    "    else:\n",
    "        quat_values = rot_data\n",
    "\n",
    "    num_samples = quat_values.shape[0]\n",
    "    angular_dist = np.zeros(num_samples)\n",
    "\n",
    "    for i in range(num_samples - 1):\n",
    "        q1 = quat_values[i]\n",
    "        q2 = quat_values[i+1]\n",
    "\n",
    "        if np.all(np.isnan(q1)) or np.all(np.isclose(q1, 0)) or \\\n",
    "           np.all(np.isnan(q2)) or np.all(np.isclose(q2, 0)):\n",
    "            angular_dist[i] = 0 # Или np.nan, в зависимости от желаемого поведения\n",
    "            continue\n",
    "        try:\n",
    "            # Преобразование кватернионов в объекты Rotation\n",
    "            r1 = R.from_quat(q1)\n",
    "            r2 = R.from_quat(q2)\n",
    "\n",
    "            # Вычисление углового расстояния: 2 * arccos(|real(p * q*)|)\n",
    "            # где p* - сопряженный кватернион q\n",
    "            # В scipy.spatial.transform.Rotation, r1.inv() * r2 дает относительное вращение.\n",
    "            # Угол этого относительного вращения - это и есть угловое расстояние.\n",
    "            relative_rotation = r1.inv() * r2\n",
    "            \n",
    "            # Угол rotation vector соответствует угловому расстоянию\n",
    "            # Норма rotation vector - это угол в радианах\n",
    "            angle = np.linalg.norm(relative_rotation.as_rotvec())\n",
    "            angular_dist[i] = angle\n",
    "        except ValueError:\n",
    "            angular_dist[i] = 0 # В случае недействительных кватернионов\n",
    "            pass\n",
    "            \n",
    "    return angular_dist\n",
    "\n",
    "def build_two_branch_model(pad_len, imu_dim, tof_dim, n_classes, wd=1e-4):\n",
    "    inp = Input(shape=(pad_len, imu_dim+tof_dim))\n",
    "    imu = Lambda(lambda t: t[:, :, :imu_dim])(inp)\n",
    "    tof = Lambda(lambda t: t[:, :, imu_dim:])(inp)\n",
    "\n",
    "    # IMU deep branch\n",
    "    x1 = residual_se_cnn_block(imu, 64, 3, drop=0.1, wd=wd)\n",
    "    x1 = residual_se_cnn_block(x1, 128, 5, drop=0.1, wd=wd)\n",
    "\n",
    "    # TOF/Thermal lighter branch\n",
    "    x2 = Conv1D(64, 3, padding='same', use_bias=False, kernel_regularizer=l2(wd))(tof)\n",
    "    x2 = BatchNormalization()(x2); x2 = Activation('relu')(x2)\n",
    "    x2 = MaxPooling1D(2)(x2); x2 = Dropout(0.2)(x2)\n",
    "    x2 = Conv1D(128, 3, padding='same', use_bias=False, kernel_regularizer=l2(wd))(x2)\n",
    "    x2 = BatchNormalization()(x2); x2 = Activation('relu')(x2)\n",
    "    x2 = MaxPooling1D(2)(x2); x2 = Dropout(0.2)(x2)\n",
    "\n",
    "    merged = Concatenate()([x1, x2])\n",
    "\n",
    "    xa = Bidirectional(LSTM(128, return_sequences=True, kernel_regularizer=l2(wd)))(merged)\n",
    "    xb = Bidirectional(GRU(128, return_sequences=True, kernel_regularizer=l2(wd)))(merged)\n",
    "    xc = GaussianNoise(0.09)(merged)\n",
    "    xc = Dense(16, activation='elu')(xc)\n",
    "    \n",
    "    x = Concatenate()([xa, xb, xc])\n",
    "    x = Dropout(0.4)(x)\n",
    "    x = attention_layer(x)\n",
    "\n",
    "    for units, drop in [(256, 0.5), (128, 0.3)]:\n",
    "        x = Dense(units, use_bias=False, kernel_regularizer=l2(wd))(x)\n",
    "        x = BatchNormalization()(x); x = Activation('relu')(x)\n",
    "        x = Dropout(drop)(x)\n",
    "\n",
    "    out = Dense(n_classes, activation='softmax', kernel_regularizer=l2(wd))(x)\n",
    "    return Model(inp, out)\n",
    "\n",
    "tmp_model = build_two_branch_model(127,7,325,18)\n",
    "print(\"▶ INFERENCE MODE – loading artefacts from\", PRETRAINED_DIR)\n",
    "final_feature_cols = np.load(PRETRAINED_DIR / \"feature_cols.npy\", allow_pickle=True).tolist()\n",
    "pad_len        = int(np.load(PRETRAINED_DIR / \"sequence_maxlen.npy\"))\n",
    "scaler         = joblib.load(PRETRAINED_DIR / \"scaler.pkl\")\n",
    "gesture_classes = np.load(PRETRAINED_DIR / \"gesture_classes.npy\", allow_pickle=True)\n",
    "\n",
    "\n",
    "custom_objs = {\n",
    "    'time_sum': time_sum, 'squeeze_last_axis': squeeze_last_axis, 'expand_last_axis': expand_last_axis,\n",
    "    'se_block': se_block, 'residual_se_cnn_block': residual_se_cnn_block, 'attention_layer': attention_layer,\n",
    "}\n",
    "\n",
    "# ----------------------------------------------------------------- #\n",
    "# Load any Models\n",
    "# * is 2 Train Model Load\n",
    "# ----------------------------------------------------------------- #\n",
    "\n",
    "models1 = []\n",
    "print(f\"  Loading models for ensemble inference...\")\n",
    "for fold in range(10):\n",
    "    MODEL_DIR = \"/kaggle/input/cmi-d-111\"\n",
    "    \n",
    "    model_path = f\"{MODEL_DIR}/D-111_{fold}.h5\"\n",
    "    print(\">>>LoadModel>>>\",model_path)\n",
    "    model = load_model(model_path, compile=False, custom_objects=custom_objs)\n",
    "    models1.append(model)\n",
    "print(\"-\"*50)\n",
    "\n",
    "for fold in range(10):\n",
    "    MODEL_DIR = \"/kaggle/input/cmi-d-111\"\n",
    "    \n",
    "    model_path = f\"{MODEL_DIR}/v0629_{fold}.h5\"\n",
    "    print(\">>>LoadModel>>>\",model_path)\n",
    "    model = load_model(model_path, compile=False, custom_objects=custom_objs)\n",
    "    models1.append(model)\n",
    "print(\"-\"*50)\n",
    "print(f\"[INFO]NumUseModels:{len(models1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b487a047",
   "metadata": {
    "_cell_guid": "50488121-c655-4e6d-bb0d-d73414c98ed0",
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "ede06ff2-e2dc-4a14-a7b6-421a54b8bb9a",
    "execution": {
     "iopub.execute_input": "2025-08-18T05:37:02.898329Z",
     "iopub.status.busy": "2025-08-18T05:37:02.897470Z",
     "iopub.status.idle": "2025-08-18T05:37:02.905315Z",
     "shell.execute_reply": "2025-08-18T05:37:02.904703Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.016507,
     "end_time": "2025-08-18T05:37:02.906437",
     "exception": false,
     "start_time": "2025-08-18T05:37:02.889930",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# predict_1\n",
    "\n",
    "def predict1(sequence: pl.DataFrame, demographics: pl.DataFrame) -> str:\n",
    "    df_seq = sequence.to_pandas()\n",
    "    linear_accel = remove_gravity_from_acc(df_seq, df_seq)\n",
    "    df_seq['linear_acc_x'], df_seq['linear_acc_y'], df_seq['linear_acc_z'] = linear_accel[:, 0], linear_accel[:, 1], linear_accel[:, 2]\n",
    "    df_seq['linear_acc_mag'] = np.sqrt(df_seq['linear_acc_x']**2 + df_seq['linear_acc_y']**2 + df_seq['linear_acc_z']**2)\n",
    "    df_seq['linear_acc_mag_jerk'] = df_seq['linear_acc_mag'].diff().fillna(0)\n",
    "    angular_vel = calculate_angular_velocity_from_quat(df_seq)\n",
    "    df_seq['angular_vel_x'], df_seq['angular_vel_y'], df_seq['angular_vel_z'] = angular_vel[:, 0], angular_vel[:, 1], angular_vel[:, 2]\n",
    "    df_seq['angular_distance'] = calculate_angular_distance(df_seq)\n",
    "    \n",
    "    for i in range(1, 6):\n",
    "        pixel_cols = [f\"tof_{i}_v{p}\" for p in range(64)]; tof_data = df_seq[pixel_cols].replace(-1, np.nan)\n",
    "        df_seq[f'tof_{i}_mean'], df_seq[f'tof_{i}_std'], df_seq[f'tof_{i}_min'], df_seq[f'tof_{i}_max'] = tof_data.mean(axis=1), tof_data.std(axis=1), tof_data.min(axis=1), tof_data.max(axis=1)\n",
    "        \n",
    "    mat_unscaled = df_seq[final_feature_cols].ffill().bfill().fillna(0).values.astype('float32')\n",
    "    mat_scaled = scaler.transform(mat_unscaled)\n",
    "    pad_input = pad_sequences([mat_scaled], maxlen=pad_len, padding='post', truncating='post', dtype='float32')\n",
    "    \n",
    "    all_preds = [model.predict(pad_input, verbose=0)[0] for model in models1] # 主出力のみ取得\n",
    "    avg_pred = np.mean(all_preds, axis=0)\n",
    "    return avg_pred\n",
    "    # return str(gesture_classes[avg_pred.argmax()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d2ea7d",
   "metadata": {
    "_cell_guid": "7c4b4b2f-7a87-4bb7-a7d4-987209906dfe",
    "_uuid": "8ed75223-3db6-4998-b5d9-7d17bbea6fb3",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.006487,
     "end_time": "2025-08-18T05:37:02.919933",
     "exception": false,
     "start_time": "2025-08-18T05:37:02.913446",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "626a1f02",
   "metadata": {
    "_cell_guid": "0be9a02d-5c4b-4987-bb1f-e5ae13b4630e",
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "6468f507-f6fe-440a-a086-5d805e16ce84",
    "execution": {
     "iopub.execute_input": "2025-08-18T05:37:02.935772Z",
     "iopub.status.busy": "2025-08-18T05:37:02.935556Z",
     "iopub.status.idle": "2025-08-18T05:39:49.431106Z",
     "shell.execute_reply": "2025-08-18T05:39:49.430546Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 166.50604,
     "end_time": "2025-08-18T05:39:49.432628",
     "exception": false,
     "start_time": "2025-08-18T05:37:02.926588",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Have precomputed, skip compute.\n",
      "\n",
      "交叉验证折叠统计:\n",
      "\n",
      "Fold 1:\n",
      "类别                                                 训练集        验证集       \n",
      "Above ear - pull hair                              511        127       \n",
      "Cheek - pinch skin                                 509        128       \n",
      "Drink from bottle/cup                              129        32        \n",
      "Eyebrow - pull hair                                510        128       \n",
      "Eyelash - pull hair                                512        128       \n",
      "Feel around in tray and pull out an object         129        32        \n",
      "Forehead - pull hairline                           512        128       \n",
      "Forehead - scratch                                 512        128       \n",
      "Glasses on/off                                     128        33        \n",
      "Neck - pinch skin                                  512        128       \n",
      "Neck - scratch                                     512        128       \n",
      "Pinch knee/leg skin                                129        32        \n",
      "Pull air toward your face                          381        96        \n",
      "Scratch knee/leg skin                              129        32        \n",
      "Text on phone                                      512        128       \n",
      "Wave hello                                         382        96        \n",
      "Write name in air                                  382        95        \n",
      "Write name on leg                                  129        32        \n",
      "\n",
      "Fold 2:\n",
      "类别                                                 训练集        验证集       \n",
      "Above ear - pull hair                              511        127       \n",
      "Cheek - pinch skin                                 509        128       \n",
      "Drink from bottle/cup                              129        32        \n",
      "Eyebrow - pull hair                                510        128       \n",
      "Eyelash - pull hair                                512        128       \n",
      "Feel around in tray and pull out an object         129        32        \n",
      "Forehead - pull hairline                           512        128       \n",
      "Forehead - scratch                                 512        128       \n",
      "Glasses on/off                                     129        32        \n",
      "Neck - pinch skin                                  512        128       \n",
      "Neck - scratch                                     512        128       \n",
      "Pinch knee/leg skin                                129        32        \n",
      "Pull air toward your face                          381        96        \n",
      "Scratch knee/leg skin                              129        32        \n",
      "Text on phone                                      512        128       \n",
      "Wave hello                                         382        96        \n",
      "Write name in air                                  382        95        \n",
      "Write name on leg                                  129        32        \n",
      "\n",
      "Fold 3:\n",
      "类别                                                 训练集        验证集       \n",
      "Above ear - pull hair                              510        128       \n",
      "Cheek - pinch skin                                 510        127       \n",
      "Drink from bottle/cup                              129        32        \n",
      "Eyebrow - pull hair                                511        127       \n",
      "Eyelash - pull hair                                512        128       \n",
      "Feel around in tray and pull out an object         129        32        \n",
      "Forehead - pull hairline                           512        128       \n",
      "Forehead - scratch                                 512        128       \n",
      "Glasses on/off                                     129        32        \n",
      "Neck - pinch skin                                  512        128       \n",
      "Neck - scratch                                     512        128       \n",
      "Pinch knee/leg skin                                128        33        \n",
      "Pull air toward your face                          382        95        \n",
      "Scratch knee/leg skin                              129        32        \n",
      "Text on phone                                      512        128       \n",
      "Wave hello                                         382        96        \n",
      "Write name in air                                  382        95        \n",
      "Write name on leg                                  128        33        \n",
      "\n",
      "Fold 4:\n",
      "类别                                                 训练集        验证集       \n",
      "Above ear - pull hair                              510        128       \n",
      "Cheek - pinch skin                                 510        127       \n",
      "Drink from bottle/cup                              129        32        \n",
      "Eyebrow - pull hair                                511        127       \n",
      "Eyelash - pull hair                                512        128       \n",
      "Feel around in tray and pull out an object         128        33        \n",
      "Forehead - pull hairline                           512        128       \n",
      "Forehead - scratch                                 512        128       \n",
      "Glasses on/off                                     129        32        \n",
      "Neck - pinch skin                                  512        128       \n",
      "Neck - scratch                                     512        128       \n",
      "Pinch knee/leg skin                                129        32        \n",
      "Pull air toward your face                          382        95        \n",
      "Scratch knee/leg skin                              128        33        \n",
      "Text on phone                                      512        128       \n",
      "Wave hello                                         383        95        \n",
      "Write name in air                                  381        96        \n",
      "Write name on leg                                  129        32        \n",
      "\n",
      "Fold 5:\n",
      "类别                                                 训练集        验证集       \n",
      "Above ear - pull hair                              510        128       \n",
      "Cheek - pinch skin                                 510        127       \n",
      "Drink from bottle/cup                              128        33        \n",
      "Eyebrow - pull hair                                510        128       \n",
      "Eyelash - pull hair                                512        128       \n",
      "Feel around in tray and pull out an object         129        32        \n",
      "Forehead - pull hairline                           512        128       \n",
      "Forehead - scratch                                 512        128       \n",
      "Glasses on/off                                     129        32        \n",
      "Neck - pinch skin                                  512        128       \n",
      "Neck - scratch                                     512        128       \n",
      "Pinch knee/leg skin                                129        32        \n",
      "Pull air toward your face                          382        95        \n",
      "Scratch knee/leg skin                              129        32        \n",
      "Text on phone                                      512        128       \n",
      "Wave hello                                         383        95        \n",
      "Write name in air                                  381        96        \n",
      "Write name on leg                                  129        32        \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import kagglehub\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.amp import autocast\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from transformers import BertConfig, BertModel\n",
    "\n",
    "\n",
    "def remove_gravity_from_acc(acc_data, rot_data):\n",
    "    if isinstance(acc_data, pd.DataFrame):\n",
    "        acc_values = acc_data[['acc_x', 'acc_y', 'acc_z']].values\n",
    "    else:\n",
    "        acc_values = acc_data\n",
    "    if isinstance(rot_data, pd.DataFrame):\n",
    "        quat_values = rot_data[['rot_x', 'rot_y', 'rot_z', 'rot_w']].values\n",
    "    else:\n",
    "        quat_values = rot_data\n",
    "    num_samples = acc_values.shape[0]\n",
    "    linear_accel = np.zeros_like(acc_values)\n",
    "    gravity_world = np.array([0, 0, 9.81])\n",
    "    for i in range(num_samples):\n",
    "        if np.all(np.isnan(quat_values[i])) or np.all(np.isclose(quat_values[i], 0)):\n",
    "            linear_accel[i, :] = acc_values[i, :] \n",
    "            continue\n",
    "        try:\n",
    "            rotation = R.from_quat(quat_values[i])\n",
    "            gravity_sensor_frame = rotation.apply(gravity_world, inverse=True)\n",
    "            linear_accel[i, :] = acc_values[i, :] - gravity_sensor_frame\n",
    "        except ValueError:\n",
    "             linear_accel[i, :] = acc_values[i, :]\n",
    "    return linear_accel\n",
    "\n",
    "def calculate_angular_velocity_from_quat(rot_data, time_delta=1/200): # Assuming 200Hz sampling rate\n",
    "    if isinstance(rot_data, pd.DataFrame):\n",
    "        quat_values = rot_data[['rot_x', 'rot_y', 'rot_z', 'rot_w']].values\n",
    "    else:\n",
    "        quat_values = rot_data\n",
    "    num_samples = quat_values.shape[0]\n",
    "    angular_vel = np.zeros((num_samples, 3))\n",
    "    for i in range(num_samples - 1):\n",
    "        q_t = quat_values[i]\n",
    "        q_t_plus_dt = quat_values[i+1]\n",
    "        if np.all(np.isnan(q_t)) or np.all(np.isclose(q_t, 0)) or \\\n",
    "           np.all(np.isnan(q_t_plus_dt)) or np.all(np.isclose(q_t_plus_dt, 0)):\n",
    "            continue\n",
    "        try:\n",
    "            rot_t = R.from_quat(q_t)\n",
    "            rot_t_plus_dt = R.from_quat(q_t_plus_dt)\n",
    "            delta_rot = rot_t.inv() * rot_t_plus_dt\n",
    "            angular_vel[i, :] = delta_rot.as_rotvec() / time_delta\n",
    "        except ValueError:\n",
    "            pass\n",
    "    return angular_vel\n",
    "\n",
    "def calculate_angular_distance(rot_data):\n",
    "    if isinstance(rot_data, pd.DataFrame):\n",
    "        quat_values = rot_data[['rot_x', 'rot_y', 'rot_z', 'rot_w']].values\n",
    "    else:\n",
    "        quat_values = rot_data\n",
    "    num_samples = quat_values.shape[0]\n",
    "    angular_dist = np.zeros(num_samples)\n",
    "    for i in range(num_samples - 1):\n",
    "        q1 = quat_values[i]\n",
    "        q2 = quat_values[i+1]\n",
    "        if np.all(np.isnan(q1)) or np.all(np.isclose(q1, 0)) or \\\n",
    "           np.all(np.isnan(q2)) or np.all(np.isclose(q2, 0)):\n",
    "            angular_dist[i] = 0\n",
    "            continue\n",
    "        try:\n",
    "            r1 = R.from_quat(q1)\n",
    "            r2 = R.from_quat(q2)\n",
    "            relative_rotation = r1.inv() * r2\n",
    "            angle = np.linalg.norm(relative_rotation.as_rotvec())\n",
    "            angular_dist[i] = angle\n",
    "        except ValueError:\n",
    "            angular_dist[i] = 0 # В случае недействительных кватернионов\n",
    "            pass\n",
    "    return angular_dist\n",
    "\n",
    "\n",
    "class CMIFeDataset(Dataset):\n",
    "    def __init__(self, data_path, config):\n",
    "        self.config = config\n",
    "        self.init_feature_names(data_path)\n",
    "        df = self.generate_features(pd.read_csv(data_path, usecols=set(self.base_cols+self.feature_cols)))\n",
    "        self.generate_dataset(df)\n",
    "\n",
    "    def init_feature_names(self, data_path):\n",
    "        self.imu_engineered_features = [\n",
    "            'acc_mag', 'rot_angle',\n",
    "            'acc_mag_jerk', 'rot_angle_vel',\n",
    "            'linear_acc_mag', 'linear_acc_mag_jerk',\n",
    "            'angular_vel_x', 'angular_vel_y', 'angular_vel_z',\n",
    "            'angular_distance'\n",
    "        ]\n",
    "\n",
    "        self.tof_mode = self.config.get(\"tof_mode\", \"stats\")\n",
    "        self.tof_region_stats = ['mean', 'std', 'min', 'max']\n",
    "        self.tof_cols = self.generate_tof_feature_names()\n",
    "\n",
    "        columns = pd.read_csv(data_path, nrows=0).columns.tolist()\n",
    "        imu_cols_base = ['linear_acc_x', 'linear_acc_y', 'linear_acc_z']\n",
    "        imu_cols_base.extend([c for c in columns if c.startswith('rot_') and c not in ['rot_angle', 'rot_angle_vel']])\n",
    "        self.imu_cols = list(dict.fromkeys(imu_cols_base + self.imu_engineered_features))\n",
    "        self.thm_cols = [c for c in columns if c.startswith('thm_')]\n",
    "        self.feature_cols = self.imu_cols + self.thm_cols + self.tof_cols\n",
    "        self.imu_dim = len(self.imu_cols)\n",
    "        self.thm_dim = len(self.thm_cols)\n",
    "        self.tof_dim = len(self.tof_cols)\n",
    "        self.base_cols = ['acc_x', 'acc_y', 'acc_z',\n",
    "                          'rot_x', 'rot_y', 'rot_z', 'rot_w',\n",
    "                          'sequence_id', 'subject', \n",
    "                          'sequence_type', 'gesture', 'orientation'] + [c for c in columns if c.startswith('thm_')] + [f\"tof_{i}_v{p}\" for i in range(1, 6) for p in range(64)]\n",
    "        self.fold_cols = ['subject', 'sequence_type', 'gesture', 'orientation']\n",
    "\n",
    "    def generate_tof_feature_names(self):\n",
    "        features = []\n",
    "        if self.config.get(\"tof_raw\", False):\n",
    "            for i in range(1, 6):\n",
    "                features.extend([f\"tof_{i}_v{p}\" for p in range(64)])\n",
    "        for i in range(1, 6):\n",
    "            if self.tof_mode != 0:\n",
    "                for stat in self.tof_region_stats:\n",
    "                    features.append(f'tof_{i}_{stat}')\n",
    "                if self.tof_mode > 1:\n",
    "                    for r in range(self.tof_mode):\n",
    "                        for stat in self.tof_region_stats:\n",
    "                            features.append(f'tof{self.tof_mode}_{i}_region_{r}_{stat}')\n",
    "                if self.tof_mode == -1:\n",
    "                    for mode in [2, 4, 8, 16, 32]:\n",
    "                        for r in range(mode):\n",
    "                            for stat in self.tof_region_stats:\n",
    "                                features.append(f'tof{mode}_{i}_region_{r}_{stat}')\n",
    "        return features\n",
    "\n",
    "    def compute_features(self, df):\n",
    "        df['acc_mag'] = np.sqrt(df['acc_x']**2 + df['acc_y']**2 + df['acc_z']**2)\n",
    "        df['rot_angle'] = 2 * np.arccos(df['rot_w'].clip(-1, 1))\n",
    "        df['acc_mag_jerk'] = df.groupby('sequence_id')['acc_mag'].diff().fillna(0)\n",
    "        df['rot_angle_vel'] = df.groupby('sequence_id')['rot_angle'].diff().fillna(0)\n",
    "            \n",
    "        linear_accel_list = []\n",
    "        for _, group in df.groupby('sequence_id'):\n",
    "            acc_data_group = group[['acc_x', 'acc_y', 'acc_z']]\n",
    "            rot_data_group = group[['rot_x', 'rot_y', 'rot_z', 'rot_w']]\n",
    "            linear_accel_group = remove_gravity_from_acc(acc_data_group, rot_data_group)\n",
    "            linear_accel_list.append(pd.DataFrame(linear_accel_group, columns=['linear_acc_x', 'linear_acc_y', 'linear_acc_z'], index=group.index))\n",
    "        df_linear_accel = pd.concat(linear_accel_list)\n",
    "        df = pd.concat([df, df_linear_accel], axis=1)\n",
    "        df['linear_acc_mag'] = np.sqrt(df['linear_acc_x']**2 + df['linear_acc_y']**2 + df['linear_acc_z']**2)\n",
    "        df['linear_acc_mag_jerk'] = df.groupby('sequence_id')['linear_acc_mag'].diff().fillna(0)\n",
    "    \n",
    "        angular_vel_list = []\n",
    "        for _, group in df.groupby('sequence_id'):\n",
    "            rot_data_group = group[['rot_x', 'rot_y', 'rot_z', 'rot_w']]\n",
    "            angular_vel_group = calculate_angular_velocity_from_quat(rot_data_group)\n",
    "            angular_vel_list.append(pd.DataFrame(angular_vel_group, columns=['angular_vel_x', 'angular_vel_y', 'angular_vel_z'], index=group.index))\n",
    "        df_angular_vel = pd.concat(angular_vel_list)\n",
    "        df = pd.concat([df, df_angular_vel], axis=1)\n",
    "    \n",
    "        angular_distance_list = []\n",
    "        for _, group in df.groupby('sequence_id'):\n",
    "            rot_data_group = group[['rot_x', 'rot_y', 'rot_z', 'rot_w']]\n",
    "            angular_dist_group = calculate_angular_distance(rot_data_group)\n",
    "            angular_distance_list.append(pd.DataFrame(angular_dist_group, columns=['angular_distance'], index=group.index))\n",
    "        df_angular_distance = pd.concat(angular_distance_list)\n",
    "        df = pd.concat([df, df_angular_distance], axis=1)\n",
    "\n",
    "        if self.tof_mode != 0:\n",
    "            new_columns = {}\n",
    "            for i in range(1, 6):\n",
    "                pixel_cols = [f\"tof_{i}_v{p}\" for p in range(64)]\n",
    "                tof_data = df[pixel_cols].replace(-1, np.nan)\n",
    "                new_columns.update({\n",
    "                    f'tof_{i}_mean': tof_data.mean(axis=1),\n",
    "                    f'tof_{i}_std': tof_data.std(axis=1),\n",
    "                    f'tof_{i}_min': tof_data.min(axis=1),\n",
    "                    f'tof_{i}_max': tof_data.max(axis=1)\n",
    "                })\n",
    "                if self.tof_mode > 1:\n",
    "                    region_size = 64 // self.tof_mode\n",
    "                    for r in range(self.tof_mode):\n",
    "                        region_data = tof_data.iloc[:, r*region_size : (r+1)*region_size]\n",
    "                        new_columns.update({\n",
    "                            f'tof{self.tof_mode}_{i}_region_{r}_mean': region_data.mean(axis=1),\n",
    "                            f'tof{self.tof_mode}_{i}_region_{r}_std': region_data.std(axis=1),\n",
    "                            f'tof{self.tof_mode}_{i}_region_{r}_min': region_data.min(axis=1),\n",
    "                            f'tof{self.tof_mode}_{i}_region_{r}_max': region_data.max(axis=1)\n",
    "                        })\n",
    "                if self.tof_mode == -1:\n",
    "                    for mode in [2, 4, 8, 16, 32]:\n",
    "                        region_size = 64 // mode\n",
    "                        for r in range(mode):\n",
    "                            region_data = tof_data.iloc[:, r*region_size : (r+1)*region_size]\n",
    "                            new_columns.update({\n",
    "                                f'tof{mode}_{i}_region_{r}_mean': region_data.mean(axis=1),\n",
    "                                f'tof{mode}_{i}_region_{r}_std': region_data.std(axis=1),\n",
    "                                f'tof{mode}_{i}_region_{r}_min': region_data.min(axis=1),\n",
    "                                f'tof{mode}_{i}_region_{r}_max': region_data.max(axis=1)\n",
    "                            })\n",
    "            df = pd.concat([df, pd.DataFrame(new_columns)], axis=1)\n",
    "        return df\n",
    "        \n",
    "    def generate_features(self, df):\n",
    "        self.le = LabelEncoder()\n",
    "        df['gesture_int'] = self.le.fit_transform(df['gesture'])\n",
    "        self.class_num = len(self.le.classes_)\n",
    "        \n",
    "        if all(c in df.columns for c in self.imu_engineered_features) and all(c in df.columns for c in self.tof_cols):\n",
    "            print(\"Have precomputed, skip compute.\")\n",
    "        else:\n",
    "            print(\"Not precomputed, do compute.\")\n",
    "            df = self.compute_features(df)\n",
    "\n",
    "        if self.config.get(\"save_precompute\", False):\n",
    "            df.to_csv(self.config.get(\"save_filename\", \"train.csv\"))\n",
    "        return df\n",
    "\n",
    "    def scale(self, data_unscaled):\n",
    "        scaler_function = self.config.get(\"scaler_function\", StandardScaler())\n",
    "        scaler = scaler_function.fit(np.concatenate(data_unscaled, axis=0))\n",
    "        return [scaler.transform(x) for x in data_unscaled], scaler\n",
    "\n",
    "    def pad(self, data_scaled, cols):\n",
    "        pad_data = np.zeros((len(data_scaled), self.pad_len, len(cols)), dtype='float32')\n",
    "        for i, seq in enumerate(data_scaled):\n",
    "            seq_len = min(len(seq), self.pad_len)\n",
    "            pad_data[i, :seq_len] = seq[:seq_len]\n",
    "        return pad_data\n",
    "\n",
    "    def get_nan_value(self, data, ratio):\n",
    "        max_value = data.max().max()\n",
    "        nan_value = -max_value * ratio\n",
    "        return nan_value\n",
    "\n",
    "    def generate_dataset(self, df):\n",
    "        seq_gp = df.groupby('sequence_id') \n",
    "        imu_unscaled, thm_unscaled, tof_unscaled = [], [], []\n",
    "        classes, lens = [], []\n",
    "        self.imu_nan_value = self.get_nan_value(df[self.imu_cols], self.config[\"nan_ratio\"][\"imu\"])\n",
    "        self.thm_nan_value = self.get_nan_value(df[self.thm_cols], self.config[\"nan_ratio\"][\"thm\"])\n",
    "        self.tof_nan_value = self.get_nan_value(df[self.tof_cols], self.config[\"nan_ratio\"][\"tof\"])\n",
    "\n",
    "        self.fold_feats = defaultdict(list)\n",
    "        for seq_id, seq_df in seq_gp:\n",
    "            imu_data = seq_df[self.imu_cols]\n",
    "            if self.config[\"fbfill\"][\"imu\"]:\n",
    "                imu_data = imu_data.ffill().bfill()\n",
    "            imu_unscaled.append(imu_data.fillna(self.imu_nan_value).values.astype('float32'))\n",
    "\n",
    "            thm_data = seq_df[self.thm_cols]\n",
    "            if self.config[\"fbfill\"][\"thm\"]:\n",
    "                thm_data = thm_data.ffill().bfill()\n",
    "            thm_unscaled.append(thm_data.fillna(self.thm_nan_value).values.astype('float32'))\n",
    "\n",
    "            tof_data = seq_df[self.tof_cols]\n",
    "            if self.config[\"fbfill\"][\"tof\"]:\n",
    "                tof_data = tof_data.ffill().bfill()\n",
    "            tof_unscaled.append(tof_data.fillna(self.tof_nan_value).values.astype('float32'))\n",
    "            \n",
    "            classes.append(seq_df['gesture_int'].iloc[0])\n",
    "            lens.append(len(imu_data))\n",
    "\n",
    "            for col in self.fold_cols:\n",
    "                self.fold_feats[col].append(seq_df[col].iloc[0])\n",
    "            \n",
    "        self.dataset_indices = classes\n",
    "        self.pad_len = int(np.percentile(lens, self.config.get(\"percent\", 95)))\n",
    "        if self.config.get(\"one_scale\", True):\n",
    "            x_unscaled = [np.concatenate([imu, thm, tof], axis=1) for imu, thm, tof in zip(imu_unscaled, thm_unscaled, tof_unscaled)]\n",
    "            x_scaled, self.x_scaler = self.scale(x_unscaled)\n",
    "            x = self.pad(x_scaled, self.imu_cols+self.thm_cols+self.tof_cols)\n",
    "            self.imu = x[..., :self.imu_dim]\n",
    "            self.thm = x[..., self.imu_dim:self.imu_dim+self.thm_dim]\n",
    "            self.tof = x[..., self.imu_dim+self.thm_dim:self.imu_dim+self.thm_dim+self.tof_dim]\n",
    "        else:\n",
    "            imu_scaled, self.imu_scaler = self.scale(imu_unscaled)\n",
    "            thm_scaled, self.thm_scaler = self.scale(thm_unscaled)\n",
    "            tof_scaled, self.tof_scaler = self.scale(tof_unscaled)\n",
    "            self.imu = self.pad(imu_scaled, self.imu_cols)\n",
    "            self.thm = self.pad(thm_scaled, self.thm_cols)\n",
    "            self.tof = self.pad(tof_scaled, self.tof_cols)\n",
    "        self.precompute_scaled_nan_values()\n",
    "        self.class_ = F.one_hot(torch.from_numpy(np.array(classes)).long(), num_classes=len(self.le.classes_)).float().numpy()\n",
    "        self.class_weight = torch.FloatTensor(compute_class_weight('balanced', classes=np.arange(len(self.le.classes_)), y=classes))\n",
    "\n",
    "    def precompute_scaled_nan_values(self):\n",
    "        dummy_df = pd.DataFrame(\n",
    "            np.array([[self.imu_nan_value]*len(self.imu_cols) + \n",
    "                     [self.thm_nan_value]*len(self.thm_cols) +\n",
    "                     [self.tof_nan_value]*len(self.tof_cols)]),\n",
    "            columns=self.imu_cols + self.thm_cols + self.tof_cols\n",
    "        )\n",
    "        \n",
    "        if self.config.get(\"one_scale\", True):\n",
    "            scaled = self.x_scaler.transform(dummy_df)\n",
    "            self.imu_scaled_nan = scaled[0, :self.imu_dim].mean()\n",
    "            self.thm_scaled_nan = scaled[0, self.imu_dim:self.imu_dim+self.thm_dim].mean()\n",
    "            self.tof_scaled_nan = scaled[0, self.imu_dim+self.thm_dim:self.imu_dim+self.thm_dim+self.tof_dim].mean()\n",
    "        else:\n",
    "            self.imu_scaled_nan = self.imu_scaler.transform(dummy_df[self.imu_cols])[0].mean()\n",
    "            self.thm_scaled_nan = self.thm_scaler.transform(dummy_df[self.thm_cols])[0].mean()\n",
    "            self.tof_scaled_nan = self.tof_scaler.transform(dummy_df[self.tof_cols])[0].mean()\n",
    "\n",
    "    def get_scaled_nan_tensors(self, imu, thm, tof):\n",
    "        return torch.full(imu.shape, self.imu_scaled_nan, device=imu.device), \\\n",
    "            torch.full(thm.shape, self.thm_scaled_nan, device=thm.device), \\\n",
    "            torch.full(tof.shape, self.tof_scaled_nan, device=tof.device)\n",
    "\n",
    "    def inference_process(self, sequence):\n",
    "        df_seq = sequence.to_pandas().copy()\n",
    "        if not all(c in df_seq.columns for c in self.imu_engineered_features):\n",
    "            df_seq['acc_mag'] = np.sqrt(df_seq['acc_x']**2 + df_seq['acc_y']**2 + df_seq['acc_z']**2)\n",
    "            df_seq['rot_angle'] = 2 * np.arccos(df_seq['rot_w'].clip(-1, 1))\n",
    "            df_seq['acc_mag_jerk'] = df_seq['acc_mag'].diff().fillna(0)\n",
    "            df_seq['rot_angle_vel'] = df_seq['rot_angle'].diff().fillna(0)\n",
    "            if all(col in df_seq.columns for col in ['acc_x', 'acc_y', 'acc_z', 'rot_x', 'rot_y', 'rot_z', 'rot_w']):\n",
    "                linear_accel = remove_gravity_from_acc(\n",
    "                    df_seq[['acc_x', 'acc_y', 'acc_z']], \n",
    "                    df_seq[['rot_x', 'rot_y', 'rot_z', 'rot_w']]\n",
    "                )\n",
    "                df_seq[['linear_acc_x', 'linear_acc_y', 'linear_acc_z']] = linear_accel\n",
    "            else:\n",
    "                df_seq['linear_acc_x'] = df_seq.get('acc_x', 0)\n",
    "                df_seq['linear_acc_y'] = df_seq.get('acc_y', 0)\n",
    "                df_seq['linear_acc_z'] = df_seq.get('acc_z', 0)\n",
    "            df_seq['linear_acc_mag'] = np.sqrt(df_seq['linear_acc_x']**2 + df_seq['linear_acc_y']**2 + df_seq['linear_acc_z']**2)\n",
    "            df_seq['linear_acc_mag_jerk'] = df_seq['linear_acc_mag'].diff().fillna(0)\n",
    "            if all(col in df_seq.columns for col in ['rot_x', 'rot_y', 'rot_z', 'rot_w']):\n",
    "                angular_vel = calculate_angular_velocity_from_quat(df_seq[['rot_x', 'rot_y', 'rot_z', 'rot_w']])\n",
    "                df_seq[['angular_vel_x', 'angular_vel_y', 'angular_vel_z']] = angular_vel\n",
    "            else:\n",
    "                df_seq[['angular_vel_x', 'angular_vel_y', 'angular_vel_z']] = 0\n",
    "            if all(col in df_seq.columns for col in ['rot_x', 'rot_y', 'rot_z', 'rot_w']):\n",
    "                df_seq['angular_distance'] = calculate_angular_distance(df_seq[['rot_x', 'rot_y', 'rot_z', 'rot_w']])\n",
    "            else:\n",
    "                df_seq['angular_distance'] = 0\n",
    "\n",
    "        if self.tof_mode != 0:\n",
    "            new_columns = {} \n",
    "            for i in range(1, 6):\n",
    "                pixel_cols = [f\"tof_{i}_v{p}\" for p in range(64)]\n",
    "                tof_data = df_seq[pixel_cols].replace(-1, np.nan)\n",
    "                new_columns.update({\n",
    "                    f'tof_{i}_mean': tof_data.mean(axis=1),\n",
    "                    f'tof_{i}_std': tof_data.std(axis=1),\n",
    "                    f'tof_{i}_min': tof_data.min(axis=1),\n",
    "                    f'tof_{i}_max': tof_data.max(axis=1)\n",
    "                })\n",
    "                if self.tof_mode > 1:\n",
    "                    region_size = 64 // self.tof_mode\n",
    "                    for r in range(self.tof_mode):\n",
    "                        region_data = tof_data.iloc[:, r*region_size : (r+1)*region_size]\n",
    "                        new_columns.update({\n",
    "                            f'tof{self.tof_mode}_{i}_region_{r}_mean': region_data.mean(axis=1),\n",
    "                            f'tof{self.tof_mode}_{i}_region_{r}_std': region_data.std(axis=1),\n",
    "                            f'tof{self.tof_mode}_{i}_region_{r}_min': region_data.min(axis=1),\n",
    "                            f'tof{self.tof_mode}_{i}_region_{r}_max': region_data.max(axis=1)\n",
    "                        })\n",
    "                if self.tof_mode == -1:\n",
    "                    for mode in [2, 4, 8, 16, 32]:\n",
    "                        region_size = 64 // mode\n",
    "                        for r in range(mode):\n",
    "                            region_data = tof_data.iloc[:, r*region_size : (r+1)*region_size]\n",
    "                            new_columns.update({\n",
    "                                f'tof{mode}_{i}_region_{r}_mean': region_data.mean(axis=1),\n",
    "                                f'tof{mode}_{i}_region_{r}_std': region_data.std(axis=1),\n",
    "                                f'tof{mode}_{i}_region_{r}_min': region_data.min(axis=1),\n",
    "                                f'tof{mode}_{i}_region_{r}_max': region_data.max(axis=1)\n",
    "                            })\n",
    "            df_seq = pd.concat([df_seq, pd.DataFrame(new_columns)], axis=1)\n",
    "        \n",
    "        imu_unscaled = df_seq[self.imu_cols]\n",
    "        if self.config[\"fbfill\"][\"imu\"]:\n",
    "            imu_unscaled = imu_unscaled.ffill().bfill()\n",
    "        imu_unscaled = imu_unscaled.fillna(self.imu_nan_value).values.astype('float32')\n",
    "\n",
    "        thm_unscaled = df_seq[self.thm_cols]\n",
    "        if self.config[\"fbfill\"][\"thm\"]:\n",
    "            thm_unscaled = thm_unscaled.ffill().bfill()\n",
    "        thm_unscaled = thm_unscaled.fillna(self.thm_nan_value).values.astype('float32')\n",
    "\n",
    "        tof_unscaled = df_seq[self.tof_cols]\n",
    "        if self.config[\"fbfill\"][\"tof\"]:\n",
    "            tof_unscaled = tof_unscaled.ffill().bfill()\n",
    "        tof_unscaled = tof_unscaled.fillna(self.tof_nan_value).values.astype('float32')\n",
    "        \n",
    "        if self.config.get(\"one_scale\", True):\n",
    "            x_unscaled = np.concatenate([imu_unscaled, thm_unscaled, tof_unscaled], axis=1)\n",
    "            x_scaled = self.x_scaler.transform(x_unscaled)\n",
    "            imu_scaled = x_scaled[..., :self.imu_dim]\n",
    "            thm_scaled = x_scaled[..., self.imu_dim:self.imu_dim+self.thm_dim]\n",
    "            tof_scaled = x_scaled[..., self.imu_dim+self.thm_dim:self.imu_dim+self.thm_dim+self.tof_dim]\n",
    "        else:\n",
    "            imu_scaled = self.imu_scaler.transform(imu_unscaled)\n",
    "            thm_scaled = self.thm_scaler.transform(thm_unscaled)\n",
    "            tof_scaled = self.tof_scaler.transform(tof_unscaled)\n",
    "\n",
    "        combined = np.concatenate([imu_scaled, thm_scaled, tof_scaled], axis=1)\n",
    "        padded = np.zeros((self.pad_len, combined.shape[1]), dtype='float32')\n",
    "        seq_len = min(combined.shape[0], self.pad_len)\n",
    "        padded[:seq_len] = combined[:seq_len]\n",
    "        imu = padded[..., :self.imu_dim]\n",
    "        thm = padded[..., self.imu_dim:self.imu_dim+self.thm_dim]\n",
    "        tof = padded[..., self.imu_dim+self.thm_dim:self.imu_dim+self.thm_dim+self.tof_dim]\n",
    "        \n",
    "        return torch.from_numpy(imu).float().unsqueeze(0), torch.from_numpy(thm).float().unsqueeze(0), torch.from_numpy(tof).float().unsqueeze(0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.imu[idx], self.thm[idx], self.tof[idx], self.class_[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.class_)\n",
    "\n",
    "class CMIFoldDataset:\n",
    "    def __init__(self, data_path, config, full_dataset_function, n_folds=5, random_seed=0):\n",
    "        self.full_dataset = full_dataset_function(data_path=data_path, config=config)\n",
    "        self.imu_dim = self.full_dataset.imu_dim\n",
    "        self.thm_dim = self.full_dataset.thm_dim\n",
    "        self.tof_dim = self.full_dataset.tof_dim\n",
    "        self.le = self.full_dataset.le\n",
    "        self.class_names = self.full_dataset.le.classes_\n",
    "        self.class_weight = self.full_dataset.class_weight\n",
    "        self.n_folds = n_folds\n",
    "        self.skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=random_seed)\n",
    "        self.folds = list(self.skf.split(np.arange(len(self.full_dataset)), np.array(self.full_dataset.dataset_indices)))\n",
    "    \n",
    "    def get_fold_datasets(self, fold_idx):\n",
    "        if self.folds is None or fold_idx >= self.n_folds:\n",
    "            return None, None\n",
    "        fold_train_idx, fold_valid_idx = self.folds[fold_idx]\n",
    "        return Subset(self.full_dataset, fold_train_idx), Subset(self.full_dataset, fold_valid_idx)\n",
    "\n",
    "    def print_fold_stats(self):\n",
    "        def get_label_counts(subset):\n",
    "            counts = {name: 0 for name in self.class_names}\n",
    "            if subset is None:\n",
    "                return counts\n",
    "            for idx in subset.indices:\n",
    "                label_idx = self.full_dataset.dataset_indices[idx]\n",
    "                counts[self.class_names[label_idx]] += 1\n",
    "            return counts\n",
    "        \n",
    "        print(\"\\n交叉验证折叠统计:\")\n",
    "        for fold_idx in range(self.n_folds):\n",
    "            train_fold, valid_fold = self.get_fold_datasets(fold_idx)\n",
    "            train_counts = get_label_counts(train_fold)\n",
    "            valid_counts = get_label_counts(valid_fold)\n",
    "                \n",
    "            print(f\"\\nFold {fold_idx + 1}:\")\n",
    "            print(f\"{'类别':<50} {'训练集':<10} {'验证集':<10}\")\n",
    "            for name in self.class_names:\n",
    "                print(f\"{name:<50} {train_counts[name]:<10} {valid_counts[name]:<10}\")\n",
    "\n",
    "\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, channels, reduction = 8):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(channels, channels // reduction, bias=True)\n",
    "        self.fc2 = nn.Linear(channels // reduction, channels, bias=True)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, C, L)\n",
    "        se = F.adaptive_avg_pool1d(x, 1).squeeze(-1)      # -> (B, C)\n",
    "        se = F.relu(self.fc1(se), inplace=True)          # -> (B, C//r)\n",
    "        se = self.sigmoid(self.fc2(se)).unsqueeze(-1)    # -> (B, C, 1)\n",
    "        return x * se                \n",
    "\n",
    "class ResNetSEBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, wd = 1e-4):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels,\n",
    "                               kernel_size=3, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels,\n",
    "                               kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "        # SE\n",
    "        self.se = SEBlock(out_channels)\n",
    "        \n",
    "        if in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels, kernel_size=1,\n",
    "                          padding=0, bias=False),\n",
    "                nn.BatchNorm1d(out_channels)\n",
    "            )\n",
    "        else:\n",
    "            self.shortcut = nn.Identity()\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x) :\n",
    "        identity = self.shortcut(x)              # (B, out, L)\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out = self.se(out)                       # (B, out, L)\n",
    "        out = out + identity\n",
    "        return self.relu(out)\n",
    "\n",
    "class CMIModel(nn.Module):\n",
    "    def __init__(self, imu_dim, thm_dim, tof_dim, n_classes, **kwargs):\n",
    "        super().__init__()\n",
    "        self.imu_branch = nn.Sequential(\n",
    "            self.residual_se_cnn_block(imu_dim, kwargs[\"imu1_channels\"], kwargs[\"imu1_layers\"],\n",
    "                                       drop=kwargs[\"imu1_dropout\"]),\n",
    "            self.residual_se_cnn_block(kwargs[\"imu1_channels\"], kwargs[\"feat_dim\"], kwargs[\"imu2_layers\"],\n",
    "                                       drop=kwargs[\"imu2_dropout\"])\n",
    "        )\n",
    "\n",
    "        self.thm_branch = nn.Sequential(\n",
    "            nn.Conv1d(thm_dim, kwargs[\"thm1_channels\"], kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm1d(kwargs[\"thm1_channels\"]),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool1d(2, ceil_mode=True),\n",
    "            nn.Dropout(kwargs[\"thm1_dropout\"]),\n",
    "            \n",
    "            nn.Conv1d(kwargs[\"thm1_channels\"], kwargs[\"feat_dim\"], kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm1d(kwargs[\"feat_dim\"]),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool1d(2, ceil_mode=True),\n",
    "            nn.Dropout(kwargs[\"thm2_dropout\"])\n",
    "        )\n",
    "        \n",
    "        self.tof_branch = nn.Sequential(\n",
    "            nn.Conv1d(tof_dim, kwargs[\"tof1_channels\"], kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm1d(kwargs[\"tof1_channels\"]),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool1d(2, ceil_mode=True),\n",
    "            nn.Dropout(kwargs[\"tof1_dropout\"]),\n",
    "            \n",
    "            nn.Conv1d(kwargs[\"tof1_channels\"], kwargs[\"feat_dim\"], kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm1d(kwargs[\"feat_dim\"]),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool1d(2, ceil_mode=True),\n",
    "            nn.Dropout(kwargs[\"tof2_dropout\"])\n",
    "        )\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, kwargs[\"feat_dim\"]))\n",
    "        self.bert = BertModel(BertConfig(\n",
    "            hidden_size=kwargs[\"feat_dim\"],\n",
    "            num_hidden_layers=kwargs[\"bert_layers\"],\n",
    "            num_attention_heads=kwargs[\"bert_heads\"],\n",
    "            intermediate_size=kwargs[\"feat_dim\"]*4\n",
    "        ))\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(kwargs[\"feat_dim\"], kwargs[\"cls1_channels\"], bias=False),\n",
    "            nn.BatchNorm1d(kwargs[\"cls1_channels\"]),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(kwargs[\"cls1_dropout\"]),\n",
    "            nn.Linear(kwargs[\"cls1_channels\"], kwargs[\"cls2_channels\"], bias=False),\n",
    "            nn.BatchNorm1d(kwargs[\"cls2_channels\"]),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(kwargs[\"cls2_dropout\"]),\n",
    "            nn.Linear(kwargs[\"cls2_channels\"], n_classes)\n",
    "        )\n",
    "    \n",
    "    def residual_se_cnn_block(self, in_channels, out_channels, num_layers, pool_size=2, drop=0.3, wd=1e-4):\n",
    "        return nn.Sequential(\n",
    "            *[ResNetSEBlock(in_channels=in_channels, out_channels=in_channels) for i in range(num_layers)],\n",
    "            ResNetSEBlock(in_channels, out_channels, wd=wd),\n",
    "            nn.MaxPool1d(pool_size),\n",
    "            nn.Dropout(drop)\n",
    "        )\n",
    "    \n",
    "    def forward(self, imu, thm, tof):\n",
    "        imu_feat = self.imu_branch(imu.permute(0, 2, 1))\n",
    "        thm_feat = self.thm_branch(thm.permute(0, 2, 1))\n",
    "        tof_feat = self.tof_branch(tof.permute(0, 2, 1))\n",
    "        \n",
    "        bert_input = torch.cat([imu_feat, thm_feat, tof_feat], dim=-1).permute(0, 2, 1)\n",
    "        cls_token = self.cls_token.expand(bert_input.size(0), -1, -1)  # (B,1,H)\n",
    "        bert_input = torch.cat([cls_token, bert_input], dim=1)  # (B,T+1,H)\n",
    "        outputs = self.bert(inputs_embeds=bert_input)\n",
    "        pred_cls = outputs.last_hidden_state[:, 0, :]\n",
    "\n",
    "        return self.classifier(pred_cls)\n",
    "\n",
    "\n",
    "CUDA0 = \"cuda:0\"\n",
    "seed = 0\n",
    "batch_size = 64\n",
    "num_workers = 4\n",
    "n_folds = 5\n",
    "\n",
    "universe_csv_path = Path(\"/kaggle/input/cmi-precompute/pytorch/all/1/tof-1_raw.csv\")\n",
    "\n",
    "deterministic = kagglehub.package_import('wasupandceacar/deterministic').deterministic\n",
    "deterministic.init_all(seed)\n",
    "def init_dataset():\n",
    "    dataset_config = {\n",
    "        \"percent\": 95,\n",
    "        \"scaler_function\": StandardScaler(),\n",
    "        \"nan_ratio\": {\n",
    "            \"imu\": 0,\n",
    "            \"thm\": 0,\n",
    "            \"tof\": 0,\n",
    "        },\n",
    "        \"fbfill\": {\n",
    "            \"imu\": True,\n",
    "            \"thm\": True,\n",
    "            \"tof\": True,\n",
    "        },\n",
    "        \"one_scale\": True,\n",
    "        \"tof_raw\": True,\n",
    "        \"tof_mode\": 16,\n",
    "        \"save_precompute\": False,\n",
    "    }\n",
    "    dataset = CMIFoldDataset(universe_csv_path, dataset_config,\n",
    "                             n_folds=n_folds, random_seed=seed, full_dataset_function=CMIFeDataset)\n",
    "    dataset.print_fold_stats()\n",
    "    return dataset\n",
    "\n",
    "def get_fold_dataset(dataset, fold):\n",
    "    _, valid_dataset = dataset.get_fold_datasets(fold)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False)\n",
    "    return valid_loader\n",
    "\n",
    "dataset = init_dataset()\n",
    "\n",
    "model_function = CMIModel\n",
    "model_args = {\"feat_dim\": 500,\n",
    "              \"imu1_channels\": 219, \"imu1_dropout\": 0.2946731587132302, \"imu2_dropout\": 0.2697745571929592,\n",
    "              \"imu1_weight_decay\": 0.0014824054650601245, \"imu2_weight_decay\": 0.002742543773142381,\n",
    "              \"imu1_layers\": 0, \"imu2_layers\": 0,\n",
    "              \"thm1_channels\": 82, \"thm1_dropout\": 0.2641274454844602, \"thm2_dropout\": 0.302896343020985, \n",
    "              \"tof1_channels\": 82, \"tof1_dropout\": 0.2641274454844602, \"tof2_dropout\": 0.3028963430209852, \n",
    "              \"bert_layers\": 8, \"bert_heads\": 10,\n",
    "              \"cls1_channels\": 937, \"cls2_channels\": 303, \"cls1_dropout\": 0.2281834512100508, \"cls2_dropout\": 0.22502521933558461}\n",
    "model_args.update({\n",
    "    \"imu_dim\": dataset.full_dataset.imu_dim, \n",
    "    \"thm_dim\": dataset.full_dataset.thm_dim,\n",
    "    \"tof_dim\": dataset.full_dataset.tof_dim,\n",
    "    \"n_classes\": dataset.full_dataset.class_num})\n",
    "model_dir = Path(\"/kaggle/input/cmi-models-public/pytorch/train_fold_model05_tof16_raw/1\")\n",
    "\n",
    "model_dicts = [\n",
    "    {\n",
    "        \"model_function\": model_function,\n",
    "        \"model_args\": model_args,\n",
    "        \"model_path\": model_dir / f\"fold{fold}/best_ema.pt\",\n",
    "    } for fold in range(n_folds)\n",
    "]\n",
    "\n",
    "models2 = list()\n",
    "for model_dict in model_dicts:\n",
    "    model_function = model_dict[\"model_function\"]\n",
    "    model_args = model_dict[\"model_args\"]\n",
    "    model_path = model_dict[\"model_path\"]\n",
    "    model = model_function(**model_args).to(CUDA0)\n",
    "    state_dict = {k.replace(\"_orig_mod.\", \"\"): v for k, v in torch.load(model_path).items()}\n",
    "    model.load_state_dict(state_dict)\n",
    "    model = model.eval()\n",
    "    models2.append(model)\n",
    "\n",
    "\n",
    "metric_package = kagglehub.package_import('wasupandceacar/cmi-metric')\n",
    "\n",
    "metric = metric_package.Metric()\n",
    "imu_only_metric = metric_package.Metric()\n",
    "\n",
    "def to_cuda(*tensors):\n",
    "    return [tensor.to(CUDA0) for tensor in tensors]\n",
    "\n",
    "def predict_valid(model, imu, thm, tof):\n",
    "    pred = model(imu, thm, tof)\n",
    "    return pred\n",
    "\n",
    "def valid(model, valid_bar):\n",
    "    with torch.no_grad():\n",
    "        for imu, thm, tof, y in valid_bar:\n",
    "            imu, thm, tof, y = to_cuda(imu, thm, tof, y)\n",
    "            with autocast(device_type='cuda', dtype=torch.bfloat16): \n",
    "                logits = predict_valid(model, imu, thm, tof)\n",
    "            metric.add(dataset.le.classes_[y.argmax(dim=1).cpu()], dataset.le.classes_[logits.argmax(dim=1).cpu()])\n",
    "            _, thm, tof = dataset.full_dataset.get_scaled_nan_tensors(imu, thm, tof)\n",
    "            with autocast(device_type='cuda', dtype=torch.bfloat16): \n",
    "                logits = model(imu, thm, tof)\n",
    "            imu_only_metric.add(dataset.le.classes_[y.argmax(dim=1).cpu()], dataset.le.classes_[logits.argmax(dim=1).cpu()])\n",
    "\n",
    "# for fold, model in enumerate(models2):\n",
    "#     valid_loader = get_fold_dataset(dataset, fold)\n",
    "#     valid_bar = tqdm(valid_loader, desc=f\"Valid\", position=0, leave=False)\n",
    "#     valid(model, valid_bar)\n",
    "\n",
    "# print(f\"\"\"\n",
    "# Normal score: {metric.score()}\n",
    "# IMU only score: {imu_only_metric.score()}\n",
    "# \"\"\")\n",
    "\n",
    "def avg_predict(models, imu, thm, tof):\n",
    "    outputs = []\n",
    "    with autocast(device_type='cuda'):\n",
    "        for model in models:\n",
    "            logits = model(imu, thm, tof)\n",
    "        outputs.append(logits)\n",
    "    return torch.mean(torch.stack(outputs), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57c77aea",
   "metadata": {
    "_cell_guid": "aa3e6f52-ea3b-463d-8ba3-1b18a90a15fe",
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "0e0d9d33-199d-4180-bd41-766e739ae8cb",
    "execution": {
     "iopub.execute_input": "2025-08-18T05:39:49.448563Z",
     "iopub.status.busy": "2025-08-18T05:39:49.447932Z",
     "iopub.status.idle": "2025-08-18T05:39:49.453343Z",
     "shell.execute_reply": "2025-08-18T05:39:49.452784Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.014429,
     "end_time": "2025-08-18T05:39:49.454517",
     "exception": false,
     "start_time": "2025-08-18T05:39:49.440088",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# predict_2\n",
    "\n",
    "def predict2(sequence: pl.DataFrame, demographics: pl.DataFrame) -> str:\n",
    "    imu, thm, tof = dataset.full_dataset.inference_process(sequence)\n",
    "    with torch.no_grad():\n",
    "        imu, thm, tof = to_cuda(imu, thm, tof)\n",
    "        logits = avg_predict(models2, imu, thm, tof)\n",
    "        probabilities = F.softmax(logits, dim=1).cpu().numpy()\n",
    "    return probabilities # logits.cpu().numpy()\n",
    "    # return dataset.le.classes_[logits.argmax(dim=1).cpu()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2130d92",
   "metadata": {
    "_cell_guid": "df7d9f50-50d9-424e-8253-c34c896807ec",
    "_uuid": "8a8aa3e9-bf4a-42d4-b8a6-d4e5ca173b20",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.006708,
     "end_time": "2025-08-18T05:39:49.468550",
     "exception": false,
     "start_time": "2025-08-18T05:39:49.461842",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42f7581a",
   "metadata": {
    "_cell_guid": "a740302f-1c27-4df5-bd55-20a1349eedbb",
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "4467066a-05f5-4c2a-bd78-3cfce5d5c291",
    "execution": {
     "iopub.execute_input": "2025-08-18T05:39:49.483876Z",
     "iopub.status.busy": "2025-08-18T05:39:49.483625Z",
     "iopub.status.idle": "2025-08-18T05:39:56.169793Z",
     "shell.execute_reply": "2025-08-18T05:39:56.168938Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 6.695471,
     "end_time": "2025-08-18T05:39:56.171099",
     "exception": false,
     "start_time": "2025-08-18T05:39:49.475628",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CompetitionMetric could not be imported. OOF/CV score will not be calculated.\n",
      "▶ ライブラリのインポート完了\n",
      "  - TensorFlow: 2.18.0\n",
      "  - PyTorch: 2.6.0+cu124\n",
      "▶ TRAINモード: False\n",
      "▶ 推論モード開始 – 学習済みモデルとアーティファクトを読み込みます...\n",
      "  モデル群A (自作5-Fold Gated GRUモデル) を読み込み中...\n",
      "  > 10個のモデルを正常に読み込みました。\n",
      "\n",
      "  モデル群B (公開TF/Kerasモデル) を読み込み中...\n",
      "  > 1個のモデルを正常に読み込みました。\n",
      "\n",
      "  モデル群C (公開PyTorchモデル) を読み込み中...\n",
      "  > 5個のモデルを正常に読み込みました。\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"gated-gru-hybrid-ensemble-v02.ipynb\n",
    "\n",
    "Automatically generated by Colab.\n",
    "\n",
    "Original file is located at\n",
    "    https://colab.research.google.com/drive/15f-PUIU6Tc6qYWYP6g7trekz1LypFFwW\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import polars as pl\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Conv1D, BatchNormalization, Activation, add, MaxPooling1D, Dropout,\n",
    "    Bidirectional, GRU, GlobalAveragePooling1D, Dense, Multiply, Reshape,\n",
    "    Lambda, Concatenate\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam as AdamTF\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.utils import Sequence, to_categorical, pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers.schedules import CosineDecay\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam as AdamTorch\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "from scipy.signal import firwin\n",
    "\n",
    "# 評価メトリクスはローカル検証/学習時にのみインポート\n",
    "try:\n",
    "    from cmi_2025_metric_copy_for_import import CompetitionMetric\n",
    "except ImportError:\n",
    "    CompetitionMetric = None\n",
    "    print(\"CompetitionMetric could not be imported. OOF/CV score will not be calculated.\")\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    \"\"\"\n",
    "    実行環境の乱数シードを統一的に設定する関数。\n",
    "    \"\"\"\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(2025)\n",
    "    tf.random.set_seed(seed)\n",
    "    tf.experimental.numpy.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
    "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "    # torch.backends.cudnn.deterministic = True # パフォーマンスが低下する可能性があるためコメントアウト\n",
    "    # torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(seed=42)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "TRAIN = False\n",
    "\n",
    "# --- パス設定 ---\n",
    "RAW_DIR = Path(\"/kaggle/input/cmi-detect-behavior-with-sensor-data\")\n",
    "# YOUR_MODELS_DIRは自分の学習済みモデルが格納されているKaggleデータセットのパスに設定してください\n",
    "YOUR_MODELS_DIR = Path(\"/kaggle/input/cmi-data-gated-gru\") # ★★★ 自分のモデルパスに変更 ★★★\n",
    "PUBLIC_TF_MODEL_DIR = Path(\"/kaggle/input/lb-0-78-quaternions-tf-bilstm-gru-attention\")\n",
    "PUBLIC_PT_MODEL_DIR = Path(\"/kaggle/input/cmi3-models-p\")\n",
    "EXPORT_DIR = Path(\"./\") # 学習済みモデルやアーティファクトの保存先\n",
    "\n",
    "# --- モデル学習ハイパーパラメータ ---\n",
    "BATCH_SIZE = 64          # バッチサイズ\n",
    "PAD_PERCENTILE = 95      # シーケンス長のパディングを決めるためのパーセンタイル値\n",
    "LR_INIT = 4e-4           # 学習率の初期値 (微調整)\n",
    "WD = 3e-3                # Weight Decay（L2正則化）の係数\n",
    "MIXUP_ALPHA = 0.4        # Mixupのα値\n",
    "EPOCHS = 360             # 最大エポック数 (増加)\n",
    "PATIENCE = 50            # EarlyStoppingのpatience (増加)\n",
    "N_SPLITS = 10             # クロスバリデーションの分割数\n",
    "MASKING_PROB = 0.25      # 学習時にTOF/THMデータをマスクする確率\n",
    "GATE_LOSS_WEIGHT = 0.2   # Gatedモデルのゲート損失に対する重み\n",
    "\n",
    "print(f\"▶ ライブラリのインポート完了\")\n",
    "print(f\"  - TensorFlow: {tf.__version__}\")\n",
    "print(f\"  - PyTorch: {torch.__version__}\")\n",
    "print(f\"▶ TRAINモード: {TRAIN}\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# PyTorchモデル用の標準化パラメータ\n",
    "mean_pt = torch.tensor([\n",
    "    0, 0, 0, 0, 0, 0, 9.0319e-03, 1.0849e+00, -2.6186e-03, 3.7651e-03,\n",
    "    -5.3660e-03, -2.8177e-03, 1.3318e-03, -1.5876e-04, 6.3495e-01,\n",
    "    6.2877e-01, 6.0607e-01, 6.2142e-01, 6.3808e-01, 6.5420e-01,\n",
    "    7.4102e-03, -3.4159e-03, -7.5237e-03, -2.6034e-02, 2.9704e-02,\n",
    "    -3.1546e-02, -2.0610e-03, -4.6986e-03, -4.7216e-03, -2.6281e-02,\n",
    "    1.5799e-02, 1.0016e-02\n",
    "], dtype=torch.float32).view(1, -1, 1).to(device)\n",
    "\n",
    "std_pt = torch.tensor([\n",
    "    1, 1, 1, 1, 1, 1, 0.2067, 0.8583, 0.3162,\n",
    "    0.2668, 0.2917, 0.2341, 0.3023, 0.3281, 1.0264, 0.8838, 0.8686, 1.0973,\n",
    "    1.0267, 0.9018, 0.4658, 0.2009, 0.2057, 1.2240, 0.9535, 0.6655, 0.2941,\n",
    "    0.3421, 0.8156, 0.6565, 1.1034, 1.5577\n",
    "], dtype=torch.float32).view(1, -1, 1).to(device) + 1e-8\n",
    "\n",
    "class ImuFeatureExtractor(nn.Module):\n",
    "    \"\"\"\n",
    "    ★★★ PyTorchモデル用の特徴量抽出器 ★★★\n",
    "    公開モデルの重みと一致させるため、元の正しい定義に修正。\n",
    "    \"\"\"\n",
    "    def __init__(self, fs=100., add_quaternion=False):\n",
    "        super().__init__()\n",
    "        self.fs = fs\n",
    "        self.add_quaternion = add_quaternion\n",
    "\n",
    "        k = 15\n",
    "\n",
    "        # ▼▼▼【ここが修正点】▼▼▼\n",
    "        # 公開モデルの重みファイルに存在する 'self.lpf' 層を再度追加する\n",
    "        self.lpf = nn.Conv1d(6, 6, kernel_size=k, padding=k//2,\n",
    "                                 groups=6, bias=False)\n",
    "        nn.init.kaiming_uniform_(self.lpf.weight, a=math.sqrt(5))\n",
    "        # ▲▲▲【ここまでが修正点】▲▲▲\n",
    "\n",
    "        self.lpf_acc  = nn.Conv1d(3, 3, k, padding=k//2, groups=3, bias=False)\n",
    "        self.lpf_gyro = nn.Conv1d(3, 3, k, padding=k//2, groups=3, bias=False)\n",
    "\n",
    "    def forward(self, imu):\n",
    "        acc  = imu[:, 0:3, :]\n",
    "        gyro = imu[:, 3:6, :]\n",
    "\n",
    "        # 1) magnitude\n",
    "        acc_mag  = torch.norm(acc,  dim=1, keepdim=True)\n",
    "        gyro_mag = torch.norm(gyro, dim=1, keepdim=True)\n",
    "\n",
    "        # 2) jerk\n",
    "        jerk = F.pad(acc[:, :, 1:] - acc[:, :, :-1], (1,0))\n",
    "        gyro_delta = F.pad(gyro[:, :, 1:] - gyro[:, :, :-1], (1,0))\n",
    "\n",
    "        # 3) energy\n",
    "        acc_pow  = acc ** 2\n",
    "        gyro_pow = gyro ** 2\n",
    "\n",
    "        # 4) LPF / HPF\n",
    "        # self.lpf は forwardパスでは使われていないが、重み読み込みのために定義が必要\n",
    "        acc_lpf  = self.lpf_acc(acc)\n",
    "        acc_hpf  = acc - acc_lpf\n",
    "        gyro_lpf = self.lpf_gyro(gyro)\n",
    "        gyro_hpf = gyro - gyro_lpf\n",
    "\n",
    "        features = [\n",
    "            acc, gyro,\n",
    "            acc_mag, gyro_mag,\n",
    "            jerk, gyro_delta,\n",
    "            acc_pow, gyro_pow,\n",
    "            acc_lpf, acc_hpf,\n",
    "            gyro_lpf, gyro_hpf,\n",
    "        ]\n",
    "        return torch.cat(features, dim=1)\n",
    "\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, channels, reduction=8):\n",
    "        super().__init__()\n",
    "        self.squeeze = nn.AdaptiveAvgPool1d(1)\n",
    "        self.excitation = nn.Sequential(\n",
    "            nn.Linear(channels, channels // reduction, bias=False), nn.ReLU(inplace=True),\n",
    "            nn.Linear(channels // reduction, channels, bias=False), nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        b, c, _ = x.size()\n",
    "        y = self.squeeze(x).view(b, c)\n",
    "        y = self.excitation(y).view(b, c, 1)\n",
    "        return x * y.expand_as(x)\n",
    "\n",
    "class ResidualSECNNBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, pool_size=2, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size, padding=kernel_size//2, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size, padding=kernel_size//2, bias=False)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "        self.se = SEBlock(out_channels)\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(nn.Conv1d(in_channels, out_channels, 1, bias=False), nn.BatchNorm1d(out_channels))\n",
    "        self.pool = nn.MaxPool1d(pool_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out = self.se(out)\n",
    "        out += self.shortcut(x)\n",
    "        return self.dropout(self.pool(F.relu(out)))\n",
    "\n",
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.attention = nn.Linear(hidden_dim, 1)\n",
    "    def forward(self, x):\n",
    "        scores = torch.tanh(self.attention(x))\n",
    "        weights = F.softmax(scores.squeeze(-1), dim=1)\n",
    "        return torch.sum(x * weights.unsqueeze(-1), dim=1)\n",
    "\n",
    "class TwoBranchModel(nn.Module):\n",
    "    def __init__(self, pad_len, imu_dim_raw, tof_dim, n_classes, dropouts=[0.3, 0.3, 0.3, 0.3, 0.4, 0.5, 0.3], feature_engineering=True, **kwargs):\n",
    "        super().__init__()\n",
    "        self.feature_engineering = feature_engineering\n",
    "        imu_dim = 32 if feature_engineering else imu_dim_raw\n",
    "        self.imu_fe = ImuFeatureExtractor(**kwargs) if feature_engineering else nn.Identity()\n",
    "        self.fir_nchan = 7\n",
    "        numtaps = 33\n",
    "        fir_kernel = torch.tensor(firwin(numtaps, cutoff=1.0, fs=10.0, pass_zero=False), dtype=torch.float32).view(1, 1, -1).repeat(self.fir_nchan, 1, 1)\n",
    "        self.register_buffer(\"fir_kernel\", fir_kernel)\n",
    "        self.imu_block1 = ResidualSECNNBlock(imu_dim, 64, 3, dropout=dropouts[0])\n",
    "        self.imu_block2 = ResidualSECNNBlock(64, 128, 5, dropout=dropouts[1])\n",
    "        self.tof_conv1 = nn.Conv1d(tof_dim, 64, 3, padding=1, bias=False)\n",
    "        self.tof_bn1, self.tof_pool1, self.tof_drop1 = nn.BatchNorm1d(64), nn.MaxPool1d(2), nn.Dropout(dropouts[2])\n",
    "        self.tof_conv2 = nn.Conv1d(64, 128, 3, padding=1, bias=False)\n",
    "        self.tof_bn2, self.tof_pool2, self.tof_drop2 = nn.BatchNorm1d(128), nn.MaxPool1d(2), nn.Dropout(dropouts[3])\n",
    "        self.bilstm = nn.LSTM(256, 128, bidirectional=True, batch_first=True)\n",
    "        self.lstm_dropout = nn.Dropout(dropouts[4])\n",
    "        self.attention = AttentionLayer(256)\n",
    "        self.dense1, self.bn_dense1, self.drop1 = nn.Linear(256, 256, bias=False), nn.BatchNorm1d(256), nn.Dropout(dropouts[5])\n",
    "        self.dense2, self.bn_dense2, self.drop2 = nn.Linear(256, 128, bias=False), nn.BatchNorm1d(128), nn.Dropout(dropouts[6])\n",
    "        self.classifier = nn.Linear(128, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        imu_raw = x[:, :, :self.fir_nchan].transpose(1, 2)\n",
    "        tof = x[:, :, self.fir_nchan:].transpose(1, 2)\n",
    "        imu_fe = self.imu_fe(imu_raw)\n",
    "        filtered = F.conv1d(imu_fe[:, :self.fir_nchan, :], self.fir_kernel, padding=self.fir_kernel.shape[-1] // 2, groups=self.fir_nchan)\n",
    "        imu = (torch.cat([filtered, imu_fe[:, self.fir_nchan:, :]], dim=1) - mean_pt) / std_pt\n",
    "        x1 = self.imu_block1(imu); x1 = self.imu_block2(x1)\n",
    "        x2 = self.tof_drop1(self.tof_pool1(F.relu(self.tof_bn1(self.tof_conv1(tof)))))\n",
    "        x2 = self.tof_drop2(self.tof_pool2(F.relu(self.tof_bn2(self.tof_conv2(x2)))))\n",
    "        merged = torch.cat([x1, x2], dim=1).transpose(1, 2)\n",
    "        lstm_out, _ = self.bilstm(merged); lstm_out = self.lstm_dropout(lstm_out)\n",
    "        attended = self.attention(lstm_out)\n",
    "        x = self.drop1(F.relu(self.bn_dense1(self.dense1(attended))))\n",
    "        x = self.drop2(F.relu(self.bn_dense2(self.dense2(x))))\n",
    "        return self.classifier(x)\n",
    "\n",
    "class PublicTwoBranchModel(nn.Module):\n",
    "    \"\"\"\n",
    "    ★★★ 公開されているPyTorchモデル（モデル群C）を読み込むための、元のアーキテクチャを持つクラス ★★★\n",
    "    \"\"\"\n",
    "    def __init__(self, pad_len, imu_dim_raw, tof_dim, n_classes, dropouts=[0.3, 0.3, 0.3, 0.3, 0.4, 0.5, 0.3], feature_engineering=True, **kwargs):\n",
    "        super().__init__()\n",
    "        self.feature_engineering = feature_engineering\n",
    "        imu_dim = 32 if feature_engineering else imu_dim_raw\n",
    "        self.imu_fe = ImuFeatureExtractor(**kwargs) if feature_engineering else nn.Identity()\n",
    "        self.fir_nchan = 7\n",
    "        numtaps = 33\n",
    "        fir_kernel = torch.tensor(firwin(numtaps, cutoff=1.0, fs=10.0, pass_zero=False), dtype=torch.float32).view(1, 1, -1).repeat(self.fir_nchan, 1, 1)\n",
    "        self.register_buffer(\"fir_kernel\", fir_kernel)\n",
    "        self.imu_block1 = ResidualSECNNBlock(imu_dim, 64, 3, dropout=dropouts[0])\n",
    "        self.imu_block2 = ResidualSECNNBlock(64, 128, 5, dropout=dropouts[1])\n",
    "        self.tof_conv1 = nn.Conv1d(tof_dim, 64, 3, padding=1, bias=False)\n",
    "        self.tof_bn1, self.tof_pool1, self.tof_drop1 = nn.BatchNorm1d(64), nn.MaxPool1d(2), nn.Dropout(dropouts[2])\n",
    "        self.tof_conv2 = nn.Conv1d(64, 128, 3, padding=1, bias=False)\n",
    "        self.tof_bn2, self.tof_pool2, self.tof_drop2 = nn.BatchNorm1d(128), nn.MaxPool1d(2), nn.Dropout(dropouts[3])\n",
    "        self.bilstm = nn.LSTM(256, 128, bidirectional=True, batch_first=True) # GRUではなくLSTM\n",
    "        self.lstm_dropout = nn.Dropout(dropouts[4])\n",
    "        self.attention = AttentionLayer(256) # 128*2 for bidirectional\n",
    "        self.dense1, self.bn_dense1, self.drop1 = nn.Linear(256, 256, bias=False), nn.BatchNorm1d(256), nn.Dropout(dropouts[5])\n",
    "        self.dense2, self.bn_dense2, self.drop2 = nn.Linear(256, 128, bias=False), nn.BatchNorm1d(128), nn.Dropout(dropouts[6])\n",
    "        self.classifier = nn.Linear(128, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        imu_raw = x[:, :, :self.fir_nchan].transpose(1, 2)\n",
    "        tof = x[:, :, self.fir_nchan:].transpose(1, 2)\n",
    "        imu_fe = self.imu_fe(imu_raw)\n",
    "        filtered = F.conv1d(imu_fe[:, :self.fir_nchan, :], self.fir_kernel, padding=self.fir_kernel.shape[-1] // 2, groups=self.fir_nchan)\n",
    "        # mean_pt, std_pt は事前に定義されているグローバル変数\n",
    "        imu = (torch.cat([filtered, imu_fe[:, self.fir_nchan:, :]], dim=1) - mean_pt) / std_pt\n",
    "        x1 = self.imu_block1(imu); x1 = self.imu_block2(x1)\n",
    "        x2 = self.tof_drop1(self.tof_pool1(F.relu(self.tof_bn1(self.tof_conv1(tof)))))\n",
    "        x2 = self.tof_drop2(self.tof_pool2(F.relu(self.tof_bn2(self.tof_conv2(x2)))))\n",
    "        merged = torch.cat([x1, x2], dim=1).transpose(1, 2)\n",
    "        lstm_out, _ = self.bilstm(merged); lstm_out = self.lstm_dropout(lstm_out)\n",
    "        attended = self.attention(lstm_out)\n",
    "        x = self.drop1(F.relu(self.bn_dense1(self.dense1(attended))))\n",
    "        x = self.drop2(F.relu(self.bn_dense2(self.dense2(x))))\n",
    "        return self.classifier(x)\n",
    "\n",
    "def pad_sequences_torch3(sequences, maxlen, padding='post', truncating='post', value=0.0):\n",
    "    result = []\n",
    "    for seq in sequences:\n",
    "        if len(seq) >= maxlen: seq = seq[:maxlen] if truncating == 'post' else seq[-maxlen:]\n",
    "        else:\n",
    "            pad_len = maxlen - len(seq)\n",
    "            pad_array = np.full((pad_len, seq.shape[1]), value)\n",
    "            seq = np.concatenate([seq, pad_array]) if padding == 'post' else np.concatenate([pad_array, seq])\n",
    "        result.append(seq)\n",
    "    return np.array(result, dtype=np.float32)\n",
    "\n",
    "# =============================================================================\n",
    "# ## 特徴量エンジニアリング関数\n",
    "# =============================================================================\n",
    "def remove_gravity_from_acc3(acc_data, rot_data):\n",
    "    \"\"\"加速度データから重力成分を除去する\"\"\"\n",
    "    acc_values = acc_data[['acc_x', 'acc_y', 'acc_z']].values\n",
    "    quat_values = rot_data[['rot_x', 'rot_y', 'rot_z', 'rot_w']].values\n",
    "    linear_accel = np.zeros_like(acc_values)\n",
    "    gravity_world = np.array([0, 0, 9.81])\n",
    "    for i in range(len(acc_values)):\n",
    "        if np.all(np.isnan(quat_values[i])):\n",
    "            linear_accel[i, :] = acc_values[i, :]\n",
    "            continue\n",
    "        try:\n",
    "            rotation = R.from_quat(quat_values[i])\n",
    "            gravity_sensor_frame = rotation.apply(gravity_world, inverse=True)\n",
    "            linear_accel[i, :] = acc_values[i, :] - gravity_sensor_frame\n",
    "        except (ValueError, IndexError):\n",
    "            linear_accel[i, :] = acc_values[i, :]\n",
    "    return linear_accel\n",
    "\n",
    "def calculate_angular_velocity_from_quat3(rot_data, time_delta=1/200):\n",
    "    \"\"\"クォータニオンから角速度を計算する\"\"\"\n",
    "    quat_values = rot_data[['rot_x', 'rot_y', 'rot_z', 'rot_w']].values\n",
    "    angular_vel = np.zeros((len(quat_values), 3))\n",
    "    for i in range(len(quat_values) - 1):\n",
    "        q_t, q_t_plus_dt = quat_values[i], quat_values[i+1]\n",
    "        if np.all(np.isnan(q_t)) or np.all(np.isnan(q_t_plus_dt)): continue\n",
    "        try:\n",
    "            rot_t = R.from_quat(q_t)\n",
    "            rot_t_plus_dt = R.from_quat(q_t_plus_dt)\n",
    "            delta_rot = rot_t.inv() * rot_t_plus_dt\n",
    "            angular_vel[i, :] = delta_rot.as_rotvec() / time_delta\n",
    "        except (ValueError, IndexError): pass\n",
    "    return angular_vel\n",
    "\n",
    "def calculate_angular_distance3(rot_data):\n",
    "    \"\"\"クォータニオンから角距離を計算する\"\"\"\n",
    "    quat_values = rot_data[['rot_x', 'rot_y', 'rot_z', 'rot_w']].values\n",
    "    angular_dist = np.zeros(len(quat_values))\n",
    "    for i in range(len(quat_values) - 1):\n",
    "        q1, q2 = quat_values[i], quat_values[i+1]\n",
    "        if np.all(np.isnan(q1)) or np.all(np.isnan(q2)): continue\n",
    "        try:\n",
    "            r1 = R.from_quat(q1)\n",
    "            r2 = R.from_quat(q2)\n",
    "            relative_rotation = r1.inv() * r2\n",
    "            angular_dist[i] = np.linalg.norm(relative_rotation.as_rotvec())\n",
    "        except (ValueError, IndexError): pass\n",
    "    return angular_dist\n",
    "\n",
    "def time_sum(x): return K.sum(x, axis=1)\n",
    "def squeeze_last_axis(x): return tf.squeeze(x, axis=-1)\n",
    "def expand_last_axis(x): return tf.expand_dims(x, axis=-1)\n",
    "\n",
    "def se_block(x, reduction=8):\n",
    "    \"\"\"Squeeze-and-Excitationブロック\"\"\"\n",
    "    ch = x.shape[-1]\n",
    "    se = GlobalAveragePooling1D()(x)\n",
    "    se = Dense(ch // reduction, activation='relu')(se)\n",
    "    se = Dense(ch, activation='sigmoid')(se)\n",
    "    se = Reshape((1, ch))(se)\n",
    "    return Multiply()([x, se])\n",
    "\n",
    "def residual_se_cnn_block(x, filters, kernel_size, pool_size=2, drop=0.3, wd=1e-4):\n",
    "    \"\"\"Residual SE-CNNブロック\"\"\"\n",
    "    shortcut = x\n",
    "    # 2層のConv1D\n",
    "    for _ in range(2):\n",
    "        x = Conv1D(filters, kernel_size, padding='same', use_bias=False, kernel_regularizer=l2(wd))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "    # SEブロック\n",
    "    x = se_block(x)\n",
    "    # ショートカット接続\n",
    "    if shortcut.shape[-1] != filters:\n",
    "        shortcut = Conv1D(filters, 1, padding='same', use_bias=False, kernel_regularizer=l2(wd))(shortcut)\n",
    "        shortcut = BatchNormalization()(shortcut)\n",
    "    x = add([x, shortcut])\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling1D(pool_size)(x)\n",
    "    x = Dropout(drop)(x)\n",
    "    return x\n",
    "\n",
    "def attention_layer(inputs):\n",
    "    \"\"\"アテンション層\"\"\"\n",
    "    score = Dense(1, activation='tanh')(inputs)\n",
    "    score = Lambda(squeeze_last_axis)(score)\n",
    "    weights = Activation('softmax')(score)\n",
    "    weights = Lambda(expand_last_axis)(weights)\n",
    "    context = Multiply()([inputs, weights])\n",
    "    context = Lambda(time_sum)(context)\n",
    "    return context\n",
    "\n",
    "class GatedMixupGenerator(Sequence):\n",
    "    \"\"\"Mixupとセンサーマスキングを適用するデータジェネレータ\"\"\"\n",
    "    def __init__(self, X, y, batch_size, imu_dim, class_weight=None, alpha=0.2, masking_prob=0.0):\n",
    "        self.X, self.y, self.batch, self.imu_dim = X, y, batch_size, imu_dim\n",
    "        self.class_weight, self.alpha, self.masking_prob = class_weight, alpha, masking_prob\n",
    "        self.indices = np.arange(len(X))\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.X) / self.batch))\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        idx = self.indices[i*self.batch:(i+1)*self.batch]\n",
    "        Xb, yb = self.X[idx].copy(), self.y[idx].copy()\n",
    "\n",
    "        sample_weights = np.ones(len(Xb), dtype='float32')\n",
    "        if self.class_weight:\n",
    "            sample_weights = np.array([self.class_weight.get(i, 1.0) for i in yb.argmax(axis=1)])\n",
    "\n",
    "        gate_target = np.ones(len(Xb), dtype='float32')\n",
    "        if self.masking_prob > 0:\n",
    "            for j in range(len(Xb)):\n",
    "                if np.random.rand() < self.masking_prob:\n",
    "                    Xb[j, :, self.imu_dim:] = 0\n",
    "                    gate_target[j] = 0.0\n",
    "\n",
    "        if self.alpha > 0:\n",
    "            lam = np.random.beta(self.alpha, self.alpha)\n",
    "            perm = np.random.permutation(len(Xb))\n",
    "            X_mix = lam * Xb + (1 - lam) * Xb[perm]\n",
    "            y_mix = lam * yb + (1 - lam) * yb[perm]\n",
    "            gate_target_mix = lam * gate_target + (1 - lam) * gate_target[perm]\n",
    "            sample_weights_mix = lam * sample_weights + (1 - lam) * sample_weights[perm]\n",
    "            return X_mix, {'main_output': y_mix, 'tof_gate': gate_target_mix}, sample_weights_mix\n",
    "\n",
    "        return Xb, {'main_output': yb, 'tof_gate': gate_target}, sample_weights\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        np.random.shuffle(self.indices)\n",
    "\n",
    "def build_gated_two_branch_model(pad_len, imu_dim, tof_dim, n_classes, wd=1e-4):\n",
    "    \"\"\"\n",
    "    自作のGated Two-Branchモデルを構築する関数。\n",
    "    [改良点] LSTMをGRUに変更、全結合層を1層追加。\n",
    "    \"\"\"\n",
    "    inp = Input(shape=(pad_len, imu_dim + tof_dim))\n",
    "    imu = Lambda(lambda t: t[:, :, :imu_dim])(inp)\n",
    "    tof = Lambda(lambda t: t[:, :, imu_dim:])(inp)\n",
    "\n",
    "    # IMUブランチ (Deep)\n",
    "    x1 = residual_se_cnn_block(imu, 64, 3, drop=0.1, wd=wd)\n",
    "    x1 = residual_se_cnn_block(x1, 128, 5, drop=0.1, wd=wd)\n",
    "\n",
    "    # TOF/THMブランチ (Light) with Gating\n",
    "    x2_base = Conv1D(64, 3, padding='same', use_bias=False, kernel_regularizer=l2(wd))(tof)\n",
    "    x2_base = BatchNormalization()(x2_base); x2_base = Activation('relu')(x2_base)\n",
    "    x2_base = MaxPooling1D(2)(x2_base); x2_base = Dropout(0.2)(x2_base)\n",
    "    x2_base = Conv1D(128, 3, padding='same', use_bias=False, kernel_regularizer=l2(wd))(x2_base)\n",
    "    x2_base = BatchNormalization()(x2_base); x2_base = Activation('relu')(x2_base)\n",
    "    x2_base = MaxPooling1D(2)(x2_base); x2_base = Dropout(0.2)(x2_base)\n",
    "\n",
    "    # Gating機構\n",
    "    gate_input = GlobalAveragePooling1D()(tof)\n",
    "    gate_input = Dense(16, activation='relu')(gate_input)\n",
    "    gate = Dense(1, activation='sigmoid', name='tof_gate')(gate_input)\n",
    "    x2 = Multiply()([x2_base, gate])\n",
    "\n",
    "    # ブランチのマージと後続層\n",
    "    merged = Concatenate()([x1, x2])\n",
    "    # ★改良点: LSTM -> GRU\n",
    "    x = Bidirectional(GRU(256, return_sequences=True, kernel_regularizer=l2(wd)))(merged)\n",
    "    x = Dropout(0.45)(x)\n",
    "    x = attention_layer(x)\n",
    "\n",
    "    # ★改良点: 全結合層を1層追加して表現力を向上\n",
    "    for units, drop in [(512, 0.5), (256, 0.4), (128, 0.3)]:\n",
    "        x = Dense(units, use_bias=False, kernel_regularizer=l2(wd))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = Dropout(drop)(x)\n",
    "\n",
    "    out = Dense(n_classes, activation='softmax', name='main_output', kernel_regularizer=l2(wd))(x)\n",
    "\n",
    "    return Model(inputs=inp, outputs=[out, gate])\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# ### 推論モード (`TRAIN = False`)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "print(\"▶ 推論モード開始 – 学習済みモデルとアーティファクトを読み込みます...\")\n",
    "\n",
    "# --- モデル群A (自作TF/Kerasモデル) の読み込み ---\n",
    "print(\"  モデル群A (自作5-Fold Gated GRUモデル) を読み込み中...\")\n",
    "final_feature_cols_A = np.load(YOUR_MODELS_DIR / \"final_feature_cols.npy\", allow_pickle=True).tolist()\n",
    "pad_len_A = int(np.load(YOUR_MODELS_DIR / \"sequence_maxlen.npy\"))\n",
    "scaler_A = joblib.load(YOUR_MODELS_DIR / \"scaler.pkl\")\n",
    "gesture_classes = np.load(YOUR_MODELS_DIR / \"gesture_classes.npy\", allow_pickle=True)\n",
    "custom_objs_A = {'time_sum': time_sum, 'squeeze_last_axis': squeeze_last_axis, 'expand_last_axis': expand_last_axis,\n",
    "                 'se_block': se_block, 'residual_se_cnn_block': residual_se_cnn_block, 'attention_layer': attention_layer}\n",
    "models_A = [load_model(YOUR_MODELS_DIR / f\"final_model_fold_{f}.h5\", compile=False, custom_objects=custom_objs_A) for f in range(N_SPLITS)]\n",
    "print(f\"  > {len(models_A)}個のモデルを正常に読み込みました。\")\n",
    "\n",
    "# --- モデル群B (公開TF/Kerasモデル) の読み込み ---\n",
    "print(\"\\n  モデル群B (公開TF/Kerasモデル) を読み込み中...\")\n",
    "final_feature_cols_B = np.load(PUBLIC_TF_MODEL_DIR / \"feature_cols.npy\", allow_pickle=True).tolist()\n",
    "pad_len_B = int(np.load(PUBLIC_TF_MODEL_DIR / \"sequence_maxlen.npy\"))\n",
    "scaler_B = joblib.load(PUBLIC_TF_MODEL_DIR / \"scaler.pkl\")\n",
    "custom_objs_B = custom_objs_A # public modelも同じカスタムオブジェクトを使用\n",
    "model_B = load_model(PUBLIC_TF_MODEL_DIR / \"gesture_two_branch_mixup.h5\", compile=False, custom_objects=custom_objs_B)\n",
    "print(\"  > 1個のモデルを正常に読み込みました。\")\n",
    "\n",
    "# --- モデル群C (公開PyTorchモデル) の読み込み ---\n",
    "print(\"\\n  モデル群C (公開PyTorchモデル) を読み込み中...\")\n",
    "final_feature_cols_C = np.load(PUBLIC_PT_MODEL_DIR / \"feature_cols.npy\", allow_pickle=True).tolist()\n",
    "pad_len_C = int(np.load(PUBLIC_PT_MODEL_DIR / \"sequence_maxlen.npy\"))\n",
    "scaler_C = joblib.load(PUBLIC_PT_MODEL_DIR / \"scaler.pkl\")\n",
    "\n",
    "pt_models = []\n",
    "for f in range(5):\n",
    "    checkpoint = torch.load(PUBLIC_PT_MODEL_DIR / f\"gesture_two_branch_fold{f}.pth\", map_location=device)\n",
    "    cfg = {'pad_len': checkpoint['pad_len'], 'imu_dim_raw': checkpoint['imu_dim'],\n",
    "           'tof_dim': checkpoint['tof_dim'], 'n_classes': checkpoint['n_classes']}\n",
    "    m = PublicTwoBranchModel(**cfg).to(device)\n",
    "    m.load_state_dict(checkpoint['model_state_dict'])\n",
    "    m.eval()\n",
    "    pt_models.append(m)\n",
    "print(f\"  > {len(pt_models)}個のモデルを正常に読み込みました。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efc1a3a5",
   "metadata": {
    "_cell_guid": "75f45de1-f56c-4f58-ae44-8edaf5b94855",
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "3d540ab3-d32e-4f32-9cee-d37bc032b7de",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-08-18T05:39:56.187354Z",
     "iopub.status.busy": "2025-08-18T05:39:56.186830Z",
     "iopub.status.idle": "2025-08-18T05:39:56.205858Z",
     "shell.execute_reply": "2025-08-18T05:39:56.205310Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.028254,
     "end_time": "2025-08-18T05:39:56.207006",
     "exception": false,
     "start_time": "2025-08-18T05:39:56.178752",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# predict_3\n",
    "\n",
    "# --- `predict`関数の定義 ---\n",
    "def predict3(sequence: pl.DataFrame, demographics: pl.DataFrame) -> str:\n",
    "    df_seq_orig = sequence.to_pandas()\n",
    "    df_seq_A = df_seq_orig.copy()\n",
    "    \n",
    "    linear_accel_A = remove_gravity_from_acc3(df_seq_A[['acc_x','acc_y','acc_z']], df_seq_A[['rot_x','rot_y','rot_z','rot_w']])\n",
    "    df_seq_A['linear_acc_x'], df_seq_A['linear_acc_y'], df_seq_A['linear_acc_z'] = linear_accel_A[:,0], linear_accel_A[:,1], linear_accel_A[:,2]\n",
    "    df_seq_A['linear_acc_mag'] = np.linalg.norm(linear_accel_A, axis=1)\n",
    "    df_seq_A['linear_acc_mag_jerk'] = df_seq_A['linear_acc_mag'].diff().fillna(0)\n",
    "    angular_vel_A = calculate_angular_velocity_from_quat3(df_seq_A[['rot_x','rot_y','rot_z','rot_w']])\n",
    "    df_seq_A['angular_vel_x'], df_seq_A['angular_vel_y'], df_seq_A['angular_vel_z'] = angular_vel_A[:,0], angular_vel_A[:,1], angular_vel_A[:,2]\n",
    "    df_seq_A['angular_distance'] = calculate_angular_distance3(df_seq_A[['rot_x','rot_y','rot_z','rot_w']])\n",
    "    for col in ['rot_x', 'rot_y', 'rot_z', 'rot_w']:\n",
    "        df_seq_A[f'{col}_diff'] = df_seq_A[col].diff().fillna(0)\n",
    "    cols_for_stats=['linear_acc_mag','linear_acc_mag_jerk','angular_distance']\n",
    "    for col in cols_for_stats:\n",
    "        df_seq_A[f'{col}_skew'], df_seq_A[f'{col}_kurt'] = df_seq_A[col].skew(), df_seq_A[col].kurtosis()\n",
    "    for i in range(1,6):\n",
    "        if f'tof_{i}_v0' in df_seq_A.columns:\n",
    "            pixel_cols=[f\"tof_{i}_v{p}\" for p in range(64)]; tof_data=df_seq_A[pixel_cols].replace(-1,np.nan)\n",
    "            df_seq_A[f'tof_{i}_mean'], df_seq_A[f'tof_{i}_std'], df_seq_A[f'tof_{i}_min'], df_seq_A[f'tof_{i}_max'] = tof_data.mean(axis=1),tof_data.std(axis=1),tof_data.min(axis=1),tof_data.max(axis=1)\n",
    "    tof_mean_cols=[f'tof_{i}_mean' for i in range(1,6) if f'tof_{i}_mean' in df_seq_A.columns]\n",
    "    if tof_mean_cols:\n",
    "        df_seq_A['tof_std_across_sensors']=df_seq_A[tof_mean_cols].std(axis=1)\n",
    "        df_seq_A['tof_range_across_sensors']=df_seq_A[tof_mean_cols].max(axis=1)-df_seq_A[tof_mean_cols].min(axis=1)\n",
    "    thm_cols=[f'thm_{i}' for i in range(1,6) if f'thm_{i}' in df_seq_A.columns]\n",
    "    if thm_cols:\n",
    "        df_seq_A['thm_std_across_sensors']=df_seq_A[thm_cols].std(axis=1)\n",
    "        df_seq_A['thm_range_across_sensors']=df_seq_A[thm_cols].max(axis=1)-df_seq_A[thm_cols].min(axis=1)\n",
    "    # (推論 A)\n",
    "    mat_A = df_seq_A[final_feature_cols_A].ffill().bfill().fillna(0).values.astype('float32')\n",
    "    mat_A = scaler_A.transform(mat_A)\n",
    "    pad_input_A = pad_sequences([mat_A], maxlen=pad_len_A, padding='post', dtype='float32')\n",
    "    preds_A_folds = [model.predict(pad_input_A, verbose=0)[0] for model in models_A]\n",
    "    avg_pred_A = np.mean(preds_A_folds, axis=0)\n",
    "\n",
    "    # --- 2. モデル群B (公開TFモデル) の予測 ---\n",
    "    df_seq_B = df_seq_orig.copy()\n",
    "    # (特徴量生成 B)\n",
    "    df_seq_B['acc_mag']=np.sqrt(df_seq_B['acc_x']**2+df_seq_B['acc_y']**2+df_seq_B['acc_z']**2)\n",
    "    df_seq_B['rot_angle']=2*np.arccos(df_seq_B['rot_w'].clip(-1,1))\n",
    "    df_seq_B['acc_mag_jerk']=df_seq_B['acc_mag'].diff().fillna(0)\n",
    "    df_seq_B['rot_angle_vel']=df_seq_B['rot_angle'].diff().fillna(0)\n",
    "    linear_accel_B=remove_gravity_from_acc3(df_seq_B,df_seq_B)\n",
    "    df_seq_B['linear_acc_x'],df_seq_B['linear_acc_y'],df_seq_B['linear_acc_z']=linear_accel_B[:,0],linear_accel_B[:,1],linear_accel_B[:,2]\n",
    "    df_seq_B['linear_acc_mag']=np.sqrt(df_seq_B['linear_acc_x']**2+df_seq_B['linear_acc_y']**2+df_seq_B['linear_acc_z']**2)\n",
    "    df_seq_B['linear_acc_mag_jerk']=df_seq_B['linear_acc_mag'].diff().fillna(0)\n",
    "    angular_vel_B=calculate_angular_velocity_from_quat3(df_seq_B)\n",
    "    df_seq_B['angular_vel_x'],df_seq_B['angular_vel_y'],df_seq_B['angular_vel_z']=angular_vel_B[:,0],angular_vel_B[:,1],angular_vel_B[:,2]\n",
    "    df_seq_B['angular_distance']=calculate_angular_distance3(df_seq_B)\n",
    "    for i in range(1,6):\n",
    "        if f'tof_{i}_v0' in df_seq_B.columns:\n",
    "            pixel_cols=[f\"tof_{i}_v{p}\" for p in range(64)]; tof_data=df_seq_B[pixel_cols].replace(-1,np.nan)\n",
    "            df_seq_B[f\"tof_{i}_mean\"],df_seq_B[f\"tof_{i}_std\"],df_seq_B[f\"tof_{i}_min\"],df_seq_B[f\"tof_{i}_max\"]=tof_data.mean(axis=1),tof_data.std(axis=1),tof_data.min(axis=1),tof_data.max(axis=1)\n",
    "    # (推論 B)\n",
    "    mat_B = df_seq_B[final_feature_cols_B].ffill().bfill().fillna(0).values.astype('float32')\n",
    "    mat_B = scaler_B.transform(mat_B)\n",
    "    pad_input_B = pad_sequences([mat_B], maxlen=pad_len_B, padding='post', dtype='float32')\n",
    "    pred_B = model_B.predict(pad_input_B, verbose=0)\n",
    "    if isinstance(pred_B, list): pred_B = pred_B[0]\n",
    "\n",
    "    # --- 3. モデル群C (公開PyTorchモデル) の予測 ---\n",
    "    df_seq_C = df_seq_orig.copy() # Cは特徴量生成が不要なため、コピーのみ\n",
    "    mat_C = df_seq_C[final_feature_cols_C].ffill().bfill().fillna(0).values.astype('float32')\n",
    "    mat_C = scaler_C.transform(mat_C)\n",
    "    pad_input_C = pad_sequences_torch3([mat_C], maxlen=pad_len_C, padding='pre', truncating='pre')\n",
    "    with torch.no_grad():\n",
    "        pt_input = torch.from_numpy(pad_input_C).to(device)\n",
    "        preds_C_folds = [model(pt_input) for model in pt_models]\n",
    "        avg_pred_C_logits = torch.mean(torch.stack(preds_C_folds), dim=0)\n",
    "        avg_pred_C = torch.softmax(avg_pred_C_logits, dim=1).cpu().numpy()\n",
    "\n",
    "    # --- 4. 加重平均による最終決定 ---\n",
    "    \n",
    "    weights = {'A': 0.50, 'B': 0.20, 'C': 0.30}\n",
    "\n",
    "    final_pred_proba = (weights['A'] * avg_pred_A + weights['B'] * pred_B + weights['C'] * avg_pred_C)\n",
    "\n",
    "    return final_pred_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb051c2e",
   "metadata": {
    "_cell_guid": "bc59ea4f-069b-4465-a0dc-82c911bce8f1",
    "_kg_hide-input": false,
    "_kg_hide-output": true,
    "_uuid": "5a1e7f85-d0a6-4dee-b37a-73486b8d9166",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-08-18T05:39:56.222608Z",
     "iopub.status.busy": "2025-08-18T05:39:56.222360Z",
     "iopub.status.idle": "2025-08-18T05:39:56.232192Z",
     "shell.execute_reply": "2025-08-18T05:39:56.231327Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.019116,
     "end_time": "2025-08-18T05:39:56.233482",
     "exception": false,
     "start_time": "2025-08-18T05:39:56.214366",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'w': 0.274, 'p': 1, 'n': 'p0'}, {'w': 0.342, 'p': 2, 'n': 'p1'}, {'w': 0.382, 'p': 3, 'n': 'p2'}] \n",
      "\n",
      " [{'w': 0.382, 'p': 3, 'n': 'p2'}, {'w': 0.342, 'p': 2, 'n': 'p1'}, {'w': 0.274, 'p': 1, 'n': 'p0'}] \n",
      "-----------\n",
      "[{'w': 0.28500000000000003, 'p': 1, 'n': 'p0'}, {'w': 0.338, 'p': 2, 'n': 'p1'}, {'w': 0.375, 'p': 3, 'n': 'p2'}] \n",
      "\n",
      " [{'w': 0.393, 'p': 3, 'n': 'p2'}, {'w': 0.338, 'p': 2, 'n': 'p1'}, {'w': 0.267, 'p': 1, 'n': 'p0'}]\n",
      "-----------\n",
      "[{'w': 0.28500000000000003, 'p': 1, 'n': 'p0'}, {'w': 0.338, 'p': 2, 'n': 'p1'}, {'w': 0.375, 'p': 3, 'n': 'p2'}] \n",
      "\n",
      " [{'w': 0.267, 'p': 1, 'n': 'p0'}, {'w': 0.338, 'p': 2, 'n': 'p1'}, {'w': 0.393, 'p': 3, 'n': 'p2'}]\n",
      "-----------\n",
      "[{'w': 0.28482, 'p': 1, 'n': 'p0'}, {'w': 0.338, 'p': 2, 'n': 'p1'}, {'w': 0.37517999999999996, 'p': 3, 'n': 'p2'}]\n",
      "-----------\n",
      "[0.28482, 0.676, 1.12554]\n"
     ]
    }
   ],
   "source": [
    "# help func, example\n",
    "\n",
    "pred0,pred1,pred2, ws, cws, aws = 1,2,3, [0.274,0.342,0.382], [+0.011, -0.004, -0.007], [0.99, 0.01]\n",
    "\n",
    "lp = [{ 'w':ws[0], 'p':pred0, 'n':'p0' },\n",
    "      { 'w':ws[1], 'p':pred1, 'n':'p1' },\n",
    "      { 'w':ws[2], 'p':pred2, 'n':'p2' }] \n",
    "\n",
    "lps_asc  = [{'w':p['w'], 'p':p['p'], 'n':p['n']} for p in lp]\n",
    "lps_desc = [{'w':p['w'], 'p':p['p'], 'n':p['n']} for p in lp]\n",
    "\n",
    "lps_asc  = sorted(lps_asc,  key=lambda k:k['p'],reverse=False)\n",
    "lps_desc = sorted(lps_desc, key=lambda k:k['p'],reverse=True)\n",
    "\n",
    "print(lps_asc, \"\\n\\n\", lps_desc, \"\") #------------------------\n",
    "\n",
    "for p,cw in zip(lps_asc,  cws): p['w'] += cw\n",
    "for p,cw in zip(lps_desc, cws): p['w'] += cw\n",
    "    \n",
    "print(\"-\"*11)\n",
    "print(lps_asc, \"\\n\\n\", lps_desc)     #------------------------\n",
    "\n",
    "lps_asc  = sorted(lps_asc,  key=lambda k:k['n'],reverse=False)\n",
    "lps_desc = sorted(lps_desc, key=lambda k:k['n'],reverse=False)\n",
    "\n",
    "print(\"-\"*11)\n",
    "print(lps_asc, \"\\n\\n\", lps_desc)     #------------------------\n",
    "\n",
    "lps = []\n",
    "\n",
    "for a,d in zip(lps_asc, lps_desc):\n",
    "    one_dict = {\n",
    "        'w':a['w']* aws[0]+aws[1] *d['w'],\n",
    "        'p':a['p'],\n",
    "        'n':a['n']\n",
    "    }\n",
    "    lps.append(one_dict)\n",
    "\n",
    "print(\"-\"*11)                        #------------------------\n",
    "print(lps)\n",
    "\n",
    "wps = [ps[\"w\"]*ps[\"p\"] for ps in lps]\n",
    "\n",
    "print(\"-\"*11)                        #------------------------\n",
    "print(wps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a3e516",
   "metadata": {
    "_cell_guid": "6ec9366e-7821-4e8e-b409-926cfea59624",
    "_uuid": "a5b188e6-b266-4881-8d3d-89bda08fc998",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.006963,
     "end_time": "2025-08-18T05:39:56.247786",
     "exception": false,
     "start_time": "2025-08-18T05:39:56.240823",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## predict\n",
    "\n",
    "enumeration \n",
    "\n",
    "Yes, it seems that in order to \"catch the outlines\" - the initial random search will be faster. Well, let's start with this.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd53b599",
   "metadata": {
    "_cell_guid": "3b6f94fd-0e45-49b4-bede-b6da200d827d",
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "27dc211f-94c6-4142-a0b2-6430aa405f69",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-08-18T05:39:56.263710Z",
     "iopub.status.busy": "2025-08-18T05:39:56.263089Z",
     "iopub.status.idle": "2025-08-18T05:39:56.267836Z",
     "shell.execute_reply": "2025-08-18T05:39:56.267340Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.013459,
     "end_time": "2025-08-18T05:39:56.268935",
     "exception": false,
     "start_time": "2025-08-18T05:39:56.255476",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# first glance\n",
    "\n",
    "# def predict(sequence, demographics):\n",
    "#     pred0 = predict1(sequence, demographics)[0]; # print(pred0)\n",
    "#     pred1 = predict2(sequence, demographics)[0]; # print(pred1)\n",
    "#     pred2 = predict3(sequence, demographics)[0]; # print(pred2)\n",
    "#     preds = []\n",
    "#     #--------------------------------------------------------------------------------------\n",
    "#     # wts  = np.asarray([[0.270,0.345,0.385],[0.270,0.342,0.388],[0.270,0.348,0.382]]) # wts1\n",
    "#     #------------------------------------------------------------------------------------------\n",
    "#     wts  = np.asarray([[0.265,0.350,0.385],[0.265,0.345,0.390],[0.265,0.3474,0.3876]]) # wts2\n",
    "#     #------------------------------------------------------------------------------------------\n",
    "#     #print (wts)\n",
    "#     #------------------------------------------------------------------------------------------\n",
    "#     #                           option.0               option.1                option.2\n",
    "#     #------------------------------------------------------------------------------------------\n",
    "#     cs123 = np.asarray([[1.0031,0.9980,0.99891],[1.0035,0.9977,0.99881],[1.0041,0.9974,0.9985]])\n",
    "#     cs132 = np.asarray([[1.0031,0.99891,0.9980],[1.0035,0.99881,0.9977],[1.0041,0.9985,0.9974]])\n",
    "#     #------------------------------------------------------------------------------------------\n",
    "#     cs213 = np.asarray([[0.9980,1.0031,0.99891],[0.9977,1.0035,0.99881],[0.9974,1.0041,0.9985]])\n",
    "#     cs231 = np.asarray([[0.99891,1.0031,0.9980],[0.99881,1.0035,0.9977],[0.9985,1.0041,0.9974]])\n",
    "#     #------------------------------------------------------------------------------------------\n",
    "#     cs312 = np.asarray([[0.9980,0.99891,1.0031],[0.9977,0.99881,1.0035],[0.9974,0.9985,1.0041]])\n",
    "#     cs321 = np.asarray([[0.99891,0.9980,1.0031],[0.99881,0.9977,1.0035],[0.9985,0.9974,1.0041]])\n",
    "#     #------------------------------------------------------------------------------------------\n",
    "    \n",
    "#     # wts_1\n",
    "#     # w,option = 0,0  #  v2  #  Lb=0.84    less and less current LB positions not visible \n",
    "#     # w,option = 0,1  #  v3  #  Lb=0.84    yet and unclear where to go apparently we will\n",
    "#     # w,option = 0,2  #  v4  #  Lb=0.84    resort to random schemes first, just like in \n",
    "#     # w,option = 1,0  #  v5  #  Lb=0.84    previous works to try to quickly \"remove\" \n",
    "#     # w,option = 2,0  #  v6  #  Lb=0.84    unnecessary search spaces. 1-2 days + hv_blend\n",
    "\n",
    "#     # wts_2:                           [ _, [0.265,0.345,0.390], [0.265,0.3474,0.3876] ]\n",
    "#     # w,option = 1,1  #  v7  #  Lb=?\n",
    "#     # w,option = 1,2  #  v8  #  Lb=?\n",
    "#     # w,option = 2,1  #  v9  #  Lb=?\n",
    "#     w,option = 2,2  #  v10 #  Lb=?\n",
    "    \n",
    "#     c123,c132 = cs123[option],cs132[option]\n",
    "#     c213,c231 = cs213[option],cs231[option]\n",
    "#     c312,c321 = cs312[option],cs321[option]\n",
    "\n",
    "#     for a,b,c in zip(pred0,pred1,pred2):\n",
    "#         if   a <= b <= c: _wts = c123 * wts[w]\n",
    "#         elif a <= c <= b: _wts = c132 * wts[w]\n",
    "#         elif b <= a <= c: _wts = c213 * wts[w]\n",
    "#         elif b <= c <= a: _wts = c231 * wts[w]\n",
    "#         elif c <= a <= b: _wts = c312 * wts[w]\n",
    "#         elif c <= b <= a: _wts = c321 * wts[w]\n",
    "#         p = a*_wts[0] + b*_wts[1] + c*_wts[2]\n",
    "#         preds.append(p)\n",
    "\n",
    "#     avg_pred =  np.asarray(preds)\n",
    "\n",
    "#     # avg_pred = pred0*0.33 + pred1*0.33 + pred2*0.34; print(avg_pred) \n",
    "                \n",
    "#     return dataset.le.classes_[avg_pred.argmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0dfbb4b6",
   "metadata": {
    "_cell_guid": "3191034b-87dc-4cf9-bc4a-c449f9a60498",
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "0b92931a-b97c-4d5f-b3e8-37bf7aa387cc",
    "execution": {
     "iopub.execute_input": "2025-08-18T05:39:56.284912Z",
     "iopub.status.busy": "2025-08-18T05:39:56.284280Z",
     "iopub.status.idle": "2025-08-18T05:39:56.290044Z",
     "shell.execute_reply": "2025-08-18T05:39:56.289468Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.014844,
     "end_time": "2025-08-18T05:39:56.291129",
     "exception": false,
     "start_time": "2025-08-18T05:39:56.276285",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.2690988, 0.344103 , 0.3864195])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "wts  = np.asarray([0.268,0.345,0.387]) \n",
    "c123 = np.asarray([1.0041,0.9974,0.9985])\n",
    "_wts = c123 * wts\n",
    "_wts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d022febe",
   "metadata": {
    "_cell_guid": "0e851084-3ae1-40c2-9728-29167f12d4f8",
    "_uuid": "dd288b0e-f378-4cf6-a8e9-dea21c66a8ca",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-08-18T05:39:56.307017Z",
     "iopub.status.busy": "2025-08-18T05:39:56.306819Z",
     "iopub.status.idle": "2025-08-18T05:39:56.314925Z",
     "shell.execute_reply": "2025-08-18T05:39:56.314405Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.017561,
     "end_time": "2025-08-18T05:39:56.316122",
     "exception": false,
     "start_time": "2025-08-18T05:39:56.298561",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict(sequence, demographics):\n",
    "    import numpy as np\n",
    "\n",
    "    # --- 各モデルの確率分布（shape: [n_classes]） ---\n",
    "    p0 = predict1(sequence, demographics)[0]\n",
    "    p1 = predict2(sequence, demographics)[0]\n",
    "    p2 = predict3(sequence, demographics)[0]\n",
    "\n",
    "    eps = 1e-12\n",
    "\n",
    "    # --- モデル間の不一致（平均Jensen–Shannon距離） ---\n",
    "    def _jsd(a, b, eps=1e-12):\n",
    "        a = np.clip(a, eps, 1.0); b = np.clip(b, eps, 1.0)\n",
    "        m = 0.5 * (a + b)\n",
    "        kl = lambda x, y: float((x * np.log(x / y)).sum())\n",
    "        return 0.5 * kl(a, m) + 0.5 * kl(b, m)\n",
    "\n",
    "    js = (_jsd(p0, p1) + _jsd(p1, p2) + _jsd(p0, p2)) / 3.0\n",
    "\n",
    "    # --- 基本の幾何平均（log空間の加重和） ---\n",
    "    w = np.array([0.27, 0.35, 0.38], dtype=float)  # モデル0,1,2 のベース重み\n",
    "    log_geom = (\n",
    "        w[0] * np.log(np.clip(p0, eps, 1.0)) +\n",
    "        w[1] * np.log(np.clip(p1, eps, 1.0)) +\n",
    "        w[2] * np.log(np.clip(p2, eps, 1.0))\n",
    "    )\n",
    "\n",
    "    # --- \"一番自信の高いモデル\"（エントロピー最小） ---\n",
    "    def _entropy(p):\n",
    "        q = np.clip(p, eps, 1.0)\n",
    "        return -float((q * np.log(q)).sum())\n",
    "\n",
    "    ent = [_entropy(p0), _entropy(p1), _entropy(p2)]\n",
    "    best_idx = int(np.argmin(ent))\n",
    "    log_best = np.log(np.clip([p0, p1, p2][best_idx], eps, 1.0))\n",
    "\n",
    "    # --- 不一致度 js に応じたソフトゲート t（0→幾何平均, 1→ベスト単独寄り）---\n",
    "    a, b = 0.10, 0.16                      # 緩やかに切替える帯域\n",
    "    t = float(np.clip((js - a) / (b - a), 0.0, 1.0))\n",
    "\n",
    "    # ベストモデルをほんの少しだけシャープ化（温度 1.00→0.98）\n",
    "    tau = 1.0 - 0.02 * t\n",
    "    log_best_sharp = log_best / tau\n",
    "\n",
    "    # --- 最終スコア（log空間線形補間 → argmax で十分）---\n",
    "    log_mix = (1.0 - t) * log_geom + t * log_best_sharp\n",
    "\n",
    "    return dataset.le.classes_[int(np.argmax(log_mix))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4be581f",
   "metadata": {
    "_cell_guid": "473b60a1-fe35-4d7f-960c-489c1c9cd55f",
    "_uuid": "411707b5-5c4f-4074-bff3-08b9d76d388c",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-08-18T05:39:56.331817Z",
     "iopub.status.busy": "2025-08-18T05:39:56.331596Z",
     "iopub.status.idle": "2025-08-18T05:39:56.334667Z",
     "shell.execute_reply": "2025-08-18T05:39:56.334072Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.012018,
     "end_time": "2025-08-18T05:39:56.335780",
     "exception": false,
     "start_time": "2025-08-18T05:39:56.323762",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "949947da",
   "metadata": {
    "_cell_guid": "6699bce7-be44-4549-8297-a80b85dd45d0",
    "_uuid": "77e8ff1e-f127-4abc-b00a-6668c999066c",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-08-18T05:39:56.351385Z",
     "iopub.status.busy": "2025-08-18T05:39:56.351036Z",
     "iopub.status.idle": "2025-08-18T05:40:33.245232Z",
     "shell.execute_reply": "2025-08-18T05:40:33.244539Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 36.903521,
     "end_time": "2025-08-18T05:40:33.246745",
     "exception": false,
     "start_time": "2025-08-18T05:39:56.343224",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-18 05:39:57.905531: E tensorflow/core/framework/node_def_util.cc:676] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_14}}\n",
      "I0000 00:00:1755495598.907475      62 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "2025-08-18 05:40:03.557774: E tensorflow/core/framework/node_def_util.cc:676] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_14}}\n",
      "2025-08-18 05:40:08.606931: E tensorflow/core/framework/node_def_util.cc:676] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_14}}\n",
      "2025-08-18 05:40:13.977674: E tensorflow/core/framework/node_def_util.cc:676] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_14}}\n",
      "2025-08-18 05:40:20.652181: E tensorflow/core/framework/node_def_util.cc:676] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_14}}\n",
      "2025-08-18 05:40:26.345754: E tensorflow/core/framework/node_def_util.cc:676] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_14}}\n",
      "2025-08-18 05:40:31.382242: E tensorflow/core/framework/node_def_util.cc:676] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_14}}\n"
     ]
    }
   ],
   "source": [
    "import kaggle_evaluation.cmi_inference_server\n",
    "inference_server = kaggle_evaluation.cmi_inference_server.CMIInferenceServer(predict)\n",
    "\n",
    "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    inference_server.serve()\n",
    "else:\n",
    "    inference_server.run_local_gateway(\n",
    "        data_paths=(\n",
    "            '/kaggle/input/cmi-detect-behavior-with-sensor-data/test.csv',\n",
    "            '/kaggle/input/cmi-detect-behavior-with-sensor-data/test_demographics.csv',\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec056548",
   "metadata": {
    "_cell_guid": "5058253c-4357-4421-8d59-f2168eeaf92c",
    "_uuid": "cedb60f1-f94f-4f09-bf11-99ee58e31b33",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-08-18T05:40:33.264581Z",
     "iopub.status.busy": "2025-08-18T05:40:33.264092Z",
     "iopub.status.idle": "2025-08-18T05:40:33.441594Z",
     "shell.execute_reply": "2025-08-18T05:40:33.440828Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.187605,
     "end_time": "2025-08-18T05:40:33.442840",
     "exception": false,
     "start_time": "2025-08-18T05:40:33.255235",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  sequence_id              gesture\n",
      "0  SEQ_000011  Eyelash - pull hair\n",
      "1  SEQ_000001  Eyebrow - pull hair\n"
     ]
    }
   ],
   "source": [
    "if not os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    print(pd.read_parquet(\"submission.parquet\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b02780",
   "metadata": {
    "papermill": {
     "duration": 0.008822,
     "end_time": "2025-08-18T05:40:33.461663",
     "exception": false,
     "start_time": "2025-08-18T05:40:33.452841",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ✅ Meta-Model + Sample-wise Adaptive Ensemble + Test-Time Augmentation (TTA)\n",
    "이 섹션은 기존 3-모델 앙상블을 **확률 가중합 → 메타모델 기반 조합**으로 확장하고,\n",
    "**샘플별 가중치 적응**과 **시계열 TTA**를 추가합니다.\n",
    "\n",
    "구성:\n",
    "1) 시계열/인구통계 기반 **메타 피처** 생성\n",
    "2) `predict1/2/3` 확률 + 메타 피처 → **LightGBM 메타모델** 학습\n",
    "3) 추론 시 **TTA**(시프트/노이즈/드롭아웃) + 메타모델 결합\n",
    "4) 메타모델이 없으면 기존 가중합으로 **자동 폴백**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d740cf07",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T05:40:33.480652Z",
     "iopub.status.busy": "2025-08-18T05:40:33.480394Z",
     "iopub.status.idle": "2025-08-18T05:40:39.079249Z",
     "shell.execute_reply": "2025-08-18T05:40:39.078475Z"
    },
    "papermill": {
     "duration": 5.61022,
     "end_time": "2025-08-18T05:40:39.080714",
     "exception": false,
     "start_time": "2025-08-18T05:40:33.470494",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, gc, math, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    _HAS_LGB = True\n",
    "except Exception:\n",
    "    _HAS_LGB = False\n",
    "\n",
    "META_MODEL_PATH = os.environ.get('META_MODEL_PATH', '/kaggle/working/meta_lgbm.pkl')\n",
    "FALLBACK_WTS = np.asarray([0.268, 0.345, 0.387], dtype=float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6eefc932",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T05:40:39.099675Z",
     "iopub.status.busy": "2025-08-18T05:40:39.098962Z",
     "iopub.status.idle": "2025-08-18T05:40:39.112821Z",
     "shell.execute_reply": "2025-08-18T05:40:39.112273Z"
    },
    "papermill": {
     "duration": 0.024414,
     "end_time": "2025-08-18T05:40:39.113988",
     "exception": false,
     "start_time": "2025-08-18T05:40:39.089574",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _num_cols(df):\n",
    "    return [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c])]\n",
    "\n",
    "def sequence_meta_features(sequence: 'pl.DataFrame|pd.DataFrame'):\n",
    "    \"\"\"시계열 원본에서 샘플 통계 피처를 생성.\n",
    "    - 길이, 결측비율, 전역 mean/std/skew/kurt\n",
    "    - 1차 차분 통계, IQR, 이상치 비율\n",
    "    - 열 단위 요약(상위 10개 컬럼 평균)\n",
    "    입력이 polars거나 pandas여도 동작하도록 최소 호환 처리.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import polars as pl\n",
    "        if isinstance(sequence, pl.DataFrame):\n",
    "            df = sequence.to_pandas()\n",
    "        else:\n",
    "            df = sequence\n",
    "    except Exception:\n",
    "        df = sequence\n",
    "\n",
    "    if not isinstance(df, pd.DataFrame):\n",
    "        df = pd.DataFrame(df)\n",
    "\n",
    "    cols = _num_cols(df)\n",
    "    if not cols:\n",
    "        return np.zeros(32, dtype=float)\n",
    "    x = df[cols].astype(float)\n",
    "    n = len(x)\n",
    "    miss_ratio = x.isna().mean().mean() if n else 0.0\n",
    "    x = x.fillna(x.median())\n",
    "\n",
    "    v = []\n",
    "    arr = x.to_numpy()\n",
    "    v += [n, miss_ratio]\n",
    "    v += [float(np.nanmean(arr)), float(np.nanstd(arr)+1e-12)]\n",
    "    v += [float(pd.DataFrame(arr).skew(numeric_only=True).mean()),\n",
    "          float(pd.DataFrame(arr).kurt(numeric_only=True).mean())]\n",
    "    # diff stats\n",
    "    if n>1:\n",
    "        diff = np.diff(arr, axis=0)\n",
    "        v += [float(np.nanmean(diff)), float(np.nanstd(diff)+1e-12)]\n",
    "    else:\n",
    "        v += [0.0, 0.0]\n",
    "    # IQR & outlier ratio\n",
    "    q1 = np.nanpercentile(arr, 25)\n",
    "    q3 = np.nanpercentile(arr, 75)\n",
    "    iqr = float(q3 - q1)\n",
    "    v += [iqr]\n",
    "    out_ratio = float((np.abs(arr - np.nanmedian(arr)) > 1.5*(iqr+1e-9)).mean())\n",
    "    v += [out_ratio]\n",
    "    # per-column head means (first 10 columns)\n",
    "    k = min(10, arr.shape[1])\n",
    "    col_means = np.nanmean(arr[:, :k], axis=0).tolist()\n",
    "    v += col_means\n",
    "    # pad\n",
    "    if len(v) < 32:\n",
    "        v += [0.0]*(32-len(v))\n",
    "    return np.asarray(v[:32], dtype=float)\n",
    "\n",
    "def demographics_meta_features(demo: 'pl.DataFrame|pd.DataFrame'):\n",
    "    try:\n",
    "        import polars as pl\n",
    "        if isinstance(demo, pl.DataFrame):\n",
    "            df = demo.to_pandas()\n",
    "        else:\n",
    "            df = demo\n",
    "    except Exception:\n",
    "        df = demo\n",
    "    if not isinstance(df, pd.DataFrame):\n",
    "        df = pd.DataFrame(df)\n",
    "    # 숫자만 취함 + 결측 대체\n",
    "    cols = _num_cols(df)\n",
    "    if not cols:\n",
    "        return np.zeros(8, dtype=float)\n",
    "    x = df[cols].astype(float)\n",
    "    x = x.fillna(x.median())\n",
    "    arr = x.to_numpy()\n",
    "    v = [float(np.nanmean(arr)), float(np.nanstd(arr)+1e-12), float(np.nanmin(arr)), float(np.nanmax(arr))]\n",
    "    # 상위 4개 열 평균(가변성 흡수용)\n",
    "    k = min(4, arr.shape[1])\n",
    "    v += np.nanmean(arr[:, :k], axis=0).tolist()\n",
    "    if len(v) < 8:\n",
    "        v += [0.0]*(8-len(v))\n",
    "    return np.asarray(v[:8], dtype=float)\n",
    "\n",
    "def build_meta_vector(p1, p2, p3, seq, demo):\n",
    "    \"\"\"세 모델 확률 + 메타피처를 하나의 벡터로 직렬화\"\"\"\n",
    "    p = np.concatenate([p1, p2, p3], axis=-1).astype(float)\n",
    "    s = sequence_meta_features(seq)\n",
    "    d = demographics_meta_features(demo)\n",
    "    return np.concatenate([p, s, d], axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a32b3939",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T05:40:39.132019Z",
     "iopub.status.busy": "2025-08-18T05:40:39.131784Z",
     "iopub.status.idle": "2025-08-18T05:40:39.138330Z",
     "shell.execute_reply": "2025-08-18T05:40:39.137718Z"
    },
    "papermill": {
     "duration": 0.016865,
     "end_time": "2025-08-18T05:40:39.139520",
     "exception": false,
     "start_time": "2025-08-18T05:40:39.122655",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _seq_as_df(seq):\n",
    "    try:\n",
    "        import polars as pl\n",
    "        if isinstance(seq, pl.DataFrame):\n",
    "            return seq.to_pandas()\n",
    "    except Exception:\n",
    "        pass\n",
    "    return seq if isinstance(seq, pd.DataFrame) else pd.DataFrame(seq)\n",
    "\n",
    "def tta_augmentations(sequence, n_noise=2, max_shift=2, dropout_prob=0.05, seed=42):\n",
    "    rng = np.random.RandomState(seed)\n",
    "    seq = _seq_as_df(sequence).copy()\n",
    "    cols = [c for c in seq.columns if pd.api.types.is_numeric_dtype(seq[c])]\n",
    "    seq[cols] = seq[cols].astype(float).interpolate(limit_direction='both').bfill().ffill()\n",
    "    variants = [seq]\n",
    "    # time-shifts\n",
    "    for s in range(1, max_shift+1):\n",
    "        variants.append(seq.shift(s).bfill())\n",
    "    # noise\n",
    "    for i in range(n_noise):\n",
    "        noise = rng.normal(0, 0.005 + 0.002*i, size=seq[cols].shape)\n",
    "        aug = seq.copy()\n",
    "        aug[cols] = aug[cols].to_numpy() + noise\n",
    "        variants.append(aug)\n",
    "    # feature dropout\n",
    "    aug = seq.copy()\n",
    "    mask = rng.rand(*aug[cols].shape) < dropout_prob\n",
    "    aug_vals = aug[cols].to_numpy()\n",
    "    med = np.nanmedian(aug_vals, axis=0)\n",
    "    aug_vals[mask] = med[np.where(mask)[1]]\n",
    "    aug[cols] = aug_vals\n",
    "    variants.append(aug)\n",
    "    return variants\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870f8158",
   "metadata": {
    "papermill": {
     "duration": 0.008142,
     "end_time": "2025-08-18T05:40:39.156080",
     "exception": false,
     "start_time": "2025-08-18T05:40:39.147938",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 🔧 메타모델 학습 (옵션)\n",
    "아래 셀은 **훈련 데이터 접근 가능할 때** 실행하세요. `train_sequences`, `train_demographics`, `train_targets`\n",
    "를 사용자 환경에 맞게 준비한 뒤, 3개 베이스 모델의 `predict1/2/3`을 호출해 확률을 얻고, 메타 피처를 붙여\n",
    "LightGBM를 학습합니다. 모델은 `META_MODEL_PATH`에 저장됩니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eedc1530",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T05:40:39.174143Z",
     "iopub.status.busy": "2025-08-18T05:40:39.173914Z",
     "iopub.status.idle": "2025-08-18T05:40:39.181204Z",
     "shell.execute_reply": "2025-08-18T05:40:39.180691Z"
    },
    "papermill": {
     "duration": 0.017707,
     "end_time": "2025-08-18T05:40:39.182418",
     "exception": false,
     "start_time": "2025-08-18T05:40:39.164711",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fit_meta_model(train_sequences, train_demographics, train_targets,\n",
    "                   valid_sequences=None, valid_demographics=None, valid_targets=None,\n",
    "                   num_classes=8, n_estimators=500, lr=0.05, reg_lambda=1.0, seed=42):\n",
    "    assert _HAS_LGB, 'lightgbm이 설치되어야 합니다.'\n",
    "    X_list, y_list = [], []\n",
    "    for seq, demo, y in zip(train_sequences, train_demographics, train_targets):\n",
    "        p1 = predict1(seq, demo)\n",
    "        p2 = predict2(seq, demo)\n",
    "        p3 = predict3(seq, demo)\n",
    "        x = build_meta_vector(p1, p2, p3, seq, demo)\n",
    "        X_list.append(x)\n",
    "        y_list.append(int(y))\n",
    "    X = np.vstack(X_list)\n",
    "    y = np.asarray(y_list)\n",
    "    dtrain = lgb.Dataset(X, label=y)\n",
    "    params = dict(objective='multiclass', num_class=num_classes, learning_rate=lr,\n",
    "                  n_estimators=n_estimators, reg_lambda=reg_lambda,\n",
    "                  feature_fraction=0.8, bagging_fraction=0.8, bagging_freq=1,\n",
    "                  random_state=seed, verbose=-1)\n",
    "    valid_sets = [dtrain]\n",
    "    valid_names = ['train']\n",
    "    if valid_sequences is not None and valid_targets is not None:\n",
    "        Xv = np.vstack([build_meta_vector(predict1(s, d), predict2(s, d), predict3(s, d), s, d)\n",
    "                        for s, d in zip(valid_sequences, valid_demographics)])\n",
    "        yv = np.asarray(valid_targets)\n",
    "        dvalid = lgb.Dataset(Xv, label=yv)\n",
    "        valid_sets.append(dvalid)\n",
    "        valid_names.append('valid')\n",
    "    model = lgb.train(params, dtrain, valid_sets=valid_sets, valid_names=valid_names,\n",
    "                      callbacks=[lgb.early_stopping(stopping_rounds=50), lgb.log_evaluation(50)])\n",
    "    import joblib\n",
    "    os.makedirs(os.path.dirname(META_MODEL_PATH), exist_ok=True)\n",
    "    joblib.dump(model, META_MODEL_PATH)\n",
    "    print('Saved meta model to', META_MODEL_PATH)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc6abf8",
   "metadata": {
    "papermill": {
     "duration": 0.008415,
     "end_time": "2025-08-18T05:40:39.199347",
     "exception": false,
     "start_time": "2025-08-18T05:40:39.190932",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 🚀 추론: TTA + 메타모델 + 폴백\n",
    "`predict_meta_ensemble(sequence, demographics)` 함수를 사용하면 테스트에서도 바로 적용됩니다.\n",
    "1) TTA 변형들에 대해 `predict1/2/3` 확률을 평균\n",
    "2) 메타 피처 결합 → LightGBM 메타모델이 있으면 사용\n",
    "3) 없으면 기존 가중합(`FALLBACK_WTS`)으로 폴백\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "88183095",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T05:40:39.217427Z",
     "iopub.status.busy": "2025-08-18T05:40:39.217151Z",
     "iopub.status.idle": "2025-08-18T05:40:39.223524Z",
     "shell.execute_reply": "2025-08-18T05:40:39.222926Z"
    },
    "papermill": {
     "duration": 0.016727,
     "end_time": "2025-08-18T05:40:39.224682",
     "exception": false,
     "start_time": "2025-08-18T05:40:39.207955",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _avg_probs(probs_list):\n",
    "    arr = np.stack(probs_list, axis=0)\n",
    "    return arr.mean(axis=0)\n",
    "\n",
    "def predict_meta_ensemble(sequence, demographics, num_classes=8, tta=True):\n",
    "    # 1) TTA\n",
    "    seq_variants = tta_augmentations(sequence) if tta else [sequence]\n",
    "    p1s, p2s, p3s = [], [], []\n",
    "    for sv in seq_variants:\n",
    "        p1s.append(predict1(sv, demographics))\n",
    "        p2s.append(predict2(sv, demographics))\n",
    "        p3s.append(predict3(sv, demographics))\n",
    "    p1 = _avg_probs(p1s)\n",
    "    p2 = _avg_probs(p2s)\n",
    "    p3 = _avg_probs(p3s)\n",
    "\n",
    "    # 2) meta-vector\n",
    "    x = build_meta_vector(p1, p2, p3, sequence, demographics)\n",
    "\n",
    "    # 3) meta-model or fallback\n",
    "    try:\n",
    "        import joblib\n",
    "        if _HAS_LGB and os.path.exists(META_MODEL_PATH):\n",
    "            model = joblib.load(META_MODEL_PATH)\n",
    "            proba = model.predict(x.reshape(1, -1))\n",
    "            if proba.ndim==2 and proba.shape[1]==num_classes:\n",
    "                return proba[0]\n",
    "    except Exception as e:\n",
    "        print('Meta-model load failed, fallback used:', e)\n",
    "\n",
    "    # fallback: normalized weighted average\n",
    "    w = FALLBACK_WTS / (FALLBACK_WTS.sum() + 1e-12)\n",
    "    out = w[0]*p1 + w[1]*p2 + w[2]*p3\n",
    "    out = np.maximum(out, 1e-9)\n",
    "    out = out / out.sum()\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c86dc313",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T05:40:39.242193Z",
     "iopub.status.busy": "2025-08-18T05:40:39.241970Z",
     "iopub.status.idle": "2025-08-18T05:40:39.245115Z",
     "shell.execute_reply": "2025-08-18T05:40:39.244639Z"
    },
    "papermill": {
     "duration": 0.013153,
     "end_time": "2025-08-18T05:40:39.246192",
     "exception": false,
     "start_time": "2025-08-18T05:40:39.233039",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict(sequence, demographics):\n",
    "    \"\"\"기존 predict 대체: 내부적으로 meta-ensemble을 호출\"\"\"\n",
    "    return predict_meta_ensemble(sequence, demographics)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 12518947,
     "sourceId": 102335,
     "sourceType": "competition"
    },
    {
     "datasetId": 7645099,
     "sourceId": 12139340,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7748073,
     "sourceId": 12293285,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7771623,
     "sourceId": 12328761,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7827797,
     "sourceId": 12411879,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7932089,
     "sourceId": 12573306,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 240649816,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 246893721,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 251413288,
     "sourceType": "kernelVersion"
    },
    {
     "modelId": 398856,
     "modelInstanceId": 379625,
     "sourceId": 470587,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 400086,
     "modelInstanceId": 380358,
     "sourceId": 471764,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 265.040787,
   "end_time": "2025-08-18T05:40:43.137474",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-08-18T05:36:18.096687",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
