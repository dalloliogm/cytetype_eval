{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fea1e297",
   "metadata": {
    "papermill": {
     "duration": 0.005425,
     "end_time": "2025-08-19T14:08:58.562421",
     "exception": false,
     "start_time": "2025-08-19T14:08:58.556996",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- Reference : https://www.kaggle.com/code/ryenhails/imu-only-baseline-lgbm-using-worldacc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cb89fb6",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-08-19T14:08:58.571929Z",
     "iopub.status.busy": "2025-08-19T14:08:58.571671Z",
     "iopub.status.idle": "2025-08-19T14:09:06.994678Z",
     "shell.execute_reply": "2025-08-19T14:09:06.993909Z"
    },
    "papermill": {
     "duration": 8.429172,
     "end_time": "2025-08-19T14:09:06.995940",
     "exception": false,
     "start_time": "2025-08-19T14:08:58.566768",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All imports loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# CMI BFRB Detection - LightGBM with World Acceleration Feature\n",
    "# A simple LightGBM implementation for BFRB classification with world coordinate transformation\n",
    "# \n",
    "# Key Innovation: Converting device acceleration to world coordinates using quaternion rotations\n",
    "# This helps normalize hand orientation differences across subjects and positions\n",
    "# Thanks https://www.kaggle.com/competitions/cmi-detect-behavior-with-sensor-data/discussion/583080 @tatamikenn for your idea!\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import joblib\n",
    "from typing import Tuple, List, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ML utilities\n",
    "from sklearn.model_selection import StratifiedGroupKFold, PredefinedSplit\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from lightgbm import LGBMClassifier, log_evaluation, early_stopping\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.base import clone\n",
    "import xgboost as xgb\n",
    "\n",
    "# World coordinate transformation\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "# Competition specific\n",
    "import kaggle_evaluation.cmi_inference_server\n",
    "\n",
    "print(\"✓ All imports loaded successfully\")\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "class Config: \n",
    "    \"\"\"Central configuration class for training and data parameters\"\"\"\n",
    "    \n",
    "    # Paths for Kaggle environment\n",
    "    TRAIN_PATH = \"/kaggle/input/cmi-detect-behavior-with-sensor-data/train.csv\"\n",
    "    TRAIN_DEMOGRAPHICS_PATH = \"/kaggle/input/cmi-detect-behavior-with-sensor-data/train_demographics.csv\"\n",
    "    TEST_PATH = \"/kaggle/input/cmi-detect-behavior-with-sensor-data/test.csv\"\n",
    "    TEST_DEMOGRAPHICS_PATH = \"/kaggle/input/cmi-detect-behavior-with-sensor-data/test_demographics.csv\"\n",
    "    \n",
    "    # Training parameters\n",
    "    SEED = 42\n",
    "    N_FOLDS = 5\n",
    "    \n",
    "    # Feature columns\n",
    "    ACC_COLS = ['acc_x', 'acc_y', 'acc_z']\n",
    "    ROT_COLS = ['rot_w', 'rot_x', 'rot_y', 'rot_z']\n",
    "    \n",
    "    # LightGBM parameters\n",
    "    LGBM_PARAMS = {\n",
    "        'objective': 'multiclass',\n",
    "        'n_estimators': 1024,\n",
    "        'max_depth': 8,\n",
    "        'learning_rate': 0.025,\n",
    "        'colsample_bytree': 0.5,\n",
    "        'n_jobs': -1,\n",
    "        'num_leaves': 20,\n",
    "        'random_state': 42,\n",
    "        'reg_alpha': 0.1,\n",
    "        'reg_lambda': 0.1,\n",
    "        'subsample': 0.5,\n",
    "        'verbosity': -1,\n",
    "        # 'device': 'gpu',  # Will be set automatically based on availability\n",
    "    }\n",
    "\n",
    "    RF_PARAMS = {\n",
    "        'n_estimators': 100,\n",
    "        'max_depth': 10,\n",
    "        'random_state': SEED,\n",
    "        'n_jobs': -1\n",
    "    }\n",
    "\n",
    "    XGB_PARAMS = {\n",
    "        'objective': 'multi:softmax',\n",
    "        'eval_metric': ['mlogloss', 'merror'],\n",
    "        'n_estimators': 1000,\n",
    "        'learning_rate': 0.05,\n",
    "        'max_depth': 7,\n",
    "        'subsample': 0.7,\n",
    "        'colsample_bytree': 0.7,\n",
    "        'random_state': SEED,\n",
    "        'n_jobs': -1\n",
    "    }\n",
    "\n",
    "# Set reproducibility\n",
    "np.random.seed(Config.SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44195be9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-19T14:09:07.005988Z",
     "iopub.status.busy": "2025-08-19T14:09:07.005498Z",
     "iopub.status.idle": "2025-08-19T14:09:11.113063Z",
     "shell.execute_reply": "2025-08-19T14:09:11.112107Z"
    },
    "papermill": {
     "duration": 4.113933,
     "end_time": "2025-08-19T14:09:11.114357",
     "exception": false,
     "start_time": "2025-08-19T14:09:07.000424",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ GPU available for LightGBM\n",
      "✓ Configuration loaded for Kaggle environment (Device: gpu)\n"
     ]
    }
   ],
   "source": [
    "def check_gpu_availability():\n",
    "    \"\"\"Check if GPU is available for LightGBM\"\"\"\n",
    "    try:\n",
    "        # Try to create a simple LightGBM model with GPU\n",
    "        from lightgbm import LGBMClassifier\n",
    "        import numpy as np\n",
    "        \n",
    "        # Create dummy data\n",
    "        X_dummy = np.random.rand(100, 10)\n",
    "        y_dummy = np.random.randint(0, 2, 100)\n",
    "        \n",
    "        # Try GPU\n",
    "        model = LGBMClassifier(n_estimators=1, device='gpu', verbosity=-1)\n",
    "        model.fit(X_dummy, y_dummy)\n",
    "        print(\"✓ GPU available for LightGBM\")\n",
    "        return 'gpu'\n",
    "    except:\n",
    "        print(\"⚠️  GPU not available, using CPU for LightGBM\")\n",
    "        return 'cpu'\n",
    "\n",
    "# Check GPU availability\n",
    "DEVICE = check_gpu_availability()\n",
    "\n",
    "print(f\"✓ Configuration loaded for Kaggle environment (Device: {DEVICE})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d22298e",
   "metadata": {
    "papermill": {
     "duration": 0.004776,
     "end_time": "2025-08-19T14:09:11.124314",
     "exception": false,
     "start_time": "2025-08-19T14:09:11.119538",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## GESTURE MAPPING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78170100",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-19T14:09:11.134751Z",
     "iopub.status.busy": "2025-08-19T14:09:11.134325Z",
     "iopub.status.idle": "2025-08-19T14:09:11.138837Z",
     "shell.execute_reply": "2025-08-19T14:09:11.138129Z"
    },
    "papermill": {
     "duration": 0.010888,
     "end_time": "2025-08-19T14:09:11.139972",
     "exception": false,
     "start_time": "2025-08-19T14:09:11.129084",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Gesture mapping (targets 0-7 are BFRB, 8-17 are non-BFRB)\n",
    "GESTURE_MAPPER = {\n",
    "    \"Above ear - pull hair\": 0,\n",
    "    \"Cheek - pinch skin\": 1,\n",
    "    \"Eyebrow - pull hair\": 2,\n",
    "    \"Eyelash - pull hair\": 3, \n",
    "    \"Forehead - pull hairline\": 4,\n",
    "    \"Forehead - scratch\": 5,\n",
    "    \"Neck - pinch skin\": 6, \n",
    "    \"Neck - scratch\": 7,\n",
    "    \n",
    "    \"Drink from bottle/cup\": 8,\n",
    "    \"Feel around in tray and pull out an object\": 9,\n",
    "    \"Glasses on/off\": 10,\n",
    "    \"Pinch knee/leg skin\": 11, \n",
    "    \"Pull air toward your face\": 12,\n",
    "    \"Scratch knee/leg skin\": 13,\n",
    "    \"Text on phone\": 14,\n",
    "    \"Wave hello\": 15,\n",
    "    \"Write name in air\": 16,\n",
    "    \"Write name on leg\": 17,\n",
    "}\n",
    "\n",
    "REVERSE_GESTURE_MAPPER = {v: k for k, v in GESTURE_MAPPER.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9880af54",
   "metadata": {
    "papermill": {
     "duration": 0.005246,
     "end_time": "2025-08-19T14:09:11.149833",
     "exception": false,
     "start_time": "2025-08-19T14:09:11.144587",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# COMPETITION METRIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "481b60a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-19T14:09:11.159906Z",
     "iopub.status.busy": "2025-08-19T14:09:11.159692Z",
     "iopub.status.idle": "2025-08-19T14:09:11.163911Z",
     "shell.execute_reply": "2025-08-19T14:09:11.163390Z"
    },
    "papermill": {
     "duration": 0.010432,
     "end_time": "2025-08-19T14:09:11.164931",
     "exception": false,
     "start_time": "2025-08-19T14:09:11.154499",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def competition_metric(y_true, y_pred) -> tuple:\n",
    "    \"\"\"Calculate the competition metric (Binary F1 + Macro F1) / 2\"\"\"\n",
    "    \n",
    "    # Binary F1: BFRB vs non-BFRB\n",
    "    binary_f1 = f1_score(\n",
    "        np.where(y_true <= 7, 1, 0),\n",
    "        np.where(y_pred <= 7, 1, 0),\n",
    "        zero_division=0.0,\n",
    "    )\n",
    "    \n",
    "    # Macro F1: specific gesture classification (only for BFRB gestures)\n",
    "    macro_f1 = f1_score(\n",
    "        np.where(y_true <= 7, y_true, 99),  # Map non-BFRB to 99\n",
    "        np.where(y_pred <= 7, y_pred, 99),  # Map non-BFRB to 99\n",
    "        average=\"macro\", \n",
    "        zero_division=0.0,\n",
    "    )\n",
    "    \n",
    "    # Final competition score\n",
    "    final_score = 0.5 * (binary_f1 + macro_f1)\n",
    "    \n",
    "    return final_score, binary_f1, macro_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c6cefb",
   "metadata": {
    "papermill": {
     "duration": 0.00459,
     "end_time": "2025-08-19T14:09:11.174095",
     "exception": false,
     "start_time": "2025-08-19T14:09:11.169505",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# CORE FEATURE ENGINEERING: WORLD ACCELERATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "520500b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-19T14:09:11.184085Z",
     "iopub.status.busy": "2025-08-19T14:09:11.183890Z",
     "iopub.status.idle": "2025-08-19T14:09:11.190381Z",
     "shell.execute_reply": "2025-08-19T14:09:11.189825Z"
    },
    "papermill": {
     "duration": 0.0128,
     "end_time": "2025-08-19T14:09:11.191431",
     "exception": false,
     "start_time": "2025-08-19T14:09:11.178631",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def handle_quaternion_missing_values(rot_data: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Handle missing values in quaternion data intelligently\n",
    "    \n",
    "    Key insight: Quaternions must have unit length |q| = 1\n",
    "    If one component is missing, we can reconstruct it from the others\n",
    "    \"\"\"\n",
    "    rot_cleaned = rot_data.copy()\n",
    "    \n",
    "    for i in range(len(rot_data)):\n",
    "        row = rot_data[i]\n",
    "        missing_count = np.isnan(row).sum()\n",
    "        \n",
    "        if missing_count == 0:\n",
    "            # No missing values, normalize to unit quaternion\n",
    "            norm = np.linalg.norm(row)\n",
    "            if norm > 1e-8:\n",
    "                rot_cleaned[i] = row / norm\n",
    "            else:\n",
    "                rot_cleaned[i] = [1.0, 0.0, 0.0, 0.0]  # Identity quaternion\n",
    "                \n",
    "        elif missing_count == 1:\n",
    "            # One missing value, reconstruct using unit quaternion constraint\n",
    "            # |w|² + |x|² + |y|² + |z|² = 1\n",
    "            missing_idx = np.where(np.isnan(row))[0][0]\n",
    "            valid_values = row[~np.isnan(row)]\n",
    "            \n",
    "            sum_squares = np.sum(valid_values**2)\n",
    "            if sum_squares <= 1.0:\n",
    "                missing_value = np.sqrt(max(0, 1.0 - sum_squares))\n",
    "                # Choose sign for continuity with previous quaternion\n",
    "                if i > 0 and not np.isnan(rot_cleaned[i-1, missing_idx]):\n",
    "                    if rot_cleaned[i-1, missing_idx] < 0:\n",
    "                        missing_value = -missing_value\n",
    "                rot_cleaned[i, missing_idx] = missing_value\n",
    "                rot_cleaned[i, ~np.isnan(row)] = valid_values\n",
    "            else:\n",
    "                rot_cleaned[i] = [1.0, 0.0, 0.0, 0.0]\n",
    "        else:\n",
    "            # More than one missing value, use identity quaternion\n",
    "            rot_cleaned[i] = [1.0, 0.0, 0.0, 0.0]\n",
    "    \n",
    "    return rot_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6cf61f82",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-19T14:09:11.201568Z",
     "iopub.status.busy": "2025-08-19T14:09:11.201367Z",
     "iopub.status.idle": "2025-08-19T14:09:11.206463Z",
     "shell.execute_reply": "2025-08-19T14:09:11.205760Z"
    },
    "papermill": {
     "duration": 0.011298,
     "end_time": "2025-08-19T14:09:11.207513",
     "exception": false,
     "start_time": "2025-08-19T14:09:11.196215",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_world_acceleration(acc: np.ndarray, rot: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Convert acceleration from device coordinates to world coordinates\n",
    "    \n",
    "    This is the key innovation: normalizing for device orientation\n",
    "    \n",
    "    Args:\n",
    "        acc: acceleration in device coordinates, shape (time_steps, 3) [x, y, z]\n",
    "        rot: rotation quaternion, shape (time_steps, 4) [w, x, y, z] (normalized)\n",
    "    \n",
    "    Returns:\n",
    "        acc_world: acceleration in world coordinates, shape (time_steps, 3)\n",
    "        \n",
    "    Why this matters:\n",
    "    - Device acceleration depends on how the watch is oriented on the wri/st\n",
    "    - World acceleration is independent of device orientation\n",
    "    - This helps the model focus on actual hand motion rather than wrist rotation\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Convert quaternion format from [w, x, y, z] to [x, y, z, w] for scipy\n",
    "        rot_scipy = rot[:, [1, 2, 3, 0]]\n",
    "        \n",
    "        # Verify quaternions are valid (non-zero norm)\n",
    "        norms = np.linalg.norm(rot_scipy, axis=1)\n",
    "        if np.any(norms < 1e-8):\n",
    "            # Replace problematic quaternions with identity\n",
    "            mask = norms < 1e-8\n",
    "            rot_scipy[mask] = [0.0, 0.0, 0.0, 1.0]  # Identity quaternion in scipy format\n",
    "        \n",
    "        # Create rotation object and apply transformation\n",
    "        r = R.from_quat(rot_scipy)\n",
    "        acc_world = r.apply(acc)\n",
    "        \n",
    "    except Exception:\n",
    "        # Fallback to original acceleration if transformation fails\n",
    "        print(\"Warning: World coordinate transformation failed, using device coordinates\")\n",
    "        acc_world = acc.copy()\n",
    "    \n",
    "    return acc_world"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958de020",
   "metadata": {
    "papermill": {
     "duration": 0.004351,
     "end_time": "2025-08-19T14:09:11.216361",
     "exception": false,
     "start_time": "2025-08-19T14:09:11.212010",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# SIMPLIFIED FEATURE EXTRACTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1358a9ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-19T14:09:11.226391Z",
     "iopub.status.busy": "2025-08-19T14:09:11.226181Z",
     "iopub.status.idle": "2025-08-19T14:09:11.235410Z",
     "shell.execute_reply": "2025-08-19T14:09:11.234786Z"
    },
    "papermill": {
     "duration": 0.01554,
     "end_time": "2025-08-19T14:09:11.236456",
     "exception": false,
     "start_time": "2025-08-19T14:09:11.220916",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_comprehensive_features(sequence: pl.DataFrame, demographics: pl.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extract features from IMU data with world acceleration transformation\n",
    "    \n",
    "    Feature Groups:\n",
    "    1. Device Acceleration (acc_x, acc_y, acc_z) - raw sensor data\n",
    "    2. Rotation Quaternion (rot_w, rot_x, rot_y, rot_z) - device orientation  \n",
    "    3. World Acceleration (NEW) - orientation-normalized acceleration\n",
    "    4. Demographics - subject characteristics\n",
    "    5. Sequence metadata - length, etc.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert to pandas for processing\n",
    "    seq_df = sequence.to_pandas()\n",
    "    demo_df = demographics.to_pandas()\n",
    "    \n",
    "    # Handle missing values in basic sensor data\n",
    "    acc_data = seq_df[Config.ACC_COLS].copy()\n",
    "    acc_data = acc_data.ffill().bfill().fillna(0)\n",
    "    \n",
    "    rot_data = seq_df[Config.ROT_COLS].copy()\n",
    "    rot_data = rot_data.ffill().bfill()\n",
    "    \n",
    "    # Handle quaternion missing values and normalize\n",
    "    rot_data_clean = handle_quaternion_missing_values(rot_data.values)\n",
    "    \n",
    "    # CORE INNOVATION: Compute world acceleration\n",
    "    try:\n",
    "        world_acc_data = compute_world_acceleration(acc_data.values, rot_data_clean)\n",
    "        # print(\"✓ World acceleration computed successfully\")  # Reduced verbosity\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: World acceleration computation failed: {e}\")\n",
    "        world_acc_data = acc_data.values.copy()  # Fallback to device coordinates\n",
    "    \n",
    "    # Initialize feature dictionary\n",
    "    features = {}\n",
    "    \n",
    "    # Add sequence metadata\n",
    "    features['sequence_length'] = len(seq_df)\n",
    "    \n",
    "    # Add demographics features\n",
    "    if len(demo_df) > 0:\n",
    "        demo_row = demo_df.iloc[0]\n",
    "        features['age'] = demo_row.get('age', 0)\n",
    "        features['adult_child'] = demo_row.get('adult_child', 0)\n",
    "        features['sex'] = demo_row.get('sex', 0)\n",
    "        features['handedness'] = demo_row.get('handedness', 0)\n",
    "        features['height_cm'] = demo_row.get('height_cm', 0)\n",
    "        features['shoulder_to_wrist_cm'] = demo_row.get('shoulder_to_wrist_cm', 0)\n",
    "        features['elbow_to_wrist_cm'] = demo_row.get('elbow_to_wrist_cm', 0)\n",
    "    \n",
    "    # Define feature arrays for statistical extraction\n",
    "    feature_arrays = {\n",
    "        'acc': acc_data.values,           # Device acceleration (3D)\n",
    "        'rot': rot_data_clean,            # Rotation quaternion (4D) \n",
    "        'world_acc': world_acc_data,      # World acceleration (3D) - KEY INNOVATION\n",
    "    }\n",
    "    \n",
    "    # Extract statistical features for each data source\n",
    "    for source_name, array in feature_arrays.items():\n",
    "        if array.ndim == 1:\n",
    "            array = array.reshape(-1, 1)\n",
    "        \n",
    "        n_features = array.shape[1]\n",
    "        \n",
    "        for feat_idx in range(n_features):\n",
    "            feat_data = array[:, feat_idx]\n",
    "            \n",
    "            # Create feature name\n",
    "            if source_name == 'acc':\n",
    "                axis_names = ['x', 'y', 'z']\n",
    "                prefix = f\"acc_{axis_names[feat_idx]}\"\n",
    "            elif source_name == 'rot':\n",
    "                comp_names = ['w', 'x', 'y', 'z']\n",
    "                prefix = f\"rot_{comp_names[feat_idx]}\"\n",
    "            elif source_name == 'world_acc':\n",
    "                axis_names = ['x', 'y', 'z']  \n",
    "                prefix = f\"world_acc_{axis_names[feat_idx]}\"\n",
    "            else:\n",
    "                prefix = f\"{source_name}_{feat_idx}\" if n_features > 1 else source_name\n",
    "            \n",
    "            # Extract comprehensive statistical features\n",
    "            features.update(extract_statistical_features(feat_data, prefix))\n",
    "    \n",
    "    # Compute magnitude features (important for motion intensity)\n",
    "    acc_magnitude = np.linalg.norm(acc_data.values, axis=1)\n",
    "    world_acc_magnitude = np.linalg.norm(world_acc_data, axis=1)\n",
    "    \n",
    "    features.update(extract_statistical_features(acc_magnitude, 'acc_magnitude'))\n",
    "    features.update(extract_statistical_features(world_acc_magnitude, 'world_acc_magnitude'))\n",
    "    \n",
    "    # Cross-feature: difference between device and world acceleration magnitudes\n",
    "    # This captures how much device orientation affects motion measurement\n",
    "    acc_world_diff = acc_magnitude - world_acc_magnitude\n",
    "    features.update(extract_statistical_features(acc_world_diff, 'acc_world_diff'))\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    result_df = pd.DataFrame([features])\n",
    "    \n",
    "    # Handle any remaining NaN values\n",
    "    result_df = result_df.fillna(0)\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3906a284",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-19T14:09:11.246569Z",
     "iopub.status.busy": "2025-08-19T14:09:11.246359Z",
     "iopub.status.idle": "2025-08-19T14:09:11.256874Z",
     "shell.execute_reply": "2025-08-19T14:09:11.256176Z"
    },
    "papermill": {
     "duration": 0.016872,
     "end_time": "2025-08-19T14:09:11.257942",
     "exception": false,
     "start_time": "2025-08-19T14:09:11.241070",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_statistical_features(data: np.ndarray, prefix: str) -> dict:\n",
    "    \"\"\"\n",
    "    Extract comprehensive statistical features from a 1D time series\n",
    "    \n",
    "    Returns features that capture:\n",
    "    - Central tendency: mean, median, mode region\n",
    "    - Spread: std, variance, range, IQR  \n",
    "    - Shape: skewness, kurtosis\n",
    "    - Dynamics: differences, trends, changes\n",
    "    - Segments: beginning vs middle vs end behavior\n",
    "    \"\"\"\n",
    "    \n",
    "    features = {}\n",
    "    \n",
    "    # Basic statistics\n",
    "    features[f'{prefix}_mean'] = np.mean(data)\n",
    "    features[f'{prefix}_std'] = np.std(data)\n",
    "    features[f'{prefix}_var'] = np.var(data)\n",
    "    features[f'{prefix}_min'] = np.min(data)\n",
    "    features[f'{prefix}_max'] = np.max(data)\n",
    "    features[f'{prefix}_median'] = np.median(data)\n",
    "    features[f'{prefix}_q25'] = np.percentile(data, 25)\n",
    "    features[f'{prefix}_q75'] = np.percentile(data, 75)\n",
    "    features[f'{prefix}_iqr'] = np.percentile(data, 75) - np.percentile(data, 25)\n",
    "    \n",
    "    # Range and boundary features\n",
    "    features[f'{prefix}_range'] = np.max(data) - np.min(data)\n",
    "    features[f'{prefix}_first'] = data[0] if len(data) > 0 else 0\n",
    "    features[f'{prefix}_last'] = data[-1] if len(data) > 0 else 0\n",
    "    features[f'{prefix}_delta'] = data[-1] - data[0] if len(data) > 0 else 0\n",
    "    \n",
    "    # Higher order moments (shape of distribution)\n",
    "    if len(data) > 1 and np.std(data) > 1e-8:\n",
    "        features[f'{prefix}_skew'] = pd.Series(data).skew()\n",
    "        features[f'{prefix}_kurt'] = pd.Series(data).kurtosis()\n",
    "    else:\n",
    "        features[f'{prefix}_skew'] = 0\n",
    "        features[f'{prefix}_kurt'] = 0\n",
    "    \n",
    "    # Differential features (capture dynamics)\n",
    "    if len(data) > 1:\n",
    "        diff_data = np.diff(data)\n",
    "        features[f'{prefix}_diff_mean'] = np.mean(diff_data)\n",
    "        features[f'{prefix}_diff_std'] = np.std(diff_data)\n",
    "        features[f'{prefix}_n_changes'] = np.sum(np.abs(diff_data) > np.std(data) * 0.1)  # Significant changes\n",
    "    else:\n",
    "        features[f'{prefix}_diff_mean'] = 0\n",
    "        features[f'{prefix}_diff_std'] = 0\n",
    "        features[f'{prefix}_n_changes'] = 0\n",
    "    \n",
    "    # Correlation with time (trend detection)\n",
    "    if len(data) > 2:\n",
    "        time_indices = np.arange(len(data))\n",
    "        try:\n",
    "            corr_coef = np.corrcoef(time_indices, data)[0, 1]\n",
    "            features[f'{prefix}_time_corr'] = corr_coef if not np.isnan(corr_coef) else 0\n",
    "        except:\n",
    "            features[f'{prefix}_time_corr'] = 0\n",
    "    else:\n",
    "        features[f'{prefix}_time_corr'] = 0\n",
    "    \n",
    "    # Segment features (beginning, middle, end patterns)\n",
    "    seq_len = len(data)\n",
    "    if seq_len >= 9:  # Need sufficient data for meaningful segments\n",
    "        seg_size = seq_len // 3\n",
    "        seg1 = data[:seg_size]           # Beginning (Transition phase)\n",
    "        seg2 = data[seg_size:2*seg_size] # Middle (Pause phase)  \n",
    "        seg3 = data[2*seg_size:]         # End (Gesture phase)\n",
    "        \n",
    "        features[f'{prefix}_seg1_mean'] = np.mean(seg1)\n",
    "        features[f'{prefix}_seg2_mean'] = np.mean(seg2)\n",
    "        features[f'{prefix}_seg3_mean'] = np.mean(seg3)\n",
    "        \n",
    "        features[f'{prefix}_seg1_std'] = np.std(seg1)\n",
    "        features[f'{prefix}_seg2_std'] = np.std(seg2)\n",
    "        features[f'{prefix}_seg3_std'] = np.std(seg3)\n",
    "        \n",
    "        # Segment transitions (important for distinguishing gesture types)\n",
    "        features[f'{prefix}_seg1_to_seg2'] = np.mean(seg2) - np.mean(seg1)\n",
    "        features[f'{prefix}_seg2_to_seg3'] = np.mean(seg3) - np.mean(seg2)\n",
    "    else:\n",
    "        # Not enough data for meaningful segments\n",
    "        for seg in [1, 2, 3]:\n",
    "            features[f'{prefix}_seg{seg}_mean'] = features[f'{prefix}_mean']\n",
    "            features[f'{prefix}_seg{seg}_std'] = features[f'{prefix}_std']\n",
    "        features[f'{prefix}_seg1_to_seg2'] = 0\n",
    "        features[f'{prefix}_seg2_to_seg3'] = 0\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1233464e",
   "metadata": {
    "papermill": {
     "duration": 0.004433,
     "end_time": "2025-08-19T14:09:11.267126",
     "exception": false,
     "start_time": "2025-08-19T14:09:11.262693",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# DATA LOADING AND PREPROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9180204d",
   "metadata": {
    "papermill": {
     "duration": 0.004462,
     "end_time": "2025-08-19T14:09:11.276123",
     "exception": false,
     "start_time": "2025-08-19T14:09:11.271661",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- reference : https://www.kaggle.com/code/chrisjm80/imu-thm-tof-pytorch-bilstm-gru-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bee5afcb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-19T14:09:11.286416Z",
     "iopub.status.busy": "2025-08-19T14:09:11.286209Z",
     "iopub.status.idle": "2025-08-19T14:09:11.293570Z",
     "shell.execute_reply": "2025-08-19T14:09:11.292897Z"
    },
    "papermill": {
     "duration": 0.013833,
     "end_time": "2025-08-19T14:09:11.294586",
     "exception": false,
     "start_time": "2025-08-19T14:09:11.280753",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_data_thm_tof(train_df: pl.dataframe):\n",
    "    print(\"Calculating ToF features with vectorized Polars and NumPy...\")\n",
    "\n",
    "    # 1. 'tof_1_v0', 'tof_1_v1', ... 와 같은 열 이름을 동적으로 생성\n",
    "    tof_pixel_cols = [f\"tof_{i}_v{p}\" for i in range(1, 6) for p in range(64)]\n",
    "    \n",
    "    # 2. -1 값을 Null로 대체하고, Polars의 표현식을 사용하여 NumPy 배열로 변환\n",
    "    # is_null()에 대한 is_not_null()은 .when().then()을 사용하여 구현할 수 있습니다.\n",
    "    # 먼저 필요한 열만 선택한 후 -1 값을 null로 대체합니다.\n",
    "    train_df_selected = train_df.select(pl.col(tof_pixel_cols).replace(-1, None))\n",
    "    tof_data_np = train_df_selected.to_numpy()\n",
    "    \n",
    "    # 3. NumPy를 사용하여 통계값 계산 (기존 로직과 동일)\n",
    "    reshaped_tof = tof_data_np.reshape(len(train_df), 5, 64)\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings('ignore', r'Mean of empty slice')\n",
    "        warnings.filterwarnings('ignore', r'Degrees of freedom <= 0 for slice')\n",
    "        mean_vals, std_vals = np.nanmean(reshaped_tof, axis=2), np.nanstd(reshaped_tof, axis=2)\n",
    "        min_vals, max_vals = np.nanmin(reshaped_tof, axis=2), np.nanmax(reshaped_tof, axis=2)\n",
    "    \n",
    "    # 4. Polars의 'with_columns'를 사용하여 새로운 열을 효율적으로 추가\n",
    "    # 'with_columns'는 여러 열을 한 번에 추가할 수 있어 효율적입니다.\n",
    "    for i in range(1, 6):\n",
    "        train_df = train_df.with_columns(\n",
    "            pl.Series(name=f'tof_{i}_mean', values=mean_vals[:, i-1]),\n",
    "            pl.Series(name=f'tof_{i}_std', values=std_vals[:, i-1]),\n",
    "            pl.Series(name=f'tof_{i}_min', values=min_vals[:, i-1]),\n",
    "            pl.Series(name=f'tof_{i}_max', values=max_vals[:, i-1]),\n",
    "        )\n",
    "    \n",
    "    # 5. 최종 특징 열 목록 생성 (기존 로직과 동일)\n",
    "    tof_agg_cols = [f'tof_{i}_{agg}' for i in range(1, 6) for agg in ['mean', 'std', 'min', 'max']]\n",
    "    imu_cols = [c for c in train_df.columns if c.startswith(('acc_', 'rot_'))]\n",
    "    thm_cols = [c for c in train_df.columns if c.startswith('thm_')]\n",
    "    final_feature_cols = imu_cols + thm_cols + tof_agg_cols\n",
    "    imu_dim_final = len(imu_cols)\n",
    "\n",
    "    tof_thm_aggregated_dim_final = len(thm_cols) + len(tof_agg_cols)\n",
    "    np.save(os.path.join(\"./\", \"feature_cols.npy\"), np.array(final_feature_cols))\n",
    "    print(\"  Building, scaling, and padding sequences...\")\n",
    "    # 오류나는 부\n",
    "    \"\"\"X_list, y_list, lens = [], [], []\n",
    "    for seq_id, seq_df in df.groupby('sequence_id'):\n",
    "        X_list.append(seq_df[final_feature_cols].ffill().bfill().fillna(0).values.astype('float32'))\n",
    "        y_list.append(seq_df['gesture_int'].iloc[0])\n",
    "        lens.append(len(seq_df))\n",
    "    \n",
    "    print(\"✓ Polars 데이터프레임에서 ToF 특징 계산 완료!\")\n",
    "    print(f\"새로 추가된 특징 열: {tof_agg_cols}\")\n",
    "    print(f\"최종 특징 열의 총 개수: {len(final_feature_cols)}\")\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3fa9e654",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-19T14:09:11.304573Z",
     "iopub.status.busy": "2025-08-19T14:09:11.304369Z",
     "iopub.status.idle": "2025-08-19T14:09:11.313412Z",
     "shell.execute_reply": "2025-08-19T14:09:11.312723Z"
    },
    "papermill": {
     "duration": 0.015413,
     "end_time": "2025-08-19T14:09:11.314490",
     "exception": false,
     "start_time": "2025-08-19T14:09:11.299077",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_and_prepare_data():\n",
    "    \"\"\"Load and prepare training data with comprehensive features\"\"\"\n",
    "    \n",
    "    print(\"Loading training data...\")\n",
    "    train_df = pl.read_csv(Config.TRAIN_PATH)\n",
    "    train_demographics = pl.read_csv(Config.TRAIN_DEMOGRAPHICS_PATH)\n",
    "    \n",
    "    print(\"Loading test data...\")\n",
    "    test_df = pl.read_csv(Config.TEST_PATH)\n",
    "    test_demographics = pl.read_csv(Config.TEST_DEMOGRAPHICS_PATH)\n",
    "    \n",
    "    # Get common columns between train and test (exclude thermal and ToF sensors)\n",
    "    train_cols = set(train_df.columns)\n",
    "    test_cols = set(test_df.columns)\n",
    "    common_cols = train_cols.intersection(test_cols)\n",
    "    \n",
    "    # Filter to IMU-only columns (remove thermal and ToF sensors)\n",
    "    imu_cols = [col for col in common_cols if not (col.startswith('thm_') or col.startswith('tof_'))]\n",
    "    \n",
    "    print(f\"✓ Using {len(imu_cols)} common IMU columns\")\n",
    "    print(f\"✓ Train-only columns: {train_cols - test_cols}\")\n",
    "    print(f\"✓ Test-only columns: {test_cols - train_cols}\")\n",
    "    \n",
    "    print(\"Extracting features for training sequences...\")\n",
    "    train_features_list = []\n",
    "    train_labels = []\n",
    "    train_subjects = []\n",
    "    train_sequence_ids = []\n",
    "    \n",
    "    # Group by sequence_id for training data - need to include gesture column for labels\n",
    "    train_imu_cols = imu_cols + ['gesture'] if 'gesture' not in imu_cols else imu_cols\n",
    "    train_sequences = train_df.select(pl.col(train_imu_cols)).group_by('sequence_id', maintain_order=True)\n",
    "    \n",
    "    for sequence_id, sequence_data in train_sequences:\n",
    "        # Get sequence features\n",
    "        sequence_id_val = sequence_id[0] if isinstance(sequence_id, tuple) else sequence_id\n",
    "        \n",
    "        # Get demographics for this sequence\n",
    "        subject_id = sequence_data['subject'][0]\n",
    "        subject_demographics = train_demographics.filter(pl.col('subject') == subject_id)\n",
    "        \n",
    "        # Extract features (only IMU columns for feature extraction)\n",
    "        imu_only_data = sequence_data.select(pl.col(imu_cols))\n",
    "        features = extract_comprehensive_features(imu_only_data, subject_demographics)\n",
    "        features['sequence_id'] = sequence_id_val\n",
    "        \n",
    "        train_features_list.append(features)\n",
    "        \n",
    "        # Get label (gesture) for this sequence\n",
    "        gesture = sequence_data['gesture'][0]\n",
    "        label = GESTURE_MAPPER[gesture]\n",
    "        train_labels.append(label)\n",
    "        train_subjects.append(subject_id)\n",
    "        train_sequence_ids.append(sequence_id_val)\n",
    "    \n",
    "    # Combine all training features\n",
    "    X_train = pd.concat(train_features_list, ignore_index=True)\n",
    "    y_train = np.array(train_labels)\n",
    "    subjects = np.array(train_subjects)\n",
    "    \n",
    "    print(\"Extracting features for test sequences...\")\n",
    "    test_features_list = []\n",
    "    test_sequence_ids = []\n",
    "    \n",
    "    # Group by sequence_id for test data  \n",
    "    test_sequences = test_df.select(pl.col(imu_cols)).group_by('sequence_id', maintain_order=True)\n",
    "    \n",
    "    for sequence_id, sequence_data in test_sequences:\n",
    "        sequence_id_val = sequence_id[0] if isinstance(sequence_id, tuple) else sequence_id\n",
    "        \n",
    "        # Get demographics for this sequence\n",
    "        subject_id = sequence_data['subject'][0]\n",
    "        subject_demographics = test_demographics.filter(pl.col('subject') == subject_id)\n",
    "        \n",
    "        # Extract features using the same function as training\n",
    "        features = extract_comprehensive_features(sequence_data, subject_demographics)\n",
    "        features['sequence_id'] = sequence_id_val\n",
    "        \n",
    "        test_features_list.append(features)\n",
    "        test_sequence_ids.append(sequence_id_val)\n",
    "    \n",
    "    # Combine all test features\n",
    "    X_test = pd.concat(test_features_list, ignore_index=True)\n",
    "    \n",
    "    print(f\"✓ Training features shape: {X_train.shape}\")\n",
    "    print(f\"✓ Training labels shape: {y_train.shape}\")\n",
    "    print(f\"✓ Test features shape: {X_test.shape}\")\n",
    "    print(f\"✓ Number of features: {X_train.shape[1] - 1}\")  # -1 for sequence_id\n",
    "    \n",
    "    return X_train, y_train, subjects, X_test, test_sequence_ids, imu_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072f1d4f",
   "metadata": {
    "papermill": {
     "duration": 0.00443,
     "end_time": "2025-08-19T14:09:11.323615",
     "exception": false,
     "start_time": "2025-08-19T14:09:11.319185",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# MODEL TRAINING WITH CROSS-VALIDATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca34e474",
   "metadata": {
    "papermill": {
     "duration": 0.00438,
     "end_time": "2025-08-19T14:09:11.332593",
     "exception": false,
     "start_time": "2025-08-19T14:09:11.328213",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ffcd0534",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-19T14:09:11.342705Z",
     "iopub.status.busy": "2025-08-19T14:09:11.342466Z",
     "iopub.status.idle": "2025-08-19T14:09:11.350285Z",
     "shell.execute_reply": "2025-08-19T14:09:11.349599Z"
    },
    "papermill": {
     "duration": 0.014072,
     "end_time": "2025-08-19T14:09:11.351284",
     "exception": false,
     "start_time": "2025-08-19T14:09:11.337212",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_lightgbm_models(X_train, y_train, subjects):\n",
    "    \"\"\"Train LightGBM models using stratified group k-fold cross-validation\"\"\"\n",
    "    \n",
    "    print(f\"Training LightGBM models with {Config.N_FOLDS}-fold cross-validation...\")\n",
    "    \n",
    "    # Prepare features (remove sequence_id)\n",
    "    feature_cols = [col for col in X_train.columns if col != 'sequence_id']\n",
    "    X_features = X_train[feature_cols]\n",
    "    \n",
    "    # Setup cross-validation\n",
    "    cv = StratifiedGroupKFold(n_splits=Config.N_FOLDS, shuffle=True, random_state=Config.SEED)\n",
    "    \n",
    "    models = []\n",
    "    oof_predictions = np.zeros(len(y_train))\n",
    "    cv_scores = []\n",
    "    \n",
    "    print(f\"Feature columns: {len(feature_cols)}\")\n",
    "    print(\"Starting cross-validation...\")\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(cv.split(X_features, y_train, subjects)):\n",
    "        print(f\"\\n--- Fold {fold + 1}/{Config.N_FOLDS} ---\")\n",
    "        \n",
    "        # Split data\n",
    "        X_fold_train = X_features.iloc[train_idx]\n",
    "        X_fold_val = X_features.iloc[val_idx]\n",
    "        y_fold_train = y_train[train_idx]\n",
    "        y_fold_val = y_train[val_idx]\n",
    "        \n",
    "        print(f\"Train size: {len(X_fold_train)}, Val size: {len(X_fold_val)}\")\n",
    "        \n",
    "        # Train model with monitoring and device detection\n",
    "        lgbm_params = Config.LGBM_PARAMS.copy()\n",
    "        lgbm_params['device'] = DEVICE  # Use detected device\n",
    "        \n",
    "        model = LGBMClassifier(**lgbm_params)\n",
    "        \n",
    "        print(f\"Training fold {fold + 1} with monitoring every 5 rounds (Device: {DEVICE})...\")\n",
    "        model.fit(\n",
    "            X_fold_train, y_fold_train,\n",
    "            eval_set=[(X_fold_train, y_fold_train), (X_fold_val, y_fold_val)],\n",
    "            eval_names=['train', 'valid'],\n",
    "            eval_metric=['multi_logloss', 'multi_error'],\n",
    "            callbacks=[\n",
    "                log_evaluation(period=5),  # Output every 5 rounds\n",
    "                early_stopping(stopping_rounds=100, verbose=True)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # Predictions\n",
    "        val_preds = model.predict(X_fold_val)\n",
    "        oof_predictions[val_idx] = val_preds\n",
    "        \n",
    "        # Calculate metrics\n",
    "        score, binary_f1, macro_f1 = competition_metric(y_fold_val, val_preds)\n",
    "        cv_scores.append(score)\n",
    "        \n",
    "        print(f\"Fold {fold + 1} - Competition Score: {score:.4f} (Binary F1: {binary_f1:.4f}, Macro F1: {macro_f1:.4f})\")\n",
    "        \n",
    "        models.append(model)\n",
    "    \n",
    "    # Overall CV performance\n",
    "    overall_score, overall_binary_f1, overall_macro_f1 = competition_metric(y_train, oof_predictions)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"CROSS-VALIDATION RESULTS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Overall Competition Score: {overall_score:.4f} ± {np.std(cv_scores):.4f}\")\n",
    "    print(f\"Overall Binary F1: {overall_binary_f1:.4f}\")\n",
    "    print(f\"Overall Macro F1: {overall_macro_f1:.4f}\")\n",
    "    print(f\"Fold scores: {[f'{score:.4f}' for score in cv_scores]}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    return models, cv_scores, overall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd187751",
   "metadata": {
    "papermill": {
     "duration": 0.004371,
     "end_time": "2025-08-19T14:09:11.360272",
     "exception": false,
     "start_time": "2025-08-19T14:09:11.355901",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "87f2e8ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-19T14:09:11.370338Z",
     "iopub.status.busy": "2025-08-19T14:09:11.370131Z",
     "iopub.status.idle": "2025-08-19T14:09:11.377058Z",
     "shell.execute_reply": "2025-08-19T14:09:11.376372Z"
    },
    "papermill": {
     "duration": 0.013163,
     "end_time": "2025-08-19T14:09:11.378088",
     "exception": false,
     "start_time": "2025-08-19T14:09:11.364925",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_randomforest_models(X_train, y_train, subjects):\n",
    "    \"\"\"Train RandomForest models using stratified group k-fold cross-validation\"\"\"\n",
    "    \n",
    "    print(f\"Training RandomForest models with {Config.N_FOLDS}-fold cross-validation...\")\n",
    "    \n",
    "    # Prepare features (remove sequence_id)\n",
    "    feature_cols = [col for col in X_train.columns if col != 'sequence_id']\n",
    "    X_features = X_train[feature_cols]\n",
    "    \n",
    "    # Setup cross-validation\n",
    "    cv = StratifiedGroupKFold(n_splits=Config.N_FOLDS, shuffle=True, random_state=Config.SEED)\n",
    "    \n",
    "    models = []\n",
    "    oof_predictions = np.zeros(len(y_train), dtype=int)\n",
    "    cv_scores = []\n",
    "    \n",
    "    print(f\"Feature columns: {len(feature_cols)}\")\n",
    "    print(\"Starting cross-validation...\")\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(cv.split(X_features, y_train, subjects)):\n",
    "        print(f\"\\n--- Fold {fold + 1}/{Config.N_FOLDS} ---\")\n",
    "        \n",
    "        # Split data\n",
    "        X_fold_train = X_features.iloc[train_idx]\n",
    "        X_fold_val = X_features.iloc[val_idx]\n",
    "        y_fold_train = y_train[train_idx]\n",
    "        y_fold_val = y_train[val_idx]\n",
    "        \n",
    "        print(f\"Train size: {len(X_fold_train)}, Val size: {len(X_fold_val)}\")\n",
    "        \n",
    "        # Train model\n",
    "        model = RandomForestClassifier(**Config.RF_PARAMS)\n",
    "        \n",
    "        print(f\"Training fold {fold + 1}...\")\n",
    "        model.fit(X_fold_train, y_fold_train)\n",
    "        \n",
    "        # Predictions\n",
    "        val_preds = model.predict(X_fold_val)\n",
    "        oof_predictions[val_idx] = val_preds\n",
    "        \n",
    "        # Calculate metrics\n",
    "        score, binary_f1, macro_f1 = competition_metric(y_fold_val, val_preds)\n",
    "        cv_scores.append(score)\n",
    "        \n",
    "        print(f\"Fold {fold + 1} - Competition Score: {score:.4f} (Binary F1: {binary_f1:.4f}, Macro F1: {macro_f1:.4f})\")\n",
    "        \n",
    "        models.append(model)\n",
    "    \n",
    "    # Overall CV performance\n",
    "    overall_score, overall_binary_f1, overall_macro_f1 = competition_metric(y_train, oof_predictions)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"CROSS-VALIDATION RESULTS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Overall Competition Score: {overall_score:.4f} ± {np.std(cv_scores):.4f}\")\n",
    "    print(f\"Overall Binary F1: {overall_binary_f1:.4f}\")\n",
    "    print(f\"Overall Macro F1: {overall_macro_f1:.4f}\")\n",
    "    print(f\"Fold scores: {[f'{score:.4f}' for score in cv_scores]}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    return models, cv_scores, overall_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b344fd",
   "metadata": {
    "papermill": {
     "duration": 0.004497,
     "end_time": "2025-08-19T14:09:11.387235",
     "exception": false,
     "start_time": "2025-08-19T14:09:11.382738",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee57c262",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-19T14:09:11.397273Z",
     "iopub.status.busy": "2025-08-19T14:09:11.397075Z",
     "iopub.status.idle": "2025-08-19T14:09:11.404991Z",
     "shell.execute_reply": "2025-08-19T14:09:11.404316Z"
    },
    "papermill": {
     "duration": 0.01419,
     "end_time": "2025-08-19T14:09:11.406065",
     "exception": false,
     "start_time": "2025-08-19T14:09:11.391875",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_xgboost_models(X_train, y_train, subjects):\n",
    "    \"\"\"Train XGBoost models using stratified group k-fold cross-validation\"\"\"\n",
    "    \n",
    "    print(f\"Training XGBoost models with {Config.N_FOLDS}-fold cross-validation...\")\n",
    "    \n",
    "    # Prepare features (remove sequence_id)\n",
    "    feature_cols = [col for col in X_train.columns if col != 'sequence_id']\n",
    "    X_features = X_train[feature_cols]\n",
    "    \n",
    "    # Setup cross-validation\n",
    "    cv = StratifiedGroupKFold(n_splits=Config.N_FOLDS, shuffle=True, random_state=Config.SEED)\n",
    "    \n",
    "    models = []\n",
    "    oof_predictions = np.zeros(len(y_train), dtype=int)\n",
    "    cv_scores = []\n",
    "    \n",
    "    print(f\"Feature columns: {len(feature_cols)}\")\n",
    "    print(\"Starting cross-validation...\")\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(cv.split(X_features, y_train, subjects)):\n",
    "        print(f\"\\n--- Fold {fold + 1}/{Config.N_FOLDS} ---\")\n",
    "        \n",
    "        # Split data\n",
    "        X_fold_train = X_features.iloc[train_idx]\n",
    "        X_fold_val = X_features.iloc[val_idx]\n",
    "        y_fold_train = y_train[train_idx]\n",
    "        y_fold_val = y_train[val_idx]\n",
    "        \n",
    "        print(f\"Train size: {len(X_fold_train)}, Val size: {len(X_fold_val)}\")\n",
    "        \n",
    "        # Train model with monitoring and device detection\n",
    "        xgb_params = Config.XGB_PARAMS.copy()\n",
    "        \n",
    "        # Adjust tree_method for GPU if available and desired\n",
    "        if DEVICE == 'gpu':\n",
    "            xgb_params['tree_method'] = 'gpu_hist'\n",
    "\n",
    "        model = xgb.XGBClassifier(**xgb_params)\n",
    "        \n",
    "        print(f\"Training fold {fold + 1} with early stopping (Device: {DEVICE})...\")\n",
    "        model.fit(\n",
    "            X_fold_train, y_fold_train,\n",
    "            eval_set=[(X_fold_val, y_fold_val)],\n",
    "            early_stopping_rounds=100,\n",
    "            verbose=100\n",
    "        )\n",
    "        \n",
    "        # Predictions\n",
    "        val_preds = model.predict(X_fold_val)\n",
    "        oof_predictions[val_idx] = val_preds\n",
    "        \n",
    "        # Calculate metrics\n",
    "        score, binary_f1, macro_f1 = competition_metric(y_fold_val, val_preds)\n",
    "        cv_scores.append(score)\n",
    "        \n",
    "        print(f\"Fold {fold + 1} - Competition Score: {score:.4f} (Binary F1: {binary_f1:.4f}, Macro F1: {macro_f1:.4f})\")\n",
    "        \n",
    "        models.append(model)\n",
    "    \n",
    "    # Overall CV performance\n",
    "    overall_score, overall_binary_f1, overall_macro_f1 = competition_metric(y_train, oof_predictions)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"CROSS-VALIDATION RESULTS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Overall Competition Score: {overall_score:.4f} ± {np.std(cv_scores):.4f}\")\n",
    "    print(f\"Overall Binary F1: {overall_binary_f1:.4f}\")\n",
    "    print(f\"Overall Macro F1: {overall_macro_f1:.4f}\")\n",
    "    print(f\"Fold scores: {[f'{score:.4f}' for score in cv_scores]}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    return models, cv_scores, overall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee112d8e",
   "metadata": {
    "papermill": {
     "duration": 0.004921,
     "end_time": "2025-08-19T14:09:11.415512",
     "exception": false,
     "start_time": "2025-08-19T14:09:11.410591",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# INFERENCE FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ddc3328b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-19T14:09:11.425435Z",
     "iopub.status.busy": "2025-08-19T14:09:11.425223Z",
     "iopub.status.idle": "2025-08-19T14:09:11.431057Z",
     "shell.execute_reply": "2025-08-19T14:09:11.430370Z"
    },
    "papermill": {
     "duration": 0.012119,
     "end_time": "2025-08-19T14:09:11.432192",
     "exception": false,
     "start_time": "2025-08-19T14:09:11.420073",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_prediction_function(models, feature_cols, imu_cols):\n",
    "    \"\"\"Create prediction function for Kaggle evaluation\"\"\"\n",
    "    \n",
    "    def predict(sequence: pl.DataFrame, demographics: pl.DataFrame) -> str:\n",
    "        \"\"\"\n",
    "        Prediction function for Kaggle evaluation\n",
    "        Uses ensemble of trained LightGBM models\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Filter sequence to only include IMU columns that we trained with\n",
    "            available_cols = sequence.columns\n",
    "            sequence_imu_cols = [col for col in imu_cols if col in available_cols]\n",
    "            sequence_filtered = sequence.select(pl.col(sequence_imu_cols))\n",
    "            \n",
    "            # Extract features using the same method as training\n",
    "            features = extract_comprehensive_features(sequence_filtered, demographics)\n",
    "            \n",
    "            # Ensure we have the same features as training\n",
    "            missing_features = [col for col in feature_cols if col not in features.columns]\n",
    "            if missing_features:\n",
    "                print(f\"Warning: Missing features {missing_features}, filling with zeros\")\n",
    "                for col in missing_features:\n",
    "                    features[col] = 0\n",
    "            \n",
    "            X_pred = features[feature_cols]\n",
    "            \n",
    "            # Get predictions from all models\n",
    "            predictions = []\n",
    "            for model in models:\n",
    "                pred_probs = model.predict_proba(X_pred)\n",
    "                pred_class = np.argmax(pred_probs, axis=1)[0]\n",
    "                predictions.append(pred_class)\n",
    "            \n",
    "            # Ensemble prediction (majority vote)\n",
    "            final_prediction = max(set(predictions), key=predictions.count)\n",
    "            \n",
    "            # Convert back to gesture name\n",
    "            gesture_name = REVERSE_GESTURE_MAPPER[final_prediction]\n",
    "            \n",
    "            print(f\"Predicted: {gesture_name} (class {final_prediction})\")\n",
    "            return gesture_name\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Prediction error: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return 'Text on phone'  # Fallback prediction\n",
    "    \n",
    "    return predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499e7c5f",
   "metadata": {
    "papermill": {
     "duration": 0.004367,
     "end_time": "2025-08-19T14:09:11.441139",
     "exception": false,
     "start_time": "2025-08-19T14:09:11.436772",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# MAIN EXECUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d42b595e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-19T14:09:11.451294Z",
     "iopub.status.busy": "2025-08-19T14:09:11.450703Z",
     "iopub.status.idle": "2025-08-19T14:14:53.585066Z",
     "shell.execute_reply": "2025-08-19T14:14:53.584304Z"
    },
    "papermill": {
     "duration": 342.140585,
     "end_time": "2025-08-19T14:14:53.586272",
     "exception": false,
     "start_time": "2025-08-19T14:09:11.445687",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CMI BFRB Detection - LightGBM with World Acceleration Feature\n",
      "============================================================\n",
      "🚀 Key Innovation: World Coordinate Transformation\n",
      "   Converting device acceleration to world coordinates\n",
      "   This normalizes for different wrist orientations!\n",
      "============================================================\n",
      "Loading training data...\n",
      "Loading test data...\n",
      "✓ Using 11 common IMU columns\n",
      "✓ Train-only columns: {'behavior', 'orientation', 'phase', 'sequence_type', 'gesture'}\n",
      "✓ Test-only columns: set()\n",
      "Extracting features for training sequences...\n",
      "Extracting features for test sequences...\n",
      "✓ Training features shape: (8151, 360)\n",
      "✓ Training labels shape: (8151,)\n",
      "✓ Test features shape: (2, 360)\n",
      "✓ Number of features: 359\n",
      "Training XGBoost models with 5-fold cross-validation...\n",
      "Feature columns: 359\n",
      "Starting cross-validation...\n",
      "\n",
      "--- Fold 1/5 ---\n",
      "Train size: 6623, Val size: 1528\n",
      "Training fold 1 with early stopping (Device: gpu)...\n",
      "[0]\tvalidation_0-mlogloss:2.78221\tvalidation_0-merror:0.60864\n",
      "[100]\tvalidation_0-mlogloss:1.22464\tvalidation_0-merror:0.39005\n",
      "[200]\tvalidation_0-mlogloss:1.09491\tvalidation_0-merror:0.38613\n",
      "[271]\tvalidation_0-mlogloss:1.07055\tvalidation_0-merror:0.38024\n",
      "Fold 1 - Competition Score: 0.7598 (Binary F1: 0.9781, Macro F1: 0.5415)\n",
      "\n",
      "--- Fold 2/5 ---\n",
      "Train size: 6519, Val size: 1632\n",
      "Training fold 2 with early stopping (Device: gpu)...\n",
      "[0]\tvalidation_0-mlogloss:2.79891\tvalidation_0-merror:0.65502\n",
      "[100]\tvalidation_0-mlogloss:1.44428\tvalidation_0-merror:0.46324\n",
      "[200]\tvalidation_0-mlogloss:1.34113\tvalidation_0-merror:0.43934\n",
      "[300]\tvalidation_0-mlogloss:1.32683\tvalidation_0-merror:0.43811\n",
      "[400]\tvalidation_0-mlogloss:1.33430\tvalidation_0-merror:0.43566\n",
      "[462]\tvalidation_0-mlogloss:1.34059\tvalidation_0-merror:0.43934\n",
      "Fold 2 - Competition Score: 0.7148 (Binary F1: 0.9535, Macro F1: 0.4761)\n",
      "\n",
      "--- Fold 3/5 ---\n",
      "Train size: 6526, Val size: 1625\n",
      "Training fold 3 with early stopping (Device: gpu)...\n",
      "[0]\tvalidation_0-mlogloss:2.79536\tvalidation_0-merror:0.63200\n",
      "[100]\tvalidation_0-mlogloss:1.43109\tvalidation_0-merror:0.47077\n",
      "[200]\tvalidation_0-mlogloss:1.32834\tvalidation_0-merror:0.46462\n",
      "[300]\tvalidation_0-mlogloss:1.31531\tvalidation_0-merror:0.45600\n",
      "[400]\tvalidation_0-mlogloss:1.31779\tvalidation_0-merror:0.45046\n",
      "[500]\tvalidation_0-mlogloss:1.32346\tvalidation_0-merror:0.44492\n",
      "[600]\tvalidation_0-mlogloss:1.33139\tvalidation_0-merror:0.44308\n",
      "[700]\tvalidation_0-mlogloss:1.33773\tvalidation_0-merror:0.43877\n",
      "[775]\tvalidation_0-mlogloss:1.34355\tvalidation_0-merror:0.43938\n",
      "Fold 3 - Competition Score: 0.7157 (Binary F1: 0.9490, Macro F1: 0.4825)\n",
      "\n",
      "--- Fold 4/5 ---\n",
      "Train size: 6519, Val size: 1632\n",
      "Training fold 4 with early stopping (Device: gpu)...\n",
      "[0]\tvalidation_0-mlogloss:2.80520\tvalidation_0-merror:0.69056\n",
      "[100]\tvalidation_0-mlogloss:1.46504\tvalidation_0-merror:0.47426\n",
      "[200]\tvalidation_0-mlogloss:1.35306\tvalidation_0-merror:0.47059\n",
      "[300]\tvalidation_0-mlogloss:1.33073\tvalidation_0-merror:0.46201\n",
      "[400]\tvalidation_0-mlogloss:1.32698\tvalidation_0-merror:0.45711\n",
      "[500]\tvalidation_0-mlogloss:1.33484\tvalidation_0-merror:0.45588\n",
      "[600]\tvalidation_0-mlogloss:1.34214\tvalidation_0-merror:0.44914\n",
      "[700]\tvalidation_0-mlogloss:1.35009\tvalidation_0-merror:0.45098\n",
      "[728]\tvalidation_0-mlogloss:1.35209\tvalidation_0-merror:0.45282\n",
      "Fold 4 - Competition Score: 0.7094 (Binary F1: 0.9472, Macro F1: 0.4715)\n",
      "\n",
      "--- Fold 5/5 ---\n",
      "Train size: 6417, Val size: 1734\n",
      "Training fold 5 with early stopping (Device: gpu)...\n",
      "[0]\tvalidation_0-mlogloss:2.80434\tvalidation_0-merror:0.66436\n",
      "[100]\tvalidation_0-mlogloss:1.44984\tvalidation_0-merror:0.47347\n",
      "[200]\tvalidation_0-mlogloss:1.34000\tvalidation_0-merror:0.46136\n",
      "[300]\tvalidation_0-mlogloss:1.31383\tvalidation_0-merror:0.45444\n",
      "[400]\tvalidation_0-mlogloss:1.30920\tvalidation_0-merror:0.44637\n",
      "[500]\tvalidation_0-mlogloss:1.31399\tvalidation_0-merror:0.44348\n",
      "[600]\tvalidation_0-mlogloss:1.32018\tvalidation_0-merror:0.44175\n",
      "[700]\tvalidation_0-mlogloss:1.32681\tvalidation_0-merror:0.44060\n",
      "[758]\tvalidation_0-mlogloss:1.33093\tvalidation_0-merror:0.44060\n",
      "Fold 5 - Competition Score: 0.7204 (Binary F1: 0.9575, Macro F1: 0.4833)\n",
      "\n",
      "============================================================\n",
      "CROSS-VALIDATION RESULTS\n",
      "============================================================\n",
      "Overall Competition Score: 0.7238 ± 0.0182\n",
      "Overall Binary F1: 0.9568\n",
      "Overall Macro F1: 0.4907\n",
      "Fold scores: ['0.7598', '0.7148', '0.7157', '0.7094', '0.7204']\n",
      "============================================================\n",
      "\n",
      "✓ Training completed successfully!\n",
      "✓ Final CV Score: 0.7238\n",
      "✓ Core innovation: World acceleration transformation\n",
      "✓ Models ready for inference\n",
      "\n",
      "Setting up inference server...\n",
      "Predicted: Eyelash - pull hair (class 3)\n",
      "Predicted: Eyebrow - pull hair (class 2)\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"Main execution pipeline\"\"\"\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"CMI BFRB Detection - LightGBM with World Acceleration Feature\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"🚀 Key Innovation: World Coordinate Transformation\")\n",
    "    print(\"   Converting device acceleration to world coordinates\")\n",
    "    print(\"   This normalizes for different wrist orientations!\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Load and prepare data\n",
    "    X_train, y_train, subjects, X_test, test_sequence_ids, imu_cols = load_and_prepare_data()\n",
    "    \n",
    "    # Train models\n",
    "    # models, cv_scores, overall_score = train_lightgbm_models(X_train, y_train, subjects)\n",
    "    # models, cv_scores, overall_score = train_randomforest_models(X_train, y_train, subjects)\n",
    "    models, cv_scores, overall_score = train_xgboost_models(X_train, y_train, subjects)\n",
    "    \n",
    "    # Prepare feature columns for inference\n",
    "    feature_cols = [col for col in X_train.columns if col != 'sequence_id']\n",
    "    \n",
    "    # Create prediction function\n",
    "    predict_func = create_prediction_function(models, feature_cols, imu_cols)\n",
    "    \n",
    "    print(f\"\\n✓ Training completed successfully!\")\n",
    "    print(f\"✓ Final CV Score: {overall_score:.4f}\")\n",
    "    print(f\"✓ Core innovation: World acceleration transformation\")\n",
    "    print(f\"✓ Models ready for inference\")\n",
    "    \n",
    "    return predict_func, models, cv_scores\n",
    "\n",
    "# Execute main pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    predict_function, trained_models, cv_scores = main()\n",
    "    \n",
    "    # Setup inference server\n",
    "    print(\"\\nSetting up inference server...\")\n",
    "    inference_server = kaggle_evaluation.cmi_inference_server.CMIInferenceServer(predict_function)\n",
    "    \n",
    "    if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "        inference_server.serve()\n",
    "    else:\n",
    "        # Local testing\n",
    "        inference_server.run_local_gateway(\n",
    "            data_paths=(\n",
    "                Config.TEST_PATH,\n",
    "                Config.TEST_DEMOGRAPHICS_PATH,\n",
    "            )\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 12518947,
     "sourceId": 102335,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 360.08993,
   "end_time": "2025-08-19T14:14:54.612387",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-08-19T14:08:54.522457",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
