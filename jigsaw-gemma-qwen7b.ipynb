{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":94635,"databundleVersionId":13121456,"sourceType":"competition"},{"sourceId":12564646,"sourceType":"datasetVersion","datasetId":7934362}],"dockerImageVersionId":31090,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Gemma and QWEN7B\n\nThis notebook uses two Models, Qwen7B and Gemma, to classify the posts.\n\nIt will then ensemble the two predictions. I don't know how to best ensemble the two models yet, for now we'll average.\n\n\nCredits: Kudos to the authors of these notebooks. Thanks for sharing:\n- https://www.kaggle.com/code/yangyefd/batch-gemma3-sample-rules-classification\n- https://www.kaggle.com/code/aerdem4/jigsaw-acrc-qwen7b-finetune-logits-processor-zoo","metadata":{}},{"cell_type":"markdown","source":"## How to install dependencies\n\nInteractive notebooks: Click on Add-Ons->Install Dependencies, then Run.\n\nSubmission: Dependencies will be installed automatically","metadata":{"execution":{"iopub.status.busy":"2025-07-25T09:31:47.893965Z","iopub.execute_input":"2025-07-25T09:31:47.894246Z","iopub.status.idle":"2025-07-25T09:38:21.881149Z","shell.execute_reply.started":"2025-07-25T09:31:47.894228Z","shell.execute_reply":"2025-07-25T09:38:21.880295Z"}}},{"cell_type":"markdown","source":"## Prepare Qwen7b and Gemma","metadata":{}},{"cell_type":"code","source":"import vllm, torch\nfrom logits_processor_zoo.vllm import MultipleChoiceLogitsProcessor\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T10:34:32.393250Z","iopub.execute_input":"2025-07-25T10:34:32.393567Z","iopub.status.idle":"2025-07-25T10:34:32.397175Z","shell.execute_reply.started":"2025-07-25T10:34:32.393543Z","shell.execute_reply":"2025-07-25T10:34:32.396455Z"}},"outputs":[],"execution_count":47},{"cell_type":"code","source":"def predict_with_model (sys_prompt, prompts, model_path):\n    model = vllm.LLM(\n        model_path,\n        tensor_parallel_size=torch.cuda.device_count(),\n        gpu_memory_utilization=0.45,\n        trust_remote_code=True,\n        dtype=\"half\",\n        enforce_eager=True,\n        max_model_len=4096,\n        disable_log_stats=True,\n        enable_prefix_caching=True\n    )\n    tokenizer = model.get_tokenizer()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\nqwen7b_model_path = \"/kaggle/input/jigsaw-acrc-qwen7b-v01\"\n\nqwen7b = vllm.LLM(\n    qwen7b_model_path,\n    tensor_parallel_size=torch.cuda.device_count(),\n    gpu_memory_utilization=0.45,\n    trust_remote_code=True,\n    dtype=\"half\",\n    enforce_eager=True,\n    max_model_len=4096,\n    disable_log_stats=True,\n    enable_prefix_caching=True\n)\nqwen7b_tokenizer = qwen7b.get_tokenizer()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-25T10:43:24.386369Z","iopub.execute_input":"2025-07-25T10:43:24.386678Z"}},"outputs":[{"name":"stdout","text":"INFO 07-25 10:43:24 [config.py:1604] Using max model len 4096\nWARNING 07-25 10:43:24 [cuda.py:103] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\nINFO 07-25 10:43:24 [llm_engine.py:228] Initializing a V0 LLM engine (v0.10.0) with config: model='/kaggle/input/jigsaw-acrc-qwen7b-v01', speculative_config=None, tokenizer='/kaggle/input/jigsaw-acrc-qwen7b-v01', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=None, served_model_name=/kaggle/input/jigsaw-acrc-qwen7b-v01, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=False, pooler_config=None, compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":false,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":0,\"local_cache_dir\":null}, use_cached_outputs=False, \n","output_type":"stream"},{"name":"stderr","text":"2025-07-25 10:43:29.360254: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1753440209.382065     388 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1753440209.388843     388 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"INFO 07-25 10:43:34 [__init__.py:235] Automatically detected platform cuda.\n\u001b[1;36m(VllmWorkerProcess pid=388)\u001b[0;0m INFO 07-25 10:43:35 [multiproc_worker_utils.py:226] Worker ready; awaiting tasks\n\u001b[1;36m(VllmWorkerProcess pid=388)\u001b[0;0m INFO 07-25 10:43:36 [cuda.py:346] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n\u001b[1;36m(VllmWorkerProcess pid=388)\u001b[0;0m INFO 07-25 10:43:36 [cuda.py:395] Using XFormers backend.\nINFO 07-25 10:43:37 [multiproc_worker_utils.py:138] Terminating local vLLM worker processes\n","output_type":"stream"},{"name":"stderr","text":"[W725 10:43:48.927688340 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n[W725 10:43:58.391046359 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n[W725 10:44:09.997048013 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n[W725 10:44:10.950126792 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n[W725 10:44:20.927328095 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n[W725 10:44:32.354583793 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n[W725 10:44:46.921594314 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n[W725 10:45:01.145451603 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n[W725 10:45:14.797038094 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n[W725 10:45:36.580929321 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"!ls /kaggle/input/","metadata":{"execution":{"iopub.status.busy":"2025-07-25T10:29:38.185234Z","iopub.execute_input":"2025-07-25T10:29:38.185791Z","iopub.status.idle":"2025-07-25T10:29:38.502967Z","shell.execute_reply.started":"2025-07-25T10:29:38.185766Z","shell.execute_reply":"2025-07-25T10:29:38.502253Z"}}},{"cell_type":"code","source":"import vllm, torch\nfrom logits_processor_zoo.vllm import MultipleChoiceLogitsProcessor\n\n\nGEMMA_PATH = \"/kaggle/input/gemma-7b-gguf/gemma-2-9b-it.Q5_K_M.gguf\"\n\ngemma = vllm.LLM(\n    GEMMA_PATH,\n    tensor_parallel_size=torch.cuda.device_count(),\n    gpu_memory_utilization=0.45,\n    trust_remote_code=True,\n    dtype=\"half\",\n    enforce_eager=True,\n    max_model_len=4096,\n    disable_log_stats=True,\n    enable_prefix_caching=True\n)\ngemma_tokenizer = gemma.get_tokenizer()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Prepare Training data and Examples ","metadata":{}},{"cell_type":"code","source":"import os, sys\nimport pandas as pd\n\ndata_path = \"/kaggle/input/jigsaw-agile-community-rules/test.csv\" \\\n                if os.getenv('KAGGLE_IS_COMPETITION_RERUN') \\\n                else \"/kaggle/input/jigsaw-agile-community-rules/train.csv\"\n\ndf = pd.read_csv(data_path)\ndf.head(10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T10:42:06.160171Z","iopub.status.idle":"2025-07-25T10:42:06.160494Z","shell.execute_reply.started":"2025-07-25T10:42:06.160332Z","shell.execute_reply":"2025-07-25T10:42:06.160347Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Preparing Prompts","metadata":{}},{"cell_type":"code","source":"SYS_PROMPT = \"\"\"\nYou are given a comment on reddit. Your task is to classify if it violates the given rule. Only respond Yes/No.\n\"\"\"\n\nqwen7b_prompts = []\ngemma_prompts = []\nfor i, row in df.iterrows():\n    text = f\"\"\"\nr/{row.subreddit}\nRule: {row.rule}\n\n1) {row.positive_example_1}\nViolation: Yes\n\n2) {row.negative_example_1}\nViolation: No\n\n3) {row.negative_example_2}\nViolation: No\n\n4) {row.positive_example_2}\nViolation: Yes\n\n5) {row.body}\n\"\"\"\n    print(text)\n    \n    messages = [\n        {\"role\": \"system\", \"content\": SYS_PROMPT},\n        {\"role\": \"user\", \"content\": text}\n    ]\n\n    qwen_prompt = qwen7b_tokenizer.apply_chat_template(\n        messages,\n        add_generation_prompt=True,\n        tokenize=False,\n    ) + \"Violation: \"\n    qwen7b_prompts.append(qwen_prompt)\n    gemma_prompt = gemma_tokenizer.apply_chat_template(\n        messages,\n        add_generation_prompt=True,\n        tokenize=False,\n    ) + \"Violation: \"\n    qwen7b_prompts.append(gemma_prompt)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T10:42:06.161555Z","iopub.status.idle":"2025-07-25T10:42:06.161868Z","shell.execute_reply.started":"2025-07-25T10:42:06.161709Z","shell.execute_reply":"2025-07-25T10:42:06.161724Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Predict / Apply MultiChoice processor","metadata":{}},{"cell_type":"code","source":"qwen_mclp = MultipleChoiceLogitsProcessor(qwen7b_tokenizer, \n                                     choices=[\"Yes\", \"No\"])\n\nqwen7b_outputs = qwen7b_llm.generate(\n    prompts,\n    vllm.SamplingParams(\n        seed=0,\n        skip_special_tokens=True,\n        max_tokens=1,\n        logits_processors=[mclp],\n        logprobs=len(mclp.choices)\n\n    ),\n    use_tqdm=True\n)\n\n\nqwen7b_logprobs = []\nfor lps in [output.outputs[0].logprobs[0].values() for output in qwen7b_outputs]:\n    qwen7b_logprobs.append({lp.decoded_token: lp.logprob for lp in list(lps)})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T10:42:06.163308Z","iopub.status.idle":"2025-07-25T10:42:06.163607Z","shell.execute_reply.started":"2025-07-25T10:42:06.163458Z","shell.execute_reply":"2025-07-25T10:42:06.163472Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"gemma_mclp = MultipleChoiceLogitsProcessor(gemma_tokenizer, \n                                     choices=[\"Yes\", \"No\"])\n\ngemma_outputs = gemma_llm.generate(\n    prompts,\n    vllm.SamplingParams(\n        seed=0,\n        skip_special_tokens=True,\n        max_tokens=1,\n        logits_processors=[mclp],\n        logprobs=len(mclp.choices)\n\n    ),\n    use_tqdm=True\n)\n\n\ngemma_logprobs = []\nfor lps in [output.outputs[0].logprobs[0].values() for output in gemma_outputs]:\n    gemma_logprobs.append({lp.decoded_token: lp.logprob for lp in list(lps)})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T10:42:06.164479Z","iopub.status.idle":"2025-07-25T10:42:06.164829Z","shell.execute_reply.started":"2025-07-25T10:42:06.164626Z","shell.execute_reply":"2025-07-25T10:42:06.164640Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Averaging predictions and creating a submission file\n","metadata":{}},{"cell_type":"code","source":"# Combine logprobs from Qwen and Gemma using logprob averaging\nfinal_preds = []\nfor q_lp, g_lp in zip(qwen7b_logprobs, gemma_logprobs):\n    avg_yes = (q_lp.get(\"Yes\", -999) + g_lp.get(\"Yes\", -999)) / 2\n    avg_no = (q_lp.get(\"No\", -999) + g_lp.get(\"No\", -999)) / 2\n    final_preds.append(\"Yes\" if avg_yes > avg_no else \"No\")\n\n# Format for Kaggle submission\nsubmission_df = pd.DataFrame({\n    \"example_id\": df[\"example_id\"],\n    \"label\": final_preds\n})\n\nsubmission_df.to_csv(\"submission.csv\", index=False)\nprint(\"Saved submission.csv\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T10:42:06.166549Z","iopub.status.idle":"2025-07-25T10:42:06.166853Z","shell.execute_reply.started":"2025-07-25T10:42:06.166710Z","shell.execute_reply":"2025-07-25T10:42:06.166723Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Extract individual logprobs for plotting\nqwen_yes = [lp.get(\"Yes\", -999) for lp in qwen7b_logprobs]\nqwen_no  = [lp.get(\"No\", -999) for lp in qwen7b_logprobs]\n\ngemma_yes = [lp.get(\"Yes\", -999) for lp in gemma_logprobs]\ngemma_no  = [lp.get(\"No\", -999) for lp in gemma_logprobs]\n\navg_yes = [(qy + gy) / 2 for qy, gy in zip(qwen_yes, gemma_yes)]\navg_no  = [(qn + gn) / 2 for qn, gn in zip(qwen_no, gemma_no)]\n\n# Plot histograms\nplt.figure(figsize=(12, 6))\nplt.hist(qwen_yes, bins=30, alpha=0.5, label='Qwen7B - Yes', density=True)\nplt.hist(qwen_no, bins=30, alpha=0.5, label='Qwen7B - No', density=True)\nplt.hist(gemma_yes, bins=30, alpha=0.5, label='Gemma - Yes', density=True)\nplt.hist(gemma_no, bins=30, alpha=0.5, label='Gemma - No', density=True)\nplt.hist(avg_yes, bins=30, alpha=0.6, label='Avg - Yes', histtype='step', linewidth=2)\nplt.hist(avg_no, bins=30, alpha=0.6, label='Avg - No', histtype='step', linewidth=2)\n\nplt.title(\"Histogram of Logprobs for 'Yes' and 'No' Predictions\")\nplt.xlabel(\"Log Probability\")\nplt.ylabel(\"Density\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T10:42:06.168451Z","iopub.status.idle":"2025-07-25T10:42:06.168755Z","shell.execute_reply.started":"2025-07-25T10:42:06.168600Z","shell.execute_reply":"2025-07-25T10:42:06.168622Z"}},"outputs":[{"name":"stdout","text":"INFO 07-25 10:42:10 [multiproc_worker_utils.py:125] Killing local vLLM worker processes\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}