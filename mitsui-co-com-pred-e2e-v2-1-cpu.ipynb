{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59e6b7e8",
   "metadata": {
    "papermill": {
     "duration": 0.006195,
     "end_time": "2025-08-11T17:44:06.947751",
     "exception": false,
     "start_time": "2025-08-11T17:44:06.941556",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## What this notebook does (endâ€‘toâ€‘end):\n",
    "\t1.\tLoads labels and a feature dataset (prefers mitsui_features_v2.parquet, falls back to mitsui_features.parquet, or rebuilds minimal label features if none found).\n",
    "\t2.\tMerges labels safely (even if your feature file already contains a y column).\n",
    "\t3.\tTrains one model per target (LightGBM if enough data, else ElasticNet) â€” all CPUâ€‘friendly.\n",
    "\t4.\tImplements the serverâ€‘style evaluation API predict(...):\n",
    "\tâ€¢\tUses precomputed features by (date_id, target) when available,\n",
    "\tâ€¢\tFalls back to lagâ€‘only features from the API when a date isnâ€™t in your feature dataset.\n",
    "\t5.\tStarts the Mitsui Inference Server for local gateway tests and for scoring.\n",
    "\n",
    "## Why this structure?\n",
    "\tâ€¢\tStable & fast: minimal dependency footprint, CPUâ€‘friendly.\n",
    "\tâ€¢\tLeakageâ€‘safe: all rolling features in the feature dataset were computed with .shift(1) (pastâ€‘only).\n",
    "\tâ€¢\tRobust at submission: works even if your feature dataset isnâ€™t attached â€” it will rebuild a basic labelâ€‘only set.\n",
    "\n",
    "## Files expected\n",
    "\tâ€¢\tCompetition data (autoâ€‘mounted):\n",
    "\tâ€¢\ttrain_labels.csv (labels per date)\n",
    "\tâ€¢\tServer files under kaggle_evaluation/* (the gateway & server scaffolding)\n",
    "\tâ€¢\tYour feature dataset (recommended):\n",
    "\tâ€¢\tmitsui_features_v2.parquet (preferred), or\n",
    "\tâ€¢\tmitsui_features.parquet (fallback)\n",
    "\tâ€¢\tIf missing, the notebook rebuilds minimal label features into /kaggle/working/mitsui_features.parquet.\n",
    "    Tip: keep a separate Feature Lab notebook to generate and version your feature parquet; attach it here via Add data.\n",
    "\n",
    "## Pipeline at a glance\n",
    "\n",
    "    Load labels â†’ Load (or rebuild) features â†’ Safe label merge â†’ Train perâ€‘target models â†’ Start server â†’ Predict batches\n",
    "\tâ€¢\tLightGBM is used when a target has enough rows (fast, handles nonâ€‘linearities).\n",
    "\tâ€¢\tElasticNet is the fallback for smaller targets (linear, regularized).\n",
    "\tâ€¢\tPrediction blend: 0.5 * carry_forward(y_lag_1) + 0.5 * model_pred for stability under Sharpeâ€‘like metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebd8c98",
   "metadata": {
    "papermill": {
     "duration": 0.001821,
     "end_time": "2025-08-11T17:44:06.952681",
     "exception": false,
     "start_time": "2025-08-11T17:44:06.950860",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 1) Imports, warnings, and small helpers\n",
    "\tâ€¢\tSets numpy/pandas warning filters to keep output clean.\n",
    "\tâ€¢\tUtility functions:\n",
    "\tâ€¢\t_find_one: glob helper to get the first matching file.\n",
    "\tâ€¢\t_to_pd: converts Polarsâ†’Pandas when the gateway hands Polars frames.\n",
    "\tâ€¢\t_ensure_one_row_targets: creates a single row with all target_* columns, replacing NaNs/Â±inf with 0 â€” avoids inference crashes.\n",
    "\n",
    "2) Feature loading\n",
    "\tâ€¢\t_load_features(labels_df) tries, in order:\n",
    "\t1.\t/kaggle/working/mitsui_features_v2.parquet\n",
    "\t2.\t/kaggle/working/mitsui_features.parquet\n",
    "\t3.\t/kaggle/input/**/mitsui_features_v2.parquet\n",
    "\t4.\t/kaggle/input/**/mitsui_features.parquet\n",
    "\t5.\tRebuilds a minimal feature set from train_labels.csv if none found.\n",
    "\tâ€¢\t_rebuild_minimal_features: lags 1â€“4, rolling mean/std(5), sign of previous return. All pastâ€‘only and zeroâ€‘filled.\n",
    "\n",
    "3) Safe supervised merge\n",
    "\tâ€¢\tMelts labels to long form: (date_id, target, y).\n",
    "\tâ€¢\tIf your feature parquet already contains a y column, itâ€™s dropped to avoid suffix conflicts.\n",
    "\tâ€¢\tAfter merge, the code detects which y column to use (y, y_lbl, etc.) and standardizes to a single y.\n",
    "\tâ€¢\tFEATURE_COLS are everything except keys and y.\n",
    "\n",
    "4) Training (one model per target)\n",
    "\tâ€¢\tIf LightGBM is available and the target has â‰¥ ~200 rows â†’ train LGBM with small leaves and 400 rounds.\n",
    "\tâ€¢\tElse â†’ ElasticNet with standardized inputs (StandardScaler).\n",
    "\tâ€¢\tTrains quickly on CPU and prints a summary (how many used LGBM vs ElasticNet).\n",
    "\n",
    "5) Predictâ€‘time feature index\n",
    "\tâ€¢\tBuilds a lexsorted MultiIndex feats_idx on [\"date_id\",\"target\"] for fast slicing during inference.\n",
    "\tâ€¢\tThis is crucial for the gateway loop to be efficient and avoid MultiIndex errors.\n",
    "\n",
    "6) Online inference (predict(...))\n",
    "\tâ€¢\tFor each batch, we:\n",
    "\tâ€¢\tExtract date_id from the batch â€œtestâ€ frame.\n",
    "\tâ€¢\tTry precomputed features: feats_idx.xs(date_id, level=\"date_id\", drop_level=False) â†’ gives (target, features) for that date.\n",
    "\tâ€¢\tIf not available (e.g., future forecasting dates), we build minimal lagâ€‘only features from the 4 lag tables that the API provides.\n",
    "\tâ€¢\tPredict per target:\n",
    "\tâ€¢\tUse the same feature names we trained on (usable_cols) â€” if the minimal set doesnâ€™t have some columns, theyâ€™re just skipped.\n",
    "\tâ€¢\tBlend model prediction with carry-forward (y_lag_1) for stability.\n",
    "\tâ€¢\tReturn a single-row wide DataFrame with target_0 ... target_423.\n",
    "\n",
    "7) Start the Mitsui inference server\n",
    "\tâ€¢\tLocal gateway: runs a full dryâ€‘run using the public data and prints progress (no internet needed).\n",
    "\tâ€¢\tScoring sandbox: Kaggle sets KAGGLE_IS_COMPETITION_RERUN=1; the code calls serve(), and the gateway handles submission.csv creation automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df7360ee",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-08-11T17:44:06.959738Z",
     "iopub.status.busy": "2025-08-11T17:44:06.959328Z",
     "iopub.status.idle": "2025-08-11T17:46:48.072449Z",
     "shell.execute_reply": "2025-08-11T17:46:48.071382Z"
    },
    "papermill": {
     "duration": 161.119117,
     "end_time": "2025-08-11T17:46:48.074261",
     "exception": false,
     "start_time": "2025-08-11T17:44:06.955144",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using features: /kaggle/input/feature-lab-mitsui-features-v2/mitsui_features_v2.parquet\n",
      "âœ… Trained 424 targets; using LightGBM for 424, ElasticNet for 0.\n",
      "Features used: 24\n",
      "ðŸ§ª Local gateway runâ€¦ using data at: /kaggle/input/mitsui-commodity-prediction-challenge\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# MITSUI&CO. â€“ E2E v2.1 (submit)\n",
    "# - Robust feature loading (v2â†’v1, /workingâ†’/input; rebuild minimal if needed)\n",
    "# - Merge labels safely (handles feature files that may already contain 'y')\n",
    "# - Train per target (LightGBM if enough data else ElasticNet)\n",
    "# - Server-style predict(): precomputed features by date_id,target â†’ fallback to lag-only\n",
    "# =========================================================\n",
    "\n",
    "import os, gc, glob, warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "np.seterr(invalid=\"ignore\", divide=\"ignore\", over=\"ignore\")\n",
    "\n",
    "# --------------------------\n",
    "# Helpers\n",
    "# --------------------------\n",
    "def _find_one(patterns):\n",
    "    for pat in patterns:\n",
    "        hits = glob.glob(pat, recursive=True)\n",
    "        if hits:\n",
    "            return hits[0]\n",
    "    return None\n",
    "\n",
    "def _to_pd(df):\n",
    "    try:\n",
    "        import polars as pl\n",
    "        if isinstance(df, pl.DataFrame):\n",
    "            return df.to_pandas()\n",
    "    except Exception:\n",
    "        pass\n",
    "    return df\n",
    "\n",
    "def _ensure_one_row_targets(df, tcols):\n",
    "    if df is None or len(df)==0:\n",
    "        return pd.DataFrame({c:[0.0] for c in tcols})\n",
    "    cols = [c for c in df.columns if str(c).startswith(\"target_\")]\n",
    "    if not cols:\n",
    "        return pd.DataFrame({c:[0.0] for c in tcols})\n",
    "    one = df[cols].head(1).copy().reindex(columns=tcols, fill_value=0.0)\n",
    "    return one.replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "\n",
    "def _rebuild_minimal_features(labels_df):\n",
    "    long = labels_df.melt(id_vars=\"date_id\", var_name=\"target\", value_name=\"y\")\n",
    "    long[\"y\"] = pd.to_numeric(long[\"y\"], errors=\"coerce\")\n",
    "    parts = []\n",
    "    for t, g in long.groupby(\"target\", sort=False):\n",
    "        df = g.sort_values(\"date_id\").reset_index(drop=True)\n",
    "        for L in range(1,5):\n",
    "            df[f\"y_lag_{L}\"] = df[\"y\"].shift(L)\n",
    "        y_past = df[\"y\"].shift(1)\n",
    "        df[\"roll_mean_5\"] = y_past.rolling(5).mean()\n",
    "        df[\"roll_std_5\"]  = y_past.rolling(5).std()\n",
    "        df[\"sign_lag_1\"]  = np.sign(y_past).fillna(0.0)\n",
    "        feat_cols = [c for c in df.columns if c!=\"y\"]\n",
    "        df[feat_cols] = df[feat_cols].replace([np.inf,-np.inf], np.nan).fillna(0.0)\n",
    "        df = df.dropna(subset=[\"y_lag_1\",\"y_lag_2\",\"y_lag_3\",\"y_lag_4\"]).reset_index(drop=True)\n",
    "        parts.append(df)\n",
    "        if len(parts)%100==0: gc.collect()\n",
    "    feats = pd.concat(parts, axis=0, ignore_index=True)\n",
    "    feats.to_parquet(\"/kaggle/working/mitsui_features.parquet\", index=False)\n",
    "    print(\"ðŸ” Rebuilt minimal features â†’ /kaggle/working/mitsui_features.parquet\")\n",
    "    return feats\n",
    "\n",
    "def _load_features(labels_df):\n",
    "    # Prefer v2 then v1 in /working, then in /input\n",
    "    p = _find_one([\n",
    "        \"/kaggle/working/mitsui_features_v2.parquet\",\n",
    "        \"/kaggle/working/mitsui_features.parquet\",\n",
    "        \"/kaggle/input/**/mitsui_features_v2.parquet\",\n",
    "        \"/kaggle/input/**/mitsui_features.parquet\",\n",
    "    ])\n",
    "    if p:\n",
    "        print(\"Using features:\", p)\n",
    "        return pd.read_parquet(p)\n",
    "    print(\"âš ï¸ No feature parquet found; rebuilding minimal label-only features...\")\n",
    "    return _rebuild_minimal_features(labels_df)\n",
    "\n",
    "# --------------------------\n",
    "# Load labels\n",
    "# --------------------------\n",
    "labels_path = _find_one([\"/kaggle/input/**/train_labels.csv\"])\n",
    "assert labels_path is not None, \"train_labels.csv not found. Attach the competition data.\"\n",
    "labels = pd.read_csv(labels_path).sort_values(\"date_id\").reset_index(drop=True)\n",
    "target_cols = [c for c in labels.columns if c.startswith(\"target_\")]\n",
    "assert target_cols, \"No target_* columns in train_labels.csv.\"\n",
    "\n",
    "# --------------------------\n",
    "# Load features (handles v2/v1 + working/input + rebuild)\n",
    "# --------------------------\n",
    "feats_long = _load_features(labels)\n",
    "\n",
    "# --------------------------\n",
    "# Supervised join (robust to 'y' already present in features)\n",
    "# --------------------------\n",
    "labels_long = labels.melt(id_vars=\"date_id\", var_name=\"target\", value_name=\"y\")\n",
    "\n",
    "# If features already have any y-like column, drop it to avoid suffix mess\n",
    "feats_no_y = feats_long.drop(columns=[c for c in [\"y\",\"y_x\",\"y_y\",\"y_lbl\"] if c in feats_long.columns], errors=\"ignore\")\n",
    "\n",
    "train_long = feats_no_y.merge(labels_long, on=[\"date_id\",\"target\"], how=\"left\", suffixes=(\"\", \"_lbl\"))\n",
    "\n",
    "# Find the correct y column after merge\n",
    "y_col = None\n",
    "for cand in [\"y\",\"y_lbl\",\"y_y\",\"y_x\"]:\n",
    "    if cand in train_long.columns:\n",
    "        y_col = cand; break\n",
    "assert y_col is not None, \"Could not find target column after merge.\"\n",
    "train_long = train_long.rename(columns={y_col:\"y\"}).dropna(subset=[\"y\"]).reset_index(drop=True)\n",
    "\n",
    "FEATURE_COLS = [c for c in train_long.columns if c not in [\"date_id\",\"target\",\"y\"]]\n",
    "assert FEATURE_COLS, \"No feature columns found after merge.\"\n",
    "\n",
    "# --------------------------\n",
    "# Train per-target models\n",
    "# --------------------------\n",
    "HAS_LGB = True\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "except Exception:\n",
    "    HAS_LGB = False\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "models, scalers, use_lgb = {}, {}, {}\n",
    "for t, g in train_long.groupby(\"target\", sort=False):\n",
    "    X = g[FEATURE_COLS].to_numpy(dtype=\"float32\")\n",
    "    y = g[\"y\"].to_numpy(dtype=\"float32\")\n",
    "\n",
    "    if HAS_LGB and len(g) >= 200:\n",
    "        ds = lgb.Dataset(X, label=y)\n",
    "        params = dict(\n",
    "            objective=\"regression\", metric=\"l2\",\n",
    "            learning_rate=0.05, num_leaves=16,\n",
    "            min_data_in_leaf=25, feature_fraction=0.8,\n",
    "            bagging_fraction=0.8, bagging_freq=1,\n",
    "            verbosity=-1, num_threads=0\n",
    "        )\n",
    "        model = lgb.train(params, ds, num_boost_round=400)\n",
    "        models[t] = model; use_lgb[t] = True\n",
    "    else:\n",
    "        scaler = StandardScaler()\n",
    "        Xs = scaler.fit_transform(X)\n",
    "        model = ElasticNet(alpha=0.0005, l1_ratio=0.1, random_state=42, max_iter=5000)\n",
    "        model.fit(Xs, y)\n",
    "        models[t] = model; scalers[t] = scaler; use_lgb[t] = False\n",
    "\n",
    "    if len(models)%100==0: gc.collect()\n",
    "\n",
    "print(f\"âœ… Trained {len(models)} targets; using LightGBM for {sum(use_lgb.values())}, ElasticNet for {len(models)-sum(use_lgb.values())}.\")\n",
    "print(f\"Features used: {len(FEATURE_COLS)}\")\n",
    "\n",
    "# --------------------------\n",
    "# Predict server (Mitsui API)\n",
    "# --------------------------\n",
    "# Build an index of features for quick lookup at predict time (drop 'y' if present)\n",
    "# Build a lexsorted MultiIndex for fast/valid slicing\n",
    "feats_no_y = feats_long.drop(columns=[c for c in [\"y\",\"y_x\",\"y_y\",\"y_lbl\"] if c in feats_long.columns], errors=\"ignore\")\n",
    "\n",
    "# Ensure rows are ordered first, then set index and sort\n",
    "feats_no_y = feats_no_y.sort_values([\"date_id\", \"target\"]).reset_index(drop=True)\n",
    "feats_idx = feats_no_y.set_index([\"date_id\", \"target\"])[FEATURE_COLS].sort_index()\n",
    "\n",
    "try:\n",
    "    import polars as pl\n",
    "except Exception:\n",
    "    pl = None\n",
    "\n",
    "def _minimal_lag_features(l1,l2,l3,l4, tcols):\n",
    "    L1 = _ensure_one_row_targets(_to_pd(l1), tcols)\n",
    "    L2 = _ensure_one_row_targets(_to_pd(l2), tcols)\n",
    "    L3 = _ensure_one_row_targets(_to_pd(l3), tcols)\n",
    "    L4 = _ensure_one_row_targets(_to_pd(l4), tcols)\n",
    "    rows = []\n",
    "    for t in tcols:\n",
    "        lags = np.array([float(L1.at[0,t]), float(L2.at[0,t]), float(L3.at[0,t]), float(L4.at[0,t])], dtype=\"float32\")\n",
    "        rows.append({\n",
    "            \"target\": t,\n",
    "            \"y_lag_1\": lags[0],\n",
    "            \"y_lag_2\": lags[1],\n",
    "            \"y_lag_3\": lags[2],\n",
    "            \"y_lag_4\": lags[3],\n",
    "            \"roll_mean_5\": float(np.nanmean(lags)),\n",
    "            \"roll_std_5\":  float(np.nanstd(lags)),\n",
    "            \"sign_lag_1\":  float(np.sign(lags[0]) if np.isfinite(lags[0]) else 0.0),\n",
    "        })\n",
    "    return pd.DataFrame(rows).replace([np.inf,-np.inf], np.nan).fillna(0.0)\n",
    "\n",
    "def _predict_for_date(date_id, lag1, lag2, lag3, lag4):\n",
    "    # Use precomputed features if this date exists\n",
    "    have_any = (date_id, target_cols[0]) in feats_idx.index\n",
    "    if have_any:\n",
    "        # xs() requires lexsorted index (we ensured above)\n",
    "        sub = feats_idx.xs(date_id, level=\"date_id\", drop_level=False).reset_index()\n",
    "        # Now sub has columns: ['date_id', 'target', *feature_cols]\n",
    "        feat_df = sub\n",
    "    else:\n",
    "        # Fallback: build minimal lag-only features on the fly\n",
    "        feat_df = _minimal_lag_features(lag1, lag2, lag3, lag4, target_cols)\n",
    "        feat_df.insert(0, \"date_id\", date_id)\n",
    "\n",
    "    # Only use columns that were in training\n",
    "    usable_cols = [c for c in FEATURE_COLS if c in feat_df.columns]\n",
    "    preds = {}\n",
    "    for _, r in feat_df.iterrows():\n",
    "        t = r[\"target\"]\n",
    "        x = r[usable_cols].to_numpy(dtype=\"float32\").reshape(1, -1)\n",
    "\n",
    "        if t not in models:\n",
    "            preds[t] = float(r.get(\"y_lag_1\", 0.0))  # carry-forward fallback\n",
    "            continue\n",
    "\n",
    "        if use_lgb.get(t, False):\n",
    "            yhat = float(models[t].predict(x)[0])\n",
    "        else:\n",
    "            xs = scalers[t].transform(x) if t in scalers else x\n",
    "            yhat = float(models[t].predict(xs)[0])\n",
    "\n",
    "        carry = float(r.get(\"y_lag_1\", 0.0))\n",
    "        preds[t] = 0.5 * carry + 0.5 * yhat  # stability blend\n",
    "\n",
    "    return pd.DataFrame({t: [preds.get(t, 0.0)] for t in target_cols}).replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "\n",
    "def predict(test, label_lags_1_batch, label_lags_2_batch, label_lags_3_batch, label_lags_4_batch):\n",
    "    test_pd = _to_pd(test)\n",
    "    date_id = int(test_pd[\"date_id\"].iloc[0]) if (test_pd is not None and \"date_id\" in test_pd.columns and len(test_pd)>0) else -1\n",
    "    out = _predict_for_date(date_id, label_lags_1_batch, label_lags_2_batch, label_lags_3_batch, label_lags_4_batch)\n",
    "    return pl.DataFrame(out) if pl is not None else out\n",
    "\n",
    "# --------------------------\n",
    "# Start Mitsui server\n",
    "# --------------------------\n",
    "import kaggle_evaluation.mitsui_inference_server as mitsui_srv\n",
    "inference_server = mitsui_srv.MitsuiInferenceServer(predict)\n",
    "\n",
    "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    print(\"ðŸ”Œ Starting Mitsui inference server (scoring)â€¦\")\n",
    "    inference_server.serve()\n",
    "else:\n",
    "    # Find a data root that contains the local gateway assets\n",
    "    def _find_comp_root():\n",
    "        if os.path.exists(\"/kaggle/input/mitsui-commodity-prediction-challenge\"):\n",
    "            return \"/kaggle/input/mitsui-commodity-prediction-challenge\"\n",
    "        # fallback: any folder with kaggle_evaluation inside\n",
    "        for root, dirs, files in os.walk(\"/kaggle/input\"):\n",
    "            if \"kaggle_evaluation\" in dirs:\n",
    "                return root\n",
    "        return \"/kaggle/input\"\n",
    "    comp_root = _find_comp_root()\n",
    "    print(\"ðŸ§ª Local gateway runâ€¦ using data at:\", comp_root)\n",
    "    inference_server.run_local_gateway((comp_root,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8537cd88",
   "metadata": {
    "papermill": {
     "duration": 0.002025,
     "end_time": "2025-08-11T17:46:48.078749",
     "exception": false,
     "start_time": "2025-08-11T17:46:48.076724",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Next steps (optional improvements)\n",
    "\tâ€¢\tWalkâ€‘forward CV for LGBM/ElasticNet per target or per market group (LME/JPX/US/FX).\n",
    "\tâ€¢\tFeature Lab upgrades: spreads, zâ€‘spreads, EWMA vol, rolling corr/beta from train.csv + target_pairs.csv (already supported in mitsui_features_v2.parquet).\n",
    "\tâ€¢\tClipping/winsorization of extreme predictions; targetâ€‘family ensembling; decayâ€‘weighted blends."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 13044405,
     "sourceId": 94771,
     "sourceType": "competition"
    },
    {
     "datasetId": 8041146,
     "sourceId": 12722282,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 167.405337,
   "end_time": "2025-08-11T17:46:49.404787",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-08-11T17:44:01.999450",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
